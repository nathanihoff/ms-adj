---
output:
  # bookdown::word_document2:
    # reference_docx: "word-template.docx"
  bookdown::pdf_document2:
    toc: no
    number_sections: no
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
    latex_engine: xelatex
always_allow_html: true
header-includes:
  #- \usepackage{setspace}\doublespace
  # - \usepackage[nolists, fighead, tabhead]{endfloat}
  # - \usepackage{endnotes}
  # - \let\footnote=\endnote
#- \setlength{\headheight}{14.5pt}
#- \setlength{\headheight}{13.6pt}
# - \usepackage{fancyhdr}
# - \pagestyle{fancy}
# - \lhead{N.I. Hoffmann}
# - \rhead{`r format(Sys.time(), '%B %e, %Y')`}
# - \newcommand{\beginsupplement}{\setcounter{table}{0}  
# \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} 
# \renewcommand{\thefigure}{A\arabic{figure}}}

editor_options: 
  chunk_output_type: console


citeproc: no
fontfamily: mathpazo
#fontsize: 11pt
# geometry: margin=.6in
indent: yes
link-citations: yes
linkcolor: blue
lang: 'en-US'

bibliography: "/Users/nathan/Documents/My Library.bib" 
# bibliography: "My Library.bib"  
csl: apa.csl
# csl: american-sociological-association.csl

title: "Demystifying Double Robust, Flexible Adjustment Methods for Causal Inference"
#subtitle: "PAA 2023"
author:  Nathan I. Hoffmann
#   | Department of Sociology, UCLA
date: "`r format(Sys.time(), '%B %e, %Y')`"

abstract: "Double robust methods for flexible covariate adjustment in causal inference have proliferated in recent years. Despite their apparent advantages, these methods remain underutilized by social scientists. It is also unclear whether these methods actually outperform more traditional methods in finite samples. This paper has two aims: it is a guide to some of the latest methods in doubly robust, flexible covariate adjustment for causal inference, and it compares these methods to more traditional statistical methods. It does this by using both simulated data where the treatment effect estimate is known, and then using complex survey data from the Program for International Student Assessment (PISA). Methods covered include Targeted Maximum Likelihood Estimation, Double/Debiased Machine Learning, and Augmented Inverse Propensity Weighting. Although these methods are robust to either the exposure or response model being misspecified, preliminary results show that, when both models are incorrect, bias grows rapidly in small samples."

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T)
options("yaml.eval.expr" = TRUE)

library(broom)
library(knitr)
library(here)
library(haven)
library(tidyverse)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


options("yaml.eval.expr" = TRUE, scipen = 3, digits = 2)

uclablue = '#2774AE'
gray = '#808080'
black = '#000000'
ucla_palette = c(black, uclablue, gray)

# theme_set(theme_cowplot(font_family = 'Palatino') + 
theme_set(theme_classic(base_family = 'Palatino') + 
      theme(legend.title=element_blank(), 
         panel.grid.major.y = element_line('grey80'),
         legend.background = element_rect(fill = alpha("white", 0.5))
         ))
ggplot <- function(...) ggplot2::ggplot(...) + 
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")

kable <- function(...) knitr::kable(..., format.args = list(big.mark = ","))
```

```{r, functions}
perform <- function(est_df, label){
  est_df %>%
    summarize(bias = mean(d - truth),
              percent_bias = bias/sd(d),
              rmse = sqrt(mean(d - truth)^2),
              mae = median(abs(d - truth)),
              fail_count = first(fail_count),
              comp_time = first(comp_time)) %>%
    mutate(label = label,
           n = nrow(est_df) + fail_count) %>%
    select(label, everything()) %>%
    return()
}
```


```{r load, eval = F}
# data from mx0.5
pisa <- readRDS('/Users/nathan/My Drive/1 UCLA/Dissertation/data/pisa.rds') %>%
  mutate(age = as.numeric(age))

pisa <- pisa %>%
  filter(across(c(female,  mom_ed, dad_ed, early_ed, cultural_pos, home_ed, age,
                  math1, math2, math3, math4, math5,
                  read1, read2, read3, read4, read5,
                  scie1, scie2, scie3, scie4, scie5),
                ~ !is.na(.x))) %>%
  mutate(
    country = case_when(
       country == '034400' ~ 'Hong Kong',
       country == '044600'~ 'Macao',
       country == 'Russian Federation' ~ 'Russia',
       country == 'United States of America' ~ 'United States',
       T ~ country
     ),
   birth_country = case_when(
       birth_country == '034400' ~ 'Hong Kong',
       birth_country == '044600'~ 'Macao',
       birth_country == 'United States of America' ~ 'United States',
       T ~ birth_country
     ),
   immigrant = case_when(
    birth_country != country ~ T,
    birth_country == country ~ F
    )) %>%
  # include immigrants with a birth country in the dataset, or non-immigrants
  filter((immigrant == T & birth_country %in% unique(pisa$country)) | 
           immigrant == F) %>%
   mutate(school_id = paste0(year, school_id),
         village = school_location == 'Village',
         non_urban = ifelse(!is.na(school_location), school_location %in% c('Village', 'Small Town', 'Town'), NA),
         match_country = ifelse(immigrant == T, birth_country, country))


# Include only match_country with at least 5 imm and 5 non-imm
n_min <- 5
match_country_imm <- pisa %>%
  filter(immigrant == T) %>%
  group_by(match_country) %>%
  count() %>% 
  filter(n >= n_min) %>%
  pull(match_country)

match_country_nonimm <- pisa %>%
  filter(immigrant == F) %>%
  group_by(match_country) %>%
  count() %>% 
  filter(n >= n_min) %>%
  pull(match_country)

pisa <- pisa %>%
  filter(match_country %in% match_country_imm,
         match_country %in% match_country_nonimm,
         # Remove match_country with no immigrants
         match_country %in% unique(pisa$match_country[pisa$immigrant == T])
         ) %>%
  mutate(immigrant = as.numeric(immigrant))
```

<!-- # Abstract -->

<!-- This thesis aims to compare different advanced adjustment methods. It will compare Imben & Rubins propensity matching, stabilized IPWs, TMLE, double machine learning (Chernozhukov et al 2018), Ratkovic (2021), and BART. -->

# Introduction

Statistical methods for flexible covariate adjustment in causal inference have proliferated in recent years. These methods have a number of strengths over traditional regression methods: They make few functional form assumptions, can accommodate large numbers of covariates, and produce easily interpretable treatment effect estimates. Many of these methods also have a "double robust" property: They estimate one model for the treatment exposure and another for the outcome, and as long as at least one is correctly specified, then the treatment effect will be estimated consistently. Despite their apparent advantages, these methods remain underutilized by social scientists. Part of the barrier has been lack of familiarity with these methods. It has also been unclear how these methods compare, or whether such methods actually perform better than traditional methods in finite samples. 
<!-- And when researchers want to use an existing R package for these methods, the array of possible estimation methods and other options can be dizzying.   -->

This paper makes advances on these fronts. First, it is a guide to some of the latest methods in doubly robust, flexible covariate adjustment for causal inference, explaining the methods to a social scientist audience. Second, it compares these methods to more traditional statistical methods using a type of data that social scientists frequently encounter: cross-national survey data. It does this by using both simulated data where the treatment effect estimate is known, and then using complex survey data from the Program for International Student Assessment (PISA).  

Methods covered include Targeted Maximum Likelihood Estimation [TMLE, @vanderlaan_2006_targeted], Double or Debiased Machine Learning [DML, @chernozhukov_2018_double], and Augmented Inverse Propensity Weighting [AIPW, @glynn_2010_introduction]. This paper reviews the theory behind these methods as well as simple R implementations of them on simulations and real data. These methods are compared to two methods commonly used by social scientists: ordinary least squares (OLS) regression, and matching on propensity scores estimated from logistic regression (PSM).



# Conceptual Overview

Doubly robust methods estimate two models:  

- an *outcome model*

$$\mu_d(X_i) = E(Y_i \mid D_i = d, X_i)$$

- and an *exposure model* (or treatment model or propensity score):

$$\pi(X_i) = E(D_i \mid X_i)$$

where $\mu_d(\cdot)$ is the model of control or treatment $D_i = d=\{0, 1\}$, $X_i$ is a vector of covariates for unit $i = 1, \ldots, N$ for treatment (1) and control (0), $Y_i$ is the outcome, and $\pi(\cdot)$ is the exposure model. The covariates included in $X_i$ can be different for the two models. 

An estimator is called "doubly robust" if it achieves consistent estimation of the ATE (or whatever estimand the researcher is interested in) as long as *at least one* of these two models is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  


## Origins of Doubly Robust Methods

According to @bang_2005_doubly, doubly robust methods have their origins in missing data models. @robins_1994_estimation and @rotnitzky_1998_semiparametric developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models, and @scharfstein_1999_adjusting showed that AIPW was doubly robust and extended to causal inference.

But @kang_2007_demystifying argue that doubly robust methods are older. They cite work by @cassel_1976_results, who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated. Arguably, doubly robust methods go back even further than this. The form of doubly robust methods is similar to residual-on-residual regression, which dates back to @frisch_1933_partial famous FWL theorem:

$$\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)}$$

where $\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$. This formulation writes the regression coefficient as composed of an outcome model ($\tilde Y_i$) and exposure model ($\tilde D_i$), the two models used in doubly robust estimators.

There are also links between doubly robust methods and matching with regression adjustment. This work goes back to at least @rubin_1973_use, who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.

## Assumptions

Most doubly robust methods require almost all of the standard assumptions necessary formost methods that depend on selection on observables. Although some doubly robust methods relax one or two of these, the six standard assumptions are:  

1. Consistency
2. Positivity/overlap
3. One version of treatment
4. No interference
5. IID observations
6. Conditional ignorability: $\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid X_i$

Special attention should be paid to Assumption 6: doubly robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the doubly robust methods covered in this tutorial make no functional form assumptions. Most use flexible machine learning algorithms to estimate both the outcome and exposure models, with regularization (often through cross-fitting) to avoid overfitting. 


## A simple demonstration

To demonstrate double robustness, this section presents one of the simpler doubly robust estimators: Augmented Inverse Propensity Weighting (AIPW) [@glynn_2010_introduction]. We can write this estimator as follows:

$$\begin{aligned}
\widehat{ATE} = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) 
- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}$$

For each individual in the sample, this estimator calculates two quantities:

- The treated potential outcome

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

- The control potential outcome

$$\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i)$$

Let's focus on the treated model:

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

First, assume that the outcome model $\mu_1(X_i)$ is *correctly* specified and the exposure model $\pi(X_i)$ is *incorreclty* specified. Let's also assume (for now) that we're dealing with a treated unit, i.e. $D_i = 1$. Then

$$\hat \mu_1 (X_i) = Y_i$$

and hence

$$\hat Y_{1i} = \frac{D_i(0)}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

So the model relies *only* on the outcome model. The incorrectly specified exposure model completely disappears from the equation. If we're dealing with a control unit ($D_i=0$), we get the same result:

$$\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

Now, what if the *exposure* model $\pi(X_i)$ is correctly specified and the outcome model $\mu_1(X)$ is incorrect? First, we rewrite the estimator for the treated outcome:

$$\begin{aligned}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). &&(*)
\end{aligned}$$

Since the exposure model is correclty specified, we have $D_i = \hat \pi(X_i)$ on average, so

$$E[D_i - \hat \pi(X_i)] = 0.$$

This means that the second term in equation $(*)$ is 0, so

$$E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].$$

This shows that when the exposure model is correct, then the estimator depends *only* on the exposure model. We can make similar arguments for the control model $\hat Y_{0i}$.

This demonstration shows that this estimator achieves double robustness: the estimator is robust to misspecification of either the exposure or the outcome model (but not both).

# Overview of Techniques  
Each of the methods reviewed in this paper can be thought of as a collection of estimation techniques. Each involves a model for the outcome and another for the treatment exposure, but the ways these relate and are combined varies from method to method. Choice of estimation technique for these two models is left to the discretion of the user; often ensemble learning is recommended, but in practice simpler methods can also work well.  

*Augmented Inverse Propensity Weighting (AIPW)*: The oldest of these modern methods, AIPW arose in the context of missing data imputation. As shown in the demonstration above, the method simply combines estimates from a model for the treatment exposure, $\pi(X)$, and a model for the outcome, $\mu(X)$. The name comes from the close similarity to inverse propensity weights (IPW), but whereas IPW only weights for propensity of treatment, AIPW "augments" these weights with an estimate of the response surface as well.  

*Targeted Maximum Likelihood Estimation (TMLE)*: TMLE begins by estimating the relevant part of the data-generating distribution $P(Y)$, i.e. the conditional density $Q = P(Y \mid X)$. It next estimates the exposure model. Although any estimation method can be used for these steps, the originators of the method suggest using a "super learner," i.e. ensemble learning with cross-validation. Next, the exposure model is used to calculate a "clever covariate," which is similar to an IPW. The coefficient for this clever covariate is estimated using maximum likelihood -- whence the "MLE" in "TMLE." Finally, the estimate of $Q$ is updated in a function involving the clever covariate. This process can be iterated, but usually one iteration is enough. The estimate of the distribution $Q$ can be used to calculate the estimand of interest.  

*Double or Debiased Machine Learning* (DML): The most recent of the methods reviewed here, DML is motivated by the need to handle problems with high-dimensional nuisance parameters, i.e. a large number of measured confounders. Flexible machine learning is appropriate for this task, but such methods suffer from regularization bias. DML removes this bias in a two-step procedure. First, it solves the auxiliary problem of estimating the treatment exposure model $E(D|X) =  \pi(X)$. It then uses this model to remove bias: Neyman orthogonalization allows the creation of an orthogonalized regressor, essentially partialing out the effect of covariates $X$ from treatment $D$. The debiased $D$ is then used to estimate the conditional mean of the outcome $E(Y\mid X) = \mu(X)$, which can be used to calculate the estimand of interest.  

These methods have many similarities. How do the results they give compare? The next section tests the performance of each in practice.  

# Methods  

For the comparisons in the next section, estimation methods include ordinary least squares regression ("OLS"),  propensity-score matching with scores estimated from logistic regression using the `MatchIt` package ("PSM"), the augmented inverse propensity weighted estimator with generalized additive models using the `CausalGAM` package (AIPW), targeted maximum likelihood estimation using the `tmle` package (TMLE) with the default Superlearner that relies on the `SL.glm`,` SL.step`, and `GL.glm.interaction` functions, and double/debiased machine learning with random forests with the `DoubleML` and `mlr3` packages.  


# Results

 

## Simluation with Dorie et al. (2019) Data

In 2016, the Atlantic Causal Inference Conference hosted a competition for causal inference methods that adjust on observables. @dorie_2019_automated published the results of this competition, along with the data used in the competition. Below, I test double robust methods on the 20 data sets used for the "do-it-yourself" part of the competition. The data represent a hypothetical twins study investigating the impact of birth weight on IQ. The data have 4802 observations and 52 covariates. The authors of the study specify a different data generating process for the potential outcomes in each data set. In all cases, ignorability holds, but the authors vary the following:  

- degree of nonlinearity
- percentage of treated
- overlap for the treatment group
- alignment (correspondence between the assignment mechanism and the response surface)
- treatment effect heterogeneity
- overall magnitude of the treatment effect  

The 20 data sets used here cover a range of these attributes; see the supplemental material from @dorie_2019_automated for details. For these preliminary results, I used 5 simulations of each data set, resulting in 100 data sets. I then calculated bias, percent bias (the estimator's bias as a percentage of its standard error), root mean squared error (rmse), and median absolute error (mae). I also include the number of data sets for which the method failed and the average computation time for each data set, in seconds.



```{r dorie-sim, eval = F}
set.seed(185)
sim_list <- list()
for(i in 1:20){ # up to 77
  #sim_list_nest <- list()
  for(j in 1:10){ # up to 100
    print(paste0(i,',',j))
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      i, j, extraInfo = T)
  }
  #sim_list[[i]] <- sim_list_nest
}




# dat <- sim_list
# i <- '3,1'
# 
# sim_dat <- data.frame(y = dat[[i]]$y, 
#                           d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
#                           dat[[i]]$x)
# lm_out <- lm(y ~ d + ., sim_dat)
# lm_list[[i]] <- data.frame(d = lm_out$coefficients[['d']],
#                            truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))

sim_linear <- list()
for(i in c(1, 3))
  for(j in 1:100){ 
    print(paste0(i,',',j))
    sim_linear[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      3, j, extraInfo = T)
  }
}


```


```{r lm-sim, eval = F}
lm_sim <- function(dat){
  start_time <- Sys.time()
  n <- length(dat)
  lm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    
    tryCatch({
      lm_out <- tidy(lm(y ~ d + ., sim_dat))
      lm_list[[i]] <- data.frame(d = lm_out[[2,2]],
                                 se = lm_out[[2,3]],
                                 truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
    
  }
  
  fail_count <- sum(sapply(lm_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(lm_list) == n, fail_count, fail_count + (n - length(lm_list)))
  
  end_time <- Sys.time()
  
  return(list(
    est_df = bind_rows(lm_list),
    fail_count = fail_count,
    comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
    )
}

lm_sim_out <- lm_sim(sim_list) 
lm_sim_linear <- lm_sim(sim_linear) 


lm_df <- lm_sim_out$est_df %>%
  mutate(fail_count = lm_sim_out$fail_count,
         comp_time = lm_sim_out$comp_time)

lm_df_linear <- lm_sim_linear$est_df %>%
  mutate(fail_count = lm_sim_linear$fail_count,
         comp_time = lm_sim_linear$comp_time)


write_csv(lm_df, here('files', 'lm_df.csv'))
write_csv(lm_df_linear, here('files', 'lm_df_linear.csv'))

# lm_lalonde <- lm_sim(lalonde)

```


```{r psm-sim, eval = F}
psm_sim <- function(dat){
  start_time <- Sys.time()
  n <- length(dat)
  psm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    form <- as.formula(paste0('d ~ ', paste(names(dat[[i]]$x), collapse = '+')))
    match_out <- MatchIt::matchit(form,
                           data = sim_dat,
                           method = 'nearest',
                           distance = 'glm') 
    
    # match_data <- MatchIt::match.data(match_out) 
    # apply(match_data, 1, unique)
    form2 <- as.formula(paste0('y ~ d + ', paste(names(dat[[i]]$x), collapse = '+')))
    
  tryCatch({
    psm_out <- lm(form2, 
                  MatchIt::match.data(match_out), 
                  weights = weights) %>%
      tidy()
    psm_list[[i]] <- data.frame(d = psm_out[[2,2]],
                                se = psm_out[[2,3]],
                                truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
    
  }
  
  fail_count <- sum(sapply(psm_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(psm_list) == n, fail_count, fail_count + (n - length(psm_list)))
  
  end_time <- Sys.time()
  
  return(list(
    est_df = bind_rows(psm_list),
    fail_count = fail_count,
    comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

psm_sim_out <- psm_sim(sim_list) 
psm_sim_linear <- psm_sim(sim_linear) 


psm_df <- psm_sim_out$est_df %>%
  mutate(fail_count = psm_sim_out$fail_count,
         comp_time = psm_sim_out$comp_time)

psm_df_linear <- psm_sim_linear$est_df %>%
  mutate(fail_count = psm_sim_linear$fail_count,
         comp_time = psm_sim_linear$comp_time)

write_csv(psm_df, here('files', 'psm_df.csv'))
write_csv(psm_df_linear, here('files', 'psm_df_linear.csv'))
```

```{r aipw-sim, eval = F}
aipw_sim <- function(dat, seed = 185){

  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  aipw_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = as.numeric(ifelse(dat[[i]]$z == 'trt', 1, 0)), 
                          dat[[i]]$x)
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #                              Y = sim_dat$y, W = sim_dat$d)
      
      aipw_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'AIPW')
      
      aipw_list[[i]] <- data.frame(d = aipw_out[[1]],
                                   se = aipw_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(aipw_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(aipw_list) == n, fail_count, fail_count + (n - length(aipw_list)))
  
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(aipw_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

aipw_out <- aipw_sim(sim_list)
aipw_out_linear <- aipw_sim(sim_linear)

aipw_df <- aipw_out$est_df %>%
  mutate(fail_count = aipw_out$fail_count,
         comp_time = aipw_out$comp_time)

aipw_df_linear <- aipw_out_linear$est_df %>%
  mutate(fail_count = aipw_out_linear$fail_count,
         comp_time = aipw_out_linear$comp_time)

write_csv(aipw_df,  here('files', 'aipw_df.csv'))
write_csv(aipw_df_linear,  here('files', 'aipw_df_linear.csv'))

```



```{r tmle-sim, eval = F}

tmle_sim <- function(dat, seed = 185){
  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  tmle_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #   Y = sim_dat$y, W = sim_dat$d)
      
      tmle_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'TMLE')
      
      tmle_list[[i]] <- data.frame(d = tmle_out[[1]],
                                   se = tmle_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(tmle_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(tmle_list) == n, fail_count, fail_count + (n - length(tmle_list)))
  
  end_time <- Sys.time()
  
 return(list(
      est_df = bind_rows(tmle_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}


tmle_out <- tmle_sim(sim_list)
tmle_out_linear <- tmle_sim(sim_linear)

tmle_df <- tmle_out$est_df %>%
  mutate(fail_count = tmle_out$fail_count,
         comp_time = tmle_out$comp_time)

tmle_df_linear <- tmle_out_linear$est_df %>%
  mutate(fail_count = tmle_out_linear$fail_count,
         comp_time = tmle_out_linear$comp_time)

write_csv(tmle_df, here('files', 'tmle_df.csv'))
write_csv(tmle_df_linear, here('files', 'tmle_df_linear.csv'))
```

```{r dml-sim, eval = F}
dml_sim <- function(dat, seed = 185){
  library(mlr3)
  library(mlr3learners)
  
  start_time <- Sys.time()
  
  set.seed(seed)
  
  n <- length(dat)
  
  dml_list <- list()
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    lgr::get_logger("mlr3")$set_threshold("warn")
    
    learner = lrn("regr.ranger", num.trees=500, 
                  max.depth=5, min.node.size=2)
    ml_l = learner$clone()
    ml_m = learner$clone()

    tryCatch({
      dml_out <- DoubleML::DoubleMLPLR$new(
        DoubleML::DoubleMLData$new(sim_dat,
                                 y_col = 'y',
                                 d_cols = 'd',
                                 x_cols = names(dat[[i]]$x)), 
        ml_l=ml_l, ml_m=ml_m)
      
      dml_out$fit()
      dml_list[[i]] <- data.frame(d = dml_out$all_coef[[1,1]],
                                  se = dml_out$all_se[[1,1]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(dml_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(dml_list) == n, fail_count, fail_count + (n - length(dml_list)))
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(dml_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
      )
}

dml_out <- dml_sim(sim_list)
dml_out_linear <- dml_sim(sim_linear)

dml_df <- dml_out$est_df %>%
  mutate(fail_count = dml_out$fail_count,
         comp_time = dml_out$comp_time)
dml_df_linear <- dml_out_linear$est_df %>%
  mutate(fail_count = dml_out_linear$fail_count,
         comp_time = dml_out_linear$comp_time)

write_csv(dml_df, here('files', 'dml_df.csv'))
write_csv(dml_df_linear, here('files', 'dml_df_linear.csv'))
```

```{r dorie-results}
bind_rows(
  perform(read_csv(here('files', 'lm_df.csv')), 'lm'),
  perform(read_csv(here('files', 'psm_df.csv')), 'psm'),
  perform(read_csv(here('files', 'aipw_df.csv')), 'aipw'),
  perform(read_csv(here('files', 'tmle_df.csv')), 'tmle'),
  perform(read_csv(here('files', 'dml_df.csv')), 'dml')
  ) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 5 replications each. Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.')
```

```{r dorie-results-linear}
bind_rows(
  perform(read_csv(here('files', 'lm_df_linear.csv')), 'lm'),
  perform(read_csv(here('files', 'psm_df_linear.csv')), 'psm'),
  perform(read_csv(here('files', 'aipw_df_linear.csv')), 'aipw'),
  perform(read_csv(here('files', 'tmle_df_linear.csv')), 'tmle'),
  perform(read_csv(here('files', 'dml_df_linear.csv')), 'dml')
  ) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the linear DGP for dataset 3 from Dorie et al. (2019), 5 replications each. Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.')
```

Results are shown in Table \@ref(tab:dorie-results). The methods all have similar levels of bias and error, but AIPW performs better than the other methods on all measures. AIPW did fail, however, on the most data sets; the package `CausalGAM` package does not do well with lack of overlap of factor variables between treated and control groups. 

```{r lalonde-data}
unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()
```


## Comparisons using LaLonde NSW Data

As another benchmark test, I use data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NWS), as provided by @dehejia_1999_causal. The NWS randomly provided training to disadvantaged workers, allowing an experimental estimate of the effect of the intervention. This study had `r nrow(filter(lalonde_exp, treat == T))` treated and `r nrow(filter(lalonde_exp, treat == F))` control participants

LaLonde compared these experimental estimates to estimates comparing the treated group to samples drawn from the Panel Study of Income Dynamics (PSID) and Westat's Matched Current Population Survey-Social Security Administration File (CPS). The PSID-1 sample (*n* = `r nrow(lalonde_psid1_controls)`) contains all male household heads under 55 who did not classify themselves as retired in 1975, and the PSID-3 sample (*n* = `r nrow(lalonde_psid3_controls)`) further restricts this to men who were not working in spring of 1976 or 1975. The CPS-1 sample (*n* = `r nrow(lalonde_cps1_controls)`) includes all CPS males under 55, and CPS-3 (*n* = `r nrow(lalonde_cps3_controls)`) restricts this two those who were not working in March 1976 whose income in 1975 was below the poverty level. Restricting these observational samples is meant to get closer to the population eligible for the NWS.  

Following @dehejia_1999_causal, I present results for the original samples analyzed by @lalonde_1986_evaluating, but I also include results using a subsample of the experimental group that has 1974 earnings data available (`r nrow(filter(lalonde_exp_74, treat == T))` treated and `r nrow(filter(lalonde_exp_74, treat == F))` control participants). 

I again compare the three double robust methods to a linear model fitted by OLS ("lm") and propensity score matching using logistic regression ("psm"). Results are presented in Table \@ref(tab:lalonde).  

For the original NSW data, none of the methods perform especially well. For the PSID samples, none of the estimates have a positive sign, as in the experimental estimates. For CPS, TMLE is the only method that achieves a positive sign.  

Including 1974 earnings data allows much better estimates with the observational control groups. (Note that the restricted experimental sample has a greater experimental effect estimate than the full experimental sample.) For PSID-1, TMLE performs the best, while DML performs much worse than even OLS. For PSID-3, OLS and TMLE both perform well. For CPS-1 and CPS-3, TMLE performs the best.



```{r lalonde-calc, eval = F}
lalonde_func <- function(dataset, sample, include_74 = F){
  set.seed(872)
  
  if(include_74 == F){
    lalonde_variables <- names(lalonde_exp)[3:9]
    dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
    
  } else {
    lalonde_variables <- names(lalonde_exp_74)[3:10]
    dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
  }
  
  dataset <- list(list(y = dataset$re78,
                z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                x = dataset[,lalonde_variables]))
  
  lapply(list(
    lm_sim(dataset),
    psm_sim(dataset),
    aipw_sim(dataset),
    tmle_sim(dataset),
    dml_sim(dataset)),
  bind_cols) %>%
    bind_rows() %>%
    mutate(method = c('lm', 'psm', 'aipw', 'tmle', 'dml'),
           sample = sample,
           estimate = paste0(round(d), ' (', round(se), ')')) %>%
    select(method, sample, estimate) 
}

bind_rows(
  lalonde_func(filter(lalonde_exp, treat == 0), sample = 'experimental'),
  lalonde_func(lalonde_psid1_controls, 'PSID-1'),
  lalonde_func(lalonde_psid3_controls, 'PSID-3'),
  lalonde_func(lalonde_cps1_controls, 'CPS-1'),
  lalonde_func(lalonde_cps1_controls, 'CPS-3')) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde.csv'))


bind_rows(
  lalonde_func(filter(lalonde_exp_74, treat == 0), sample = 'experimental', T),
  lalonde_func(lalonde_psid1_controls, 'PSID-1', T),
  lalonde_func(lalonde_psid3_controls, 'PSID-3', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-1', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-3', T)) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde_74.csv'))
# 
# lalonde_cps1 <- bind_rows(filter(lalonde_exp, treat == 1), lalonde_cps1_controls)
# lalonde_cps3 <- bind_rows(filter(lalonde_exp, treat == 1), lalonde_cps3_controls)
# 
# lalonde_exp <- list(list(y = as.numeric(lalonde_exp$re78),
#                 z = ifelse(lalonde_exp$treat == 1, 'trt', 'ctl'),
#                 x = lalonde_exp[,lalonde_variables]))
# lalonde_cps1 <- list(list(y = lalonde_cps1$re78,
#                 z = ifelse(lalonde_cps1$treat == 1, 'trt', 'ctl'),
#                 x = lalonde_cps1[,lalonde_variables]))
# lalonde_cps3 <- list(list(y = lalonde_cps3$re78,
#                 z = ifelse(lalonde_cps3$treat == 1, 'trt', 'ctl'),
#                 x = lalonde_cps3[,lalonde_variables]))
# 
# 
# set.seed(185)
# lapply(list(
#     lm_sim(lalonde_exp),
#     lm_sim(lalonde_cps1),
#     lm_sim(lalonde_cps3),
#     psm_sim(lalonde_exp),
#     psm_sim(lalonde_cps1),
#     psm_sim(lalonde_cps3),
#     aipw_sim(lalonde_exp),
#     aipw_sim(lalonde_cps1),
#     aipw_sim(lalonde_cps3),
#     tmle_sim(lalonde_exp),
#     tmle_sim(lalonde_cps1),
#     tmle_sim(lalonde_cps3),
#     dml_sim(lalonde_exp),
#     dml_sim(lalonde_cps1),
#     dml_sim(lalonde_cps3)), 
#   bind_cols) %>% 
#   bind_rows()  %>%
#   mutate(method = rep(c('lm', 'psm', 'aipw', 'tmle', 'dml'), each = 3),
#          sample = rep(c('experimental', 'CPS-1', 'CPS-3'), 5),
#          estimate = paste0(round(d), ' (', round(se), ')')) %>%
#   select(method, sample, estimate) %>%
#   pivot_wider(names_from = 'sample', values_from = estimate) %>%
#  write_csv(here('files', 'lalonde_74.csv'))


# lalonde <- CBPS::LaLonde %>%
#   filter(re74.miss == F) %>%
#   mutate(re74_0 = re74 == 0,
#          re75_0 = re75 == 0)
# 
# lalonde_exp <- filter(lalonde, exper == 1)
# lalonde_exp <- list(list(y = lalonde_exp$re78,
#                 z = ifelse(lalonde_exp$treat == 1, 'trt', 'ctl'),
#                 x = select(lalonde_exp, age:re75, re74_0, re75_0)))
# lalonde_psid <- filter(lalonde, (exper == 1 & treat == 1) | exper == 0)
# lalonde_psid <- list(list(y = lalonde_psid$re78,
#                 z = ifelse(lalonde_psid$treat == 1, 'trt', 'ctl'),
#                 x = select(lalonde_psid, age:re75, re74_0, re75_0)))
# 
# 
# lalonde_cps <- MatchIt::lalonde %>%
#   mutate(re74_0 = re74 == 0,
#          re75_0 = re75 == 0)
# lalonde_cps <- list(list(y = lalonde_cps$re78,
#                 z = ifelse(lalonde_cps$treat == 1, 'trt', 'ctl'),
#                 x = select(lalonde_cps, age:re75, re74_0, re75_0)))
# 
# 
# set.seed(185)
# lapply(list(
#     lm_sim(lalonde_exp),
#     lm_sim(lalonde_psid),
#     lm_sim(lalonde_cps),
#     psm_sim(lalonde_exp),
#     psm_sim(lalonde_psid),
#     psm_sim(lalonde_cps),
#     aipw_sim(lalonde_exp),
#     aipw_sim(lalonde_psid),
#     aipw_sim(lalonde_cps),
#     tmle_sim(lalonde_exp),
#     tmle_sim(lalonde_psid),
#     tmle_sim(lalonde_cps),
#     dml_sim(lalonde_exp),
#     dml_sim(lalonde_psid),
#     dml_sim(lalonde_cps)), 
#   bind_cols) %>% 
#   bind_rows()  %>%
#   mutate(method = rep(c('lm', 'psm', 'aipw', 'tmle', 'dml'), each = 3),
#          sample = rep(c('experimental', 'PSID', 'CPS'), 5),
#          estimate = paste0(round(d), ' (', round(se), ')')) %>%
#   select(method, sample, estimate) %>%
#   pivot_wider(names_from = 'sample', values_from = estimate) %>%
#   write_csv(here('files', 'lalonde.csv'))
```



```{r lalonde}
bind_rows(
  read_csv(here('files', 'lalonde.csv')) %>%
    mutate(sample = 'Original LaLonde'),
  read_csv(here('files', 'lalonde_74.csv')) %>%
    mutate(sample = 'With 1974 earnings')) %>%
  select(sample, method, everything()) %>%
  kableExtra::kable(booktabs = T, 
                    # digits = 3,
                    # linesep = '',
                    caption = 'ATE estimates for Lalonde NSW data as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups. Standard errors shown in parentheses.')

# bind_rows(
#   read_csv(here('files', 'lalonde.csv')) %>%
#     mutate(sample = 'LaLonde'),
#   read_csv(here('files', 'lalonde_74.csv')) %>%
#     mutate(sample = 'DW (with 1974 earnings)')) %>%
#   select(sample, everything()) %>%
#   kableExtra::kable(booktabs = T, 
#                     # digits = 3,
#                     linesep = '',
#                     caption = 'ATE estimates for Lalonde NSW data from Dehejia and Wahba (1999), with PSID ')


# lalonde_exp_df <- filter(lalonde, exper == 1)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_exp_df, ATT = TRUE)
# summary(fit)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_exp_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
# 
# 
# lalonde_psid_df <- filter(lalonde, (exper == 1 & treat == 1) | exper == 0)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_psid_df, ATT = TRUE)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_psid_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
```




# References
