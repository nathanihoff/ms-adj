---
output:
  # bookdown::word_document2:
    # reference_docx: "word-template.docx"
  bookdown::pdf_document2:
    toc: no
    keep_tex: true
    number_sections: yes
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
    #latex_engine: xelatex

always_allow_html: true
header-includes:
  #- \usepackage{setspace}\doublespace
  # - \usepackage[nolists, fighead, tabhead]{endfloat}
  # - \usepackage{endnotes}
  # - \let\footnote=\endnote
  # - \setlength{\headheight}{14.5pt}
  # - \setlength{\headheight}{13.6pt}
  # - \usepackage{fancyhdr}
  # - \pagestyle{fancy}
  # - \lhead{N.I. Hoffmann} 
  # - \rhead{`r format(Sys.time(), '%B %e, %Y')`}
# - \newcommand{\beginsupplement}{\setcounter{table}{0}  
# \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} 
# \renewcommand{\thefigure}{A\arabic{figure}}}

editor_options: 
  chunk_output_type: console


citeproc: no
#fontfamily: mathpazo
# fontsize: 12pt
# geometry: margin=.6in
indent: yes
link-citations: yes
linkcolor: blue
lang: 'en-US'

bibliography: "/Users/nathan/Documents/My Library.bib" 
# bibliography: "My Library.bib"  
csl: apa.csl
# csl: american-sociological-association.csl

title: "Double Robust, Flexible Adjustment Methods for Causal Inference: An Overview and an Evaluation"

# author:  Nathan I. Hoffmann, Departments of Sociology and Statistics, UCLA
# date: "`r format(Sys.time(), '%B %e, %Y')`"

# thanks: "I deeply thank my Statistics MS committee -- Chad Hazlett, Jennie E. Brand, and Onyebuchi A. Arah -- for their extensive suggestions and advice on previous drafts. I would also like to thank the members of UCLA's Practical Causal Inference Lab, UCLA's Inequality Data Science Lab, and participants at the 2023 American Sociological Association's Annual Meeting, at the 2024 PolMeth Summer Meeting, and at the 2024 All-UC Demography Conference for their feedback on previous versions of this paper."

abstract: "Double robust methods for flexible covariate adjustment in causal inference have proliferated in recent years. Despite their apparent advantages, these methods are rarely used by social scientists. It is also unclear whether these methods actually outperform more traditional methods in finite samples. This paper has two aims: It is a guide to some of the latest methods in double robust, flexible covariate adjustment using machine learning, and it compares these methods to more traditional statistical methods and flexible \"single robust\" methods using simulated, cross-sectional data where the treatment effect is known. Double robust methods covered include Augmented Inverse Probability Weighting (AIPW), Targeted Maximum Likelihood Estimation (TMLE), and Double/Debiased Machine Learning (DML). Results suggest that some of these methods do outperform traditional methods in a wide range of simulations, but not dramatically. The top performers are TMLE and AIPW in conjunction with flexible machine learning estimators, but G-computation with the same flexible estimators obtains almost identical results, and standard regression methods have only slightly higher bias. Researchers should opt for estimators that are robust to heterogeneous treatment effects, regardless of whether they are double robust."

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T, dev = c('pdf', 'png'), fig.retina = 3, ft.latex.float = 'float')
options("yaml.eval.expr" = TRUE)

library(SuperLearner)
library(broom)
library(knitr)
library(kableExtra)
library(here)
library(patchwork)
library(DoubleML)
library(mlr3)
library(mlr3pipelines)
library(mlr3learners)
library(mlr3extralearners)
library(haven)
library(tidyverse)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


options("yaml.eval.expr" = TRUE, scipen = 3, digits = 2)

uclablue = '#2774AE'
gray = '#808080'
black = '#000000'
ucla_palette = c(black, uclablue, gray)

# theme_set(theme_cowplot(font_family = 'Palatino') + 
theme_set(theme_classic(base_family = 'Palatino') + 
      theme(legend.title=element_blank(), 
         panel.grid.major.y = element_line('grey80'),
         legend.background = element_rect(fill = alpha("white", 0.5))
         ))
ggplot <- function(...) ggplot2::ggplot(...) + 
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")

kable <- function(...) knitr::kable(..., format.args = list(big.mark = ","))
```




```{r load}


unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()
```

```{r functions}
## Pred functions ####
ols_logit_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  mu_mod <- lm(y ~  d + ., data.frame(y, d, x))
  mu1_pred <- predict(mu_mod, newdata = data.frame(y, d = 1, x))
  mu0_pred <- predict(mu_mod, newdata = data.frame(y, d = 0, x))
  
  pi_mod <- glm(d ~ ., data.frame(y, x), family = binomial(link = 'logit'))
  pi_pred <- predict(pi_mod, type = 'response')
  
  # pi_pred <-case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)

  
  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}


grf_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  forest_mu <- grf::regression_forest(X = data.frame(d, x), Y = y, 
                                 tune.parameters = "all")
  mu0_pred <- predict(forest_mu, newdata = data.frame(d = 0, x))$predictions
  mu1_pred <- predict(forest_mu, newdata = data.frame(d = 1, x))$predictions
  
  forest_pi <- grf::regression_forest(X = x, Y = d, tune.parameters = "all")
  pi_pred <- predict(forest_pi, newdata = x)$predictions

  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}

superlearner_pred <- function(y, d, x, folds = 5, seed = 158){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  set.seed(seed)
  mu_fit <- SuperLearner(
    Y = y,
    X = data.frame(d, x),
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= gaussian()
  )
  
  pi_fit <- SuperLearner(
    Y = d,
    X = x,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= binomial()
  )
  

  return(list(
    mu0_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 0, x), type = 'response')$library.predict %*% mu_fit$coef),
    mu1_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 1, x), type = 'response')$library.predict %*% mu_fit$coef),
    pi_pred = as.numeric(predict(pi_fit, type = 'response')$pred),
    d = d,
    y = y))
}

## Methods ####

lm_sim <- function(dat){
  
  n <- length(dat)
  lm_list <- list()
  for(i in 1:n){
    start_time <- Sys.time()
    
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    tryCatch({
      lm_out <- tidy(lm(y ~ d + ., sim_dat))
      ate <- lm_out[[2,2]]
      
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
    
    lm_list[[i]] <- data.frame(dataset = i,
                               ate = ate,
                               #se = lm_out[[2,3]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                               dorie_dataset = dat[[i]]$dataset,
                               set = dat[[i]]$set,
                               size = dat[[i]]$size,
                               comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
    
  }
  
  # fail_count <- sum(sapply(lm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(lm_list) == n, fail_count, fail_count + (n - length(lm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(lm_list))
  
  # return(list(
  #   est_df = bind_rows(lm_list)
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  #   ))
}


psm_sim <- function(dat){
  
  n <- length(dat)
  psm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    
    start_time <- Sys.time()
    
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    
    tryCatch({
      form <- as.formula(paste0('d ~ ', paste(names(dat[[i]]$x), collapse = '+')))
      match_out <- MatchIt::matchit(form,
                             data = sim_dat,
                             method = 'nearest',
                             distance = 'glm') 
      
      # match_data <- MatchIt::match.data(match_out) 
      # apply(match_data, 1, unique)
      form2 <- as.formula(paste0('y ~ d + ', paste(names(dat[[i]]$x), collapse = '+')))
      
    
      psm_out <- lm(form2, 
                    MatchIt::match.data(match_out), 
                    weights = weights) %>%
        tidy()
      
      ate <- psm_out[[2,2]]
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
    
    psm_list[[i]] <- data.frame(dataset = i,
                                ate = ate,
                                # se = psm_out[[2,3]],
                                truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                                dorie_dataset = dat[[i]]$dataset,
                                set = dat[[i]]$set,
                                size = dat[[i]]$size,
                                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
  }
  
  # fail_count <- sum(sapply(psm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(psm_list) == n, fail_count, fail_count + (n - length(psm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(psm_list))
  
  # return(list(
  #   est_df = bind_rows(psm_list),
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  # )
}


aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}

aipw_calc_trunc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, .025) ~ quantile(pi_pred, .025),
  #   pi_pred > quantile(pi_pred, .975) ~ quantile(pi_pred, .975),
  #   T ~ pi_pred)
  
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}


tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(y)
  # H <- (d == 1)/pi_pred - (d==0)/(1-pi_pred)
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  epsilon <- glm(y ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred 
                    + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  H_0 = (1-d)/(1-pi_pred)
  H_1 = d/pi_pred
  
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H_0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H_1))
  
  ATE <- mean((target_1 - target_0), na.rm = T)
  return(ATE)
}

dml_pre <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  n <- length(y)
  n_2 <- n/2
  n_2_1 = ifelse(round(n_2) == n_2, n_2, round(n_2))
  n_2_2 = ifelse(round(n_2) == n_2, n_2, round(n_2)+1)
  
  # split the sample
  random_vec <- sample(1:n, n)
  I <- random_vec[1:n_2_1]
  I_c <- random_vec[(n_2_1+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,], 
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

dml_post <- function(y_I, d_I, x_I = NULL, y_I_c, d_I_c, x_I_c = NULL,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

## Predictor functions
ols_logit_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- lm(y ~ ., data.frame(y = y_I_c, x_I_c))
  mu_pred1 <- predict(mu_mod1, newdata = data.frame(y = y_I, x_I))

  pi_mod1 <- glm(d ~ ., data.frame(d = d_I_c, x_I_c), 
                family = binomial(link = 'logit'))
  pi_pred1 <- predict(pi_mod1, 
                     newdata = data.frame(d = d_I, x_I), 
                     type = 'response')
  
  mu_mod2 <- lm(y ~ ., data.frame(y = y_I, x_I))
  mu_pred2 <- predict(mu_mod2, newdata = data.frame(y = y_I_c, x_I_c))

  pi_mod2 <- glm(d ~ ., data.frame(d = d_I, x_I), 
                family = binomial(link = 'logit'))
  pi_pred2 <- predict(pi_mod2, 
                     newdata = data.frame(d = d_I_c, x_I_c), 
                     type = 'response')
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, 
                                       tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, 
                                       tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

superlearner_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                             folds = 5){
  
  
  mu_mod1 <- SuperLearner(
    Y = y_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod1 <- SuperLearner(
    Y = d_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  
  mu_pred1 <- predict(mu_mod1, newdata = x_I, type = 'response')$library.predict %*% mu_mod1$coef
  pi_pred1 <- predict(pi_mod1, newdata = x_I, type = 'response')$pred
  
  mu_mod2 <- SuperLearner(
    Y = y_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod2 <- SuperLearner(
    Y = d_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c, type = 'response')$library.predict %*% mu_mod2$coef
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c, type = 'response')$pred
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}


lm_lin_continuous <- function (formula, covariates, data, weights, subset, clusters, 
  se_type = NULL, ci = TRUE, alpha = 0.05, return_vcov = TRUE, 
  try_cholesky = FALSE) 
{
  if (length(all.vars(f_rhs(formula))) > 1) {
    stop("The `formula` argument, `", format(formula), "`, must only have the ", 
      "treatment variable on the right-hand side of the formula. Covariates ", 
      "should be specified in the `covariates` argument like so:\n`covariates = ", 
      paste0("~ ", paste(all.vars(f_rhs(formula))[-1], 
        sep = " + ")), "`.", "\n\n See ?lm_lin.")
  }
  if (!inherits(covariates, "formula")) {
    stop("The `covariates` argument must be specified as a formula:\n", 
      "You passed an object of class ", class(covariates))
  }
  cov_terms <- terms(covariates)
  if (attr(cov_terms, "response") != 0) {
    stop("Must not specify a response variable in `covariates`` formula.\n", 
      "`covariates` must be a right-sided formula, such as '~ x1 + x2 + x3'")
  }
  if (length(attr(cov_terms, "order")) == 0) {
    stop("`covariates` must have a variable on the right-hand side, not 0 or 1")
  }
  full_formula <- update(formula, reformulate(c(".", labels(cov_terms))))
  datargs <- enquos(formula = full_formula, weights = weights, 
    subset = subset, cluster = clusters)
  data <- enquo(data)
  model_data <- clean_model_data(data = data, datargs)
  outcome <- as.matrix(model_data$outcome)
  n <- nrow(outcome)
  design_matrix <- model_data$design_matrix
  weights <- model_data$weights
  cluster <- model_data$cluster
  has_intercept <- attr(terms(formula), "intercept")
  treat_col <- which(attr(design_matrix, "assign") == 1)
  treatment <- design_matrix[, treat_col, drop = FALSE]
  design_mat_treatment <- colnames(design_matrix)[treat_col]
  # if (any(!(treatment %in% c(0, 1)))) {
  #   vals <- sort(unique(treatment))
  #   if (has_intercept) 
  #     vals <- vals[-1]
  #   n_treats <- length(vals)
  #   names(vals) <- paste0(colnames(design_matrix)[treat_col], 
  #     vals)
  #   treatment <- outer(drop(treatment), vals, function(x, 
  #     y) as.numeric(x == y))
  # }
  demeaned_covars <- design_matrix[, setdiff(colnames(design_matrix), 
    c(design_mat_treatment, "(Intercept)")), drop = FALSE]
  if (is.numeric(weights)) {
    center <- apply(demeaned_covars, 2, weighted.mean, weights)
  }
  else {
    center <- colMeans(demeaned_covars)
  }
  demeaned_covars <- sweep(demeaned_covars, 2, center)
  original_covar_names <- colnames(demeaned_covars)
  colnames(demeaned_covars) <- paste0(ifelse(grepl("\\:|(^.+\\()", 
    colnames(demeaned_covars)), paste0("(", colnames(demeaned_covars), 
    ")"), colnames(demeaned_covars)), "_c")
  n_treat_cols <- ncol(treatment)
  n_covars <- ncol(demeaned_covars)
  n_int_covar_cols <- n_covars * (n_treat_cols)
  interacted_covars <- matrix(0, nrow = n, ncol = n_int_covar_cols)
  interacted_covars_names <- character(n_int_covar_cols)
  for (i in 1:n_covars) {
    covar_name <- colnames(demeaned_covars)[i]
    cols <- (i - 1) * n_treat_cols + (1:n_treat_cols)
    interacted_covars[, cols] <- treatment * demeaned_covars[, 
      i]
    interacted_covars_names[cols] <- paste0(colnames(treatment), 
      ":", covar_name)
  }
  colnames(interacted_covars) <- interacted_covars_names
  if (has_intercept) {
    X <- cbind(matrix(1, nrow = n, ncol = 1, dimnames = list(NULL, 
      "(Intercept)")), treatment, demeaned_covars, interacted_covars)
  }
  else {
    if (n_treat_cols == 1) {
      X <- cbind(treatment, demeaned_covars, interacted_covars)
    }
    else {
      X <- cbind(treatment, interacted_covars)
    }
  }
  return_list <- lm_robust_fit(y = outcome, X = X, weights = weights, 
    cluster = cluster, ci = ci, se_type = se_type, alpha = alpha, 
    return_vcov = return_vcov, try_cholesky = try_cholesky, 
    has_int = has_intercept, iv_stage = list(0))
  return_list <- lm_return(return_list, model_data = model_data, 
    formula = formula)
  return_list[["scaled_center"]] <- center
  setNames(return_list[["scaled_center"]], original_covar_names)
  return_list[["call"]] <- match.call()
  return(return_list)
}

environment(lm_lin_continuous) <- asNamespace('estimatr')
assignInNamespace("lm_lin", lm_lin_continuous, ns = "estimatr")



## Functions using double robust packages ####
aipw_sim <- function(dat, seed = 185){

  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  aipw_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = as.numeric(ifelse(dat[[i]]$z == 'trt', 1, 0)), 
                          dat[[i]]$x)
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, 
                                   W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #                              Y = sim_dat$y, W = sim_dat$d)
      
      aipw_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'AIPW')
      
      aipw_list[[i]] <- data.frame(d = aipw_out[[1]],
                                   se = aipw_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(aipw_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(aipw_list) == n, fail_count, fail_count + (n - length(aipw_list)))
  
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(aipw_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

tmle_sim <- function(dat, seed = 185){
  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  tmle_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #   Y = sim_dat$y, W = sim_dat$d)
      
      tmle_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'TMLE')
      
      tmle_list[[i]] <- data.frame(d = tmle_out[[1]],
                                   se = tmle_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(tmle_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(tmle_list) == n, fail_count, fail_count + (n - length(tmle_list)))
  
  end_time <- Sys.time()
  
 return(list(
      est_df = bind_rows(tmle_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

dml_sim <- function(dat, seed = 185){
  
  start_time <- Sys.time()
  
  set.seed(seed)
  
  n <- length(dat)
  
  dml_list <- list()
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    lgr::get_logger("mlr3")$set_threshold("warn")
    
    learner = lrn("regr.ranger", num.trees=500, 
                  max.depth=5, min.node.size=2)
    ml_l = learner$clone()
    ml_m = learner$clone()

    tryCatch({
      dml_out <- DoubleML::DoubleMLPLR$new(
        DoubleML::DoubleMLData$new(sim_dat,
                                 y_col = 'y',
                                 d_cols = 'd',
                                 x_cols = names(dat[[i]]$x)), 
        ml_l=ml_l, ml_m=ml_m)
      
      dml_out$fit()
      dml_list[[i]] <- data.frame(d = dml_out$all_coef[[1,1]],
                                  se = dml_out$all_se[[1,1]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(dml_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(dml_list) == n, fail_count, fail_count + (n - length(dml_list)))
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(dml_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
      )
}



## Other functions ####
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

perform <- function(est_df, label){
  est_df %>%
    # mutate(d = d/truth,
    #        truth = 1) %>%
    summarize(bias = mean(d - truth),
              percent_bias = bias/sd(d),
              rmse = sqrt(mean((ate - truth)^2)),
              mae = median(abs(d - truth))
              # fail_count = first(fail_count),
              # comp_time = first(comp_time)
              ) %>%
    mutate(label = label,
           n = nrow(est_df) + fail_count) %>%
    select(label, everything()) %>%
    return()
}

model_matrix <- function(...){
  options(na.action='na.pass')
  matrix_out <- model.matrix(...)
  options(na.action = 'na.omit')
  return(matrix_out)
}

p_val <- function(estimate, std.error){
  z <- estimate / std.error
  2*(1 - pnorm(abs(as.numeric(z))))
}

compare_tbl <- function(model_list, treatment, tidy = T){
  out_list <- list()
  k <- 1
  for(model_set in model_list){
    names(model_set$grf) <- c('estimate', 'std.error')
    
    if(tidy == T){
    original_model <- tidy(model_set$original) %>%
      filter(term == treatment)
    } else{
      original_model <- model_set$original
    }
    
    out_list[[k]] <- original_model %>%
      bind_rows(bind_rows(model_set$grf)) %>%
      bind_rows(model_set$dml) %>%
      bind_rows(model_set$dml_cre) %>%
      bind_rows(model_set$dml_wg_cre) %>%
      mutate(term = c('Original', 'AIPW (GRF)', 'DML (SuperLearner)', 
                      'DML (SuperLearner), CRE FE', 'DML (SuperLearner), hybrid FE'), 
             p.value = p_val(estimate, std.error)) %>%
      remove_rownames() %>%
      tibble()
    
    k <- k+1
  }
  return(out_list)
}

# compare_tbl_notidy <- function(model_list){
#   out_list <- list()
#   k <- 1
#   for(model_set in model_list){
#     names(model_set[[2]]) <- c('estimate', 'std.error')
#     
#     
#     out_list[[k]] <- model_set[[1]] %>%
#       bind_rows(bind_rows(model_set[[2]])) %>%
#       bind_rows(model_set[[3]]) %>%
#       mutate(term = c('Original', 'AIPW (GRF)', 'DML (SuperLearner)'), p.value = p_val(estimate, std.error)) %>%
#       tibble()
#     
#     k <- k+1
#   }
#   return(out_list)
# }

dml_grf <- function(outcome, treatment, covariates, clustervar = NULL, dataset, 
                    fe = T, target = 'treated', paper, model, drop_na_grf = F, seed = 123){
  set.seed(seed)
  
  dataset <- filter(dataset, !is.na(!!sym(outcome))) %>%
    select(all_of(c(treatment, outcome, covariates, clustervar)))
  
  if(drop_na_grf == T){
    dataset <- dataset %>%
      drop_na()
  }
  
  y <- as.numeric(dataset[[outcome]])
  X <- model_matrix(~., select(dataset, all_of(covariates)))[,]
  d <- as.numeric(dataset[[treatment]])
  
  grf_model <- grf::causal_forest(X = X, Y = y, W = d, seed = 123,
                        clusters = dataset[[clustervar]])
  
  grf_out <- grf::average_treatment_effect(grf_model, target.sample = target)
  
  
  dataset_drop_na <- as.data.frame(model_matrix(~., dataset)[,]) %>%
    # remove annoying characters
    rename_with(., ~ gsub("'", "", iconv(.x, from = "UTF-8", to='ASCII//TRANSLIT'))) %>%
    rename_with(., ~ gsub("\\[|\\]|\\/|\\*|\\)|\\(", "", .x)) %>%
    rename_with(., ~ gsub(" ", "", .x)) %>%
    # drop missing
    drop_na() %>%
    # remove uninformative columns
    select(where(~n_distinct(.) > 1)) 
  dataset_drop_na <- dataset_drop_na %>%
    left_join(dataset_drop_na %>%
                group_by(!!sym(clustervar)) %>%
                summarize(across(everything(), mean, .names = 'mean_{.col}')) %>%
                ungroup()) %>%
    mutate(intercept = 1)
  # covariates_design <- dataset_drop_na %>% 
  #   select(select(-c(treatment, outcome, clustervar))) %>%
  #   names() %>%
  #   append('intercept', after = 0)
  covariates_design <- names(dataset_drop_na)[!(names(dataset_drop_na) %in% c(treatment, outcome, clustervar)) &
                                                !str_detect(names(dataset_drop_na),  'mean_')]
  
  
  
  # dataset_drop_na$cov_mean <- dataset_drop_na %>%
  #   select(all_of(covariates_design)) %>%
  #   mutate_all(scale) %>%
  #   apply(1, mean)


  # dml_dataframe <- as.data.frame(cbind(y, d, 
  #                                      d_bar = dataset_drop_na[[paste0('mean_', treatment)]],
  #                                      as_tibble(X)[,-1], 
  #                                      as_tibble(mean_X)[,-1], 
  #                                      cluster = dataset_drop_na[[clustervar]]))
  
  
  graph_ensemble_regr = gunion(list(
      po("learner", lrn("regr.cv_glmnet", s = "lambda.min")),
      po("learner", lrn('regr.xgboost', max_depth = 4)),
      po("learner", lrn("regr.glm"))
    )) %>>%
      po("regravg", 3)
  
  ensemble_pipe_regr = as_learner(graph_ensemble_regr)
  
  # DML
  set.seed(seed)
  dml_data <- double_ml_data_from_data_frame(dataset_drop_na,
                                             x_cols = covariates_design,
                                             y_col = outcome,
                                             d_cols = treatment,
                                             cluster_cols = clustervar)
  
  # y <- as.numeric(dataset_drop_na[[outcome]])
  # # X <- select(dataset_drop_na, covariates_design)
  # X <- model_matrix(~., select(dataset_drop_na, all_of(covariates)))[,]
  # # mean_X <- model_matrix(~., select(dataset_drop_na, all_of(paste0('mean_', covariates))))[,]
  # d <- as.numeric(dataset_drop_na[[treatment]])
  # 
  # dml_data <- double_ml_data_from_matrix(X = X, y = y, d = d, 
  #                                        cluster_vars = dataset_drop_na[[clustervar]])
  
  # dml_data <- double_ml_data_from_matrix(X = select(dataset_drop_na, all_of(covariates_design)),
  #                                            y = dataset_drop_na[,outcome],
  #                                            d = dataset_drop_na[,treatment],
  #                                            cluster_vars = dataset_drop_na[,clustervar])
  obj_dml_plr_sim_pipe_ensemble = DoubleMLPLR$new(dml_data,
                                                  ml_l = ensemble_pipe_regr,
                                                  ml_m = ensemble_pipe_regr)
  obj_dml_plr_sim_pipe_ensemble$fit() 
  dml_out <- data.frame(estimate = obj_dml_plr_sim_pipe_ensemble$coef,
                         std.error = obj_dml_plr_sim_pipe_ensemble$se)
  
  if(fe == T){
    # DML CRE
    set.seed(seed)  
    dml_data_cre <- XTDML::dml_cre_data_from_data_frame(dataset_drop_na,
                                                 x_cols = covariates_design,
                                                 y_col = outcome,
                                                 d_cols = treatment,
                                                 xbar_cols = paste0('mean_', covariates_design[-length(covariates_design)]),
                                                 dbar_cols = paste0('mean_', treatment),
                                                 cluster_cols = clustervar)  
    obj_dml_cre = XTDML::dml_cre_plr$new(dml_data_cre,
                                         ml_l = ensemble_pipe_regr,
                                         ml_m = ensemble_pipe_regr)
    
    obj_dml_cre$fit()
    dml_cre_out <- data.frame(estimate = obj_dml_cre$coef_theta,
                           std.error = obj_dml_cre$se_theta)
  
  # DML WG-CRE
  require(parameters)
  set.seed(seed)  
  dml_data_wg_cre <- XTDML::dml_hybrid_data_from_data_frame(dataset_drop_na,
                                               x_cols = covariates_design,
                                               y_col = outcome,
                                               d_cols = treatment,
                                               xbar_cols = paste0('mean_', covariates_design[-length(covariates_design)]),
                                               dbar_cols = paste0('mean_', treatment),
                                               cluster_cols = clustervar)  
  obj_dml_wg_cre = XTDML::dml_hybrid_plr$new(dml_data_wg_cre,
                                       ml_l = ensemble_pipe_regr,
                                       ml_m = ensemble_pipe_regr)
  
  obj_dml_wg_cre$fit()
  dml_wg_cre_out <- data.frame(estimate = obj_dml_wg_cre$coef_theta,
                         std.error = obj_dml_wg_cre$se_theta)
  } else {
    dml_cre_out <- data.frame(estimate = NA,
                           std.error = NA)
    dml_wg_cre_out <- data.frame(estimate = NA,
                         std.error = NA)
  }
  
  return(list(grf = grf_out, 
              dml = dml_out,
              dml_cre = dml_cre_out,
              dml_wg_cre = dml_wg_cre_out))
}

```



```{r load-sims, eval = F}
lalonde_bootstrap <- function(dataset, sample, include_74 = F, iter = 100){
  
  lalonde_variables <- c('age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75', 're75_0')
  if(include_74 == F){
        dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
        
      } else {
        lalonde_variables <- c(lalonde_variables, 're74', 're74_0')
        dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
      }
  
  bootstrap_list <- list()
  
  for(i in 1:iter){
    dataset_bootstrap <- sample_n(dataset, nrow(dataset), replace = T)
    
    bootstrap_list[[i]] <- list(y = dataset_bootstrap$re78,
                  z = ifelse(dataset_bootstrap$treat == 1, 'trt', 'ctl'),
                  x = dataset_bootstrap[,lalonde_variables],
                  y.1 = NA, y.0 = NA,
                  dataset = i,
                  set = sample,
                  size = nrow(dataset_bootstrap))
  }
  
  return(bootstrap_list)
}

lalonde_setup <- function(dataset, sample, include_74 = F){
  
  lalonde_variables <- c('age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75', 're75_0')
  if(include_74 == F){
        dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
        
      } else {
        lalonde_variables <- c(lalonde_variables, 're74', 're74_0')
        dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
      }
  

    
    list(y = dataset$re78,
                  z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                  x = dataset[,lalonde_variables],
                  y.1 = NA, y.0 = NA,
                  dataset = i,
                  set = sample,
                  size = nrow(dataset)) %>%
      return()
}

set.seed(1859)
sims_lalonde <- c(
  lalonde_bootstrap(filter(lalonde_exp, treat == 0), sample = 'lalonde experimental original'),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 original'),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 original'),
  lalonde_bootstrap(filter(lalonde_exp_74, treat == 0), sample = 'lalonde experimental 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 74', include_74 = T))


# sims <- readRDS(here('files', 'sims.RDS'))

sims_main <- readRDS(here('files_main', 'sims.RDS'))
sims_large <- readRDS(here('files_large', 'sims_large.RDS'))
sims_small <- readRDS(here('files_small', 'sims_small.RDS'))

sims <- sims_main %>% append(sims_small) %>% append(sims_large) %>% append(sims_lalonde)


## Full lalonde
# sims <- list(
#   lalonde_setup(filter(lalonde_exp, treat == 0), sample = 'lalonde experimental original'),
#   lalonde_setup(lalonde_psid1_controls, 'lalonde PSID-1 original'),
#   lalonde_setup(lalonde_psid3_controls, 'lalonde PSID-3 original'),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-1 original'),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-3 original'),
#   lalonde_setup(filter(lalonde_exp_74, treat == 0), sample = 'lalonde experimental 74', include_74 = T),
#   lalonde_setup(lalonde_psid1_controls, 'lalonde PSID-1 74', include_74 = T),
#   lalonde_setup(lalonde_psid3_controls, 'lalonde PSID-3 74', include_74 = T),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-1 74', include_74 = T),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-3 74', include_74 = T))

# sims <- sims %>% append(sims_large)

# sims <- readRDS(here('files', 'sims_small.RDS'))
 
# for(i in 1:length(sims_small)){
#   # sims_large[[i]]$size <- nrow(sims_large[[i]]$x)
#   sims_small[[i]]$dataset <- 7
# }
# saveRDS(sims_large, here('files_large', 'sims_large.RDS'))
# 
# 
# sims_combined <- sims %>% append(sims_small) %>% append(sims_large)
# 
# index_df <- data.frame(dorie_dataset = sapply(sims_combined, function(x) x$dataset),
#                        set = sapply(sims_combined, function(x) x$set),
#                        size = sapply(sims_combined, function(x) x$size)) %>%
#   mutate(dataset = c(1:length(sims), 1:length(sims_small), 1:length(sims_large)))
# 
# lm_df <- read_csv(here('files_main', 'lm_df.csv')) %>% 
#   mutate(set = c(rep('main', 200), rep('linear', 200))) %>%
#   bind_rows(read_csv(here('files_small', 'lm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'lm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# 
# write_csv(lm_df, here('files', 'lm_df.csv'))
# 
# psm_df <- read_csv(here('files_main', 'psm_df.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small', 'psm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'psm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(lm_df, here('files', 'psm_df.csv'))
# 
# 
# aipw_df <- read_csv(here('files_main/homemade', 'aipw.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_df, here('files', 'aipw.csv'))
# 
# 
# aipw_trunc_df <- read_csv(here('files_main/homemade', 'aipw_trunc.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_trunc_df, here('files', 'aipw_trunc.csv'))
# 
# tmle_df <- read_csv(here('files_main/homemade', 'tmle.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'tmle.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'tmle.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(tmle_df, here('files', 'tmle.csv'))
# 
# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))



## add lalonde ####
# read_csv(here('files_combined', 'lm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lm_df.csv'))) %>%
#   write_csv(here('files', 'lm_df.csv'))
# 
# 
# read_csv(here('files_combined', 'psm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'psm_df.csv'))) %>%
#   write_csv(here('files', 'psm_df.csv'))
# 
# read_csv(here('files_combined', 'lin.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lin.csv'))) %>%
#   write_csv(here('files', 'lin.csv'))
# 
# read_csv(here('files_combined', 'gcomp.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'gcomp.csv'))) %>%
#   write_csv(here('files', 'gcomp.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw.csv'))) %>%
#   write_csv(here('files', 'aipw.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw_trunc.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw_trunc.csv'))) %>%
#   write_csv(here('files', 'aipw_trunc.csv'))
# 
# read_csv(here('files_combined/homemade', 'tmle.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'tmle.csv'))) %>%
#   write_csv(here('files', 'tmle.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_ols_logit.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_ols_logit.csv'))) %>%
#   write_csv(here('files', 'dml_ols_logit.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_grf.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_grf.csv'))) %>%
#   write_csv(here('files', 'dml_grf.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_superlearner.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_superlearner.csv'))) %>%
#   write_csv(here('files', 'dml_superlearner.csv'))
# 
# 
# readRDS(here('files_combined/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'ols_logit_pred.RDS'))) %>%
#   saveRDS(here('files', 'ols_logit_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'grf_pred.RDS'))) %>%
#   saveRDS(here('files', 'grf_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'superlearner_pred.RDS'))) %>%
#   saveRDS(here('files', 'superlearner_pred.RDS'))




# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))

```

**Keywords**: causal inference, machine learning, double robust, computational methods, simulations


# Introduction

In causal inference, functional form misspecification of underlying models can bias estimates of treatment effects [@hernan_2020_causal; @morgan_2015_counterfactuals]. There have been two important advances that attempt to overcome this. First, methodologists have developed machine learning methods that allow greater flexibility in estimation, adjusting for covariates in data-driven, complex ways [@balzer_2021_invited; @brand_2023_recent]. The second development is double robust methods [@bang_2005_doubly; @kang_2007_demystifying], which estimate two models: one for treatment exposure and another for the outcome. These models are robust to misspecification of either one of these "nuisance" models.  

Methods unifying these two developments have proliferated. These double robust methods for flexible covariate adjustment use machine learning methods to adapatively model the data generating processes. These models purportedly overcome the shortcomings of both traditional statistical methods and machine learning methods. Common statistical methods -- such as OLS regression and matching on propensity scores estimated from logistic regression -- have rigid functional form assumptions and fail to calculate stable estimates when the number of covariates is large relative to the number of observations. Machine learning methods, on the other hand, are often difficult to interpret. They can also suffer from overfitting, where the flexibility of the model becomes a weakness and predictions out-of-sample are poor, yet efforts to correct for overfitting can introduce regularization bias [@hastie_2009_elements]. Double robust methods with machine learning dispose of the constricting functional form assumptions of common statistical methods, and they correct for the regularization bias of flexible machine learning methods. They can also accommodate large numbers of covariates and produce easily interpretable treatment effect estimates. Despite their apparent advantages, these methods remain rarely used by social scientists. Part of the barrier has been lack of familiarity with these methods. It has also been unclear how these methods compare, or whether such methods actually perform better than traditional methods in finite samples.  

This paper makes advances on these fronts. First, it is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, explaining the methods to a social scientist audience. Methods covered include Augmented Inverse Probability Weighting (AIPW), Targeted Maximum Likelihood Estimation (TMLE), and Double or Debiased Machine Learning (DML). This paper reviews the theory behind these methods as well as simple R implementations. 

Second, this paper evaluates these methods, testing them on simulations from @dorie_2019_automated that cover a range of data-generating processes where ignorability holds -- meaning there are no unmeasured confounders. In the simulations, double robust methods are compared to "single robust" methods (i.e., ones with one nuisance model), including traditional or simpler statistical methods commonly used by social scientists: ordinary least squares (OLS) regression, matching on propensity scores estimated from logistic regression (PSM), and inverse probability weighting (IPW). They are also compared to two more flexible methods that may overcome the misspecification issues that motivate double robust methods: G-computation and the Lin estimator. G-computation models the outcome in separate models for treated and untreated units, then predicts the average difference between these for the full sample [@robins_1986_new]. The Lin estimator interacts treatment with mean-centered covariates in an OLS regression [@lin_2013_agnostic].  

Results show that some double robust methods outperform traditional statistical methods, but not dramatically. AIPW and TMLE perform the best, at least when used in conjunction with flexible machine learning algorithms, while DML does slightly worse than traditional methods, at least in its partially linear form that is not robust to heterogeneous treatment effects. G-computation with flexible machine learning performs as well as the lowest-error double robust methods. With its still relatively low error and much faster computation time, OLS remains a sensible choice as an estimator, and the Lin estimator -- which is as quick as standard OLS regression -- performs only slightly worse than the top machine learning methods. Due to the strong performance of G-computation and the Lin estimator, results suggest that using estimators that are robust to heterogeneous treatment effects may be more important than double robustness.  

The next section reviews literature on and motivation for double robust methods. This is followed by an introduction of notation and assumptions, while the following section gives an overview of the double robust methods covered in this paper. I next present results of the simulations, then conclude. In the Appendix, I provide simple R code to implement the three double robust methods.

<!-- Section \@ref(motivation) reviews literature on and motivation for double robust methods. Section \@ref(conceptual-overview) introduces notation and assumptions, while Section \@ref(overview-of-techniques) gives an overview of the double robust methods covered in this paper, along with example R code. Sections \@ref(simulations) and \@ref(replications) present results of the simulations and replications, respectively, and Section \@ref(conclusion) concludes. -->


# Background

## Literature Review

<!-- Although previous work has compared different methods for covariate adjustment, there has not yet been a thorough comparison and evaluation of the recent and popular double robust methods of AIPW, TMLE, and DML for a social scientist audience.  -->

Although some introductions to double robust methods exist, they do not discuss them in the context of covariate adjustment for causal inference, or their treatment is overly technical for a social scientist audience. For example, @kang_2007_demystifying provide an an excellent overview and evaluation of double robust methods, but in the context of missing data, and the authors consider AIPW but not TMLE or DML. @bang_2005_doubly introduce double robust models for both causal inference and missing data, but their treatment is rather technical, and they only discuss AIPW. @lundberg_2022_researcher provide brief schematic overviews of double robust methods for a social scientist audience, but they do not evaluate these methods.

Existing evaluations of double robust methods have focused on only one double robust method and compared it to few traditional statistical methods. @dorie_2019_automated compare estimations from a number of different flexible methods, but these do not include AIPW or DML. @chatton_2020_gcomputation compare four methods -- G-computation, IPW, full matching, and TMLE -- but the authors only consider one double robust method, and their focus is on omitted variable bias rather than determining which method is the most useful. @cousineau_2022_estimating evaluate the performance of optimization-based methods for causal inference, but these do not include the double robust methods covered in the current paper. @knaus_2022_double reviews DML-based methods in an econometrics setting but does not compare them to traditional statistical methods for covariate adjustment.  

An evaluation of multiple double robust methods that compares them to traditional statistical methods as well as flexible "single robust" methods is needed to understand just how practically useful these methods are for social scientists. This paper does this as well as provides a gentle introduction to these methods.  


## Historical Overview 

According to @bang_2005_doubly, double robust methods have their origins in missing data models. @robins_1994_estimation and @rotnitzky_1998_semiparametric developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models. Drawing on the fact that causal inference is fundamentally a missing data problem, @scharfstein_1999_adjusting showed that AIPW was double robust and extended it to causal inference.  

But @kang_2007_demystifying argue that double robust methods are older. They cite work by @cassel_1976_results, who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated. Arguably, double robust methods go back even further than this. The form of double robust methods is similar to residual-on-residual regression, which dates back to the Frisch-Waugh-Lowell (FWL) theorem [@frisch_1933_partial; @lovell_1963_seasonal]:
$$\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)},$$
where $\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$. This formulation writes the regression coefficient as composed of an outcome model ($\tilde Y_i$) and exposure model ($\tilde D_i$), the two models used in double robust estimators. Of the methods considered in this paper, double machine learning (DML) makes this connection most explicit by using residual-on-residual regression as part of its estimation strategy.  

There are also links between double robust methods and matching with regression adjustment. This work goes back at least as far as @rubin_1973_use, who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.  

Today, double robust methods abound [e.g. @arkhangelsky_2021_doublerobust; @ratkovic_2023_relaxing; @sloczynski_2018_general; @dukes_2022_doubly; @kennedy_2023_semiparametric; @xu_2024_relaxed]. Although double robust methods exist for instrumental variables [@okui_2012_doubly; @wang_2018_bounded], difference-in-differences [@santanna_2020_doubly], longitudinal data [@yu_2006_double; @tran_2019_double; @clarke_2024_double], and other causal applications, this paper focuses on three of the most popular and foundational methods for covariate adjustment in a cross-sectional setting.

## Aims of Double Robust Methods
Double robust methods for covariate adjustment aim to overcome what many consider to be the shortcomings of both traditional statistical methods and flexible machine learning methods [@diaz_2020_machine]. Statistical methods that are popular with social scientists -- such as OLS regression and matching on propensity scores from logistic regression -- have two main weaknesses that double robust methods address. First, they assume simple (linear or transformed linear) functional forms. In the presence of highly nonlinear data generating processes, they may provide biased estimates. Second, these methods cannot handle large numbers of covariates relative to sample size, i.e. sparsity. While some machine learning methods can produce estimates even when the number of covariates exceeds the number of observations (such as lasso), OLS fails in this case due to the $X^\top X$ matrix not being of full rank and hence not invertible. In cases with many covariates, but not more than the number of observations, estimation is unstable with many traditional statistical methods.  

Flexible machine learning methods also have their drawbacks. First, naive application of these methods can result in overfitting, with predictive accuracy maximized in sample but treatment effect estimation being biased. When regularization is used to correct for overfitting, the resulting estimates can be biased, a phenomenon called "regularization bias." Furthermore, machine learning methods have often been developed with a focus on prediction rather than on producing treatment effect point estimates, so results can be difficult to interpret without further processing.  

Double robust methods attempt to overcome the downsides of both traditional and machine learning methods by incorporating flexible models into a framework that avoids overfitting and regularization bias and provides easily interpretable estimates. These methods are also motivated by the idea that many older methods ignore information present in the data. Methods tend to model either only the outcome -- as in OLS regression and G-computation -- or only the treatment assignment -- as in propensity score matching or inverse probability weighting. Double robust methods, on the other hand, model both of these.  


# Conceptual Overview

Double robust methods estimate two models: an *outcome model*:
\begin{equation}
\mu_d( \mathbf X_i) = E(Y_i \mid D_i = d, \mathbf X_i)
(\#eq:outcome)
\end{equation}
and an *exposure model* (or treatment or propensity score model):
\begin{equation}
\pi(\mathbf X_i) = E(D_i \mid \mathbf X_i),
(\#eq:exposure)
\end{equation}
where $\mu_d(\cdot)$ is a model of the outcome, $D_i = d_i \in \{0, 1\}$ is the treatment assignment (where 0 is control and 1 is treated), $\mathbf X_i$ is a vector of covariates for unit $i = 1, \ldots, N$, $Y_i$ is the outcome, and $\pi(\cdot)$ is a model of the exposure. The covariates included in $\mathbf X_i$ can be different for the two models.  

Examples of outcomes $Y$ that we might be interested in include earnings, mortality, or degree completion, while possible treatments $D$ might be a job training program, a medication or medical procedure, or an education grant. We attempt to model both of these by controlling for predictors $\mathbf X$ -- such as socioeconomic status, employment history, medical conditions, or race -- that might influence selection into treatment as well as the value of the outcome. When variables in $\mathbf X$ influence both treatment and outcome, they are called confounders, and adjusting for these is essential for unbiased estimation. When the set of observable variables $\mathbf X$ includes all confounders, we say that "ignorability" or "selection on observables" holds. In the simulations below, we assume ignorability.  

The focus of this paper is on the average treatment effect (ATE), which under the potential outcomes framework [@rubin_1974_estimating] is defined as
$$\tau = E[Y_i(1) - Y_i(0)],$$
where $Y_i(1)$ and $Y_i(0)$ are the potential outcomes of $Y_i$ under treatment and control, respectively. An estimator is called "double robust" if it achieves consistent estimation of the ATE (or whatever estimand the researcher is interested in) as long as *at least one* of Equations \@ref(eq:outcome) or \@ref(eq:exposure) is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  

It is important to consider what is meant by a "correct" model specification [@keil_2018_resolving]. These estimators are robust from a statistical standpoint, but not necessarily a causal identification one. The researcher must know which variables are possible confounders and to include them in the appropriate models, while not including colliders or mediators [@hunermund_2023_double]. The simulations discussed in this paper assume conditional ignorability; rather than testing what happens when models are missing important covariates, it focuses on accurate specification of the functional form of the treatment and outcome models.    


## Assumptions

Most double robust methods require almost all of the standard assumptions necessary for most methods that depend on selection on observables. Although some double robust methods relax one or two of these, the methods discussed in this paper rely on six standard assumptions when estimating the ATE.    

1. Consistency: $Y_i(d) = Y_i \mid D_i = d$, i.e. under treatment (control), we observe the potential outcome under treatment (control).   

2. One version of treatment: All treated units receive the same version of treatment.  

3. No interference: $Y_i(D_i, D_j) = Y_i(D_i)$, i.e. the potential outcome for one unit depends only on its own treatment, not the value of other units' treatment.  

4. Positivity/overlap: $0 < \Pr(D = 1 \mid \mathbf X= \mathbf x) < 1$ for all values of $X$, i.e. there is non-zero probability of receiving treatment or control for every combination of covariates in the data. This means we can find at least one control unit to compare every treated unit to (and vice versa).  

5. Independent and identically distributed (IID) observations: In order to make population-level inference, the sample needs to be representative of the population.  

6. Conditional ignorability: $\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid \mathbf X_i$, i.e. there are no unmeasured confounders.

The first three assumptions are embedded in the potential outcomes notation. Assumptions 2 and 3, together, are also called the Stable Unit Treatment Value assumption [SUTVA, @rubin_1980_randomization]. Special attention should be paid to Assumption 6: double robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the double robust methods covered in this tutorial make no functional form assumptions.  



# Overview of Techniques  
Each of the methods reviewed in this paper can be used with a variety of estimation techniques, including both traditional statistical methods and flexible machine learning algorithms. Each involves a model for the outcome and another for the treatment exposure, but choice of estimator for these two models is left to the discretion of the user. Double robust methods are distinct in the ways these estimated models relate and are combined into a final estimate of the desired estimand.  

This section provides some intuition for the mathematical theory behind each method. In the Appendix, I provide R code to simply implement these methods. This is the code used to evaluate these methods later in this paper, and it represents basic implementations of these methods. However, for researchers wishing to put these methods into practice, using a dedicated R package for each method is probably a better idea. These packages have many flexible options, such as accounting for complex survey design, targeting estimands besides the ATE, and integrating with a variety of estimation techniques.^[In the Supplementary Material full tables of results, I present the results of using two packages to estimate the full set of simulations shown below. These include AIPW in conjunction with generalized random forests (GRF) using the `grf` package [@tibshirani_2024_grf] and a partially linear model for DML with a SuperLearner that, like below, harnesses ensemble learning with GLM, glmnet, and XGBoost, using the `DoubleML` package [@bach_2022_doubleml]. Results are fairly similar to my own R code, though computation time is markedly faster.] 

<!-- In the Supplementary Material, Table \@ref(tab:packages) suggests packages for each method.   -->

## Augmented Inverse Probability Weighting (AIPW)

The oldest of these modern methods, AIPW arose in the context of missing data imputation [@robins_1994_estimation]. @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference. Introductions to AIPW exist in the contexts of political science [@glynn_2010_introduction] and econometrics [@funk_2011_doubly]. The `AIPW` R package provides a simple implementation of the method [@zhong_2021_aipw].     

AIPW combines estimates from a model for the treatment exposure, $\pi(X)$, and a model for the outcome, $\mu(X)$. The name comes from the close similarity to inverse probability weights (IPW), but whereas IPW only weights for probability of treatment, AIPW "augments" these weights with an estimate of the response surface.    

Formally, the model can be written as the difference between an estimated outcome for treated units and an estimated outcome for untreated units (see the demonstration below):

$$\begin{aligned}
\hat \tau_{AIPW} = &\frac{1}{n} \sum_{i=1}^n \left( \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)} + \hat \mu_1(\mathbf X_i) \right) 
- \frac{1}{n} \sum_{i=1}^n \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} + \hat \mu_0(\mathbf X_i) \right)
\end{aligned}$$

In practice, AIPW weights may be very small or very large, a problem that inverse probability weights also suffer from. This can make AIPW prone to high variance. To remedy this, the predicted probabilities of treatment are often truncated, setting extremely small or large weights to some less extreme value [as in the `AIPW` R package, @zhong_2021_aipw].  

@glynn_2010_introduction provide an alternate but equivalent formula, where the basic inverse probability weight (IPW) estimator (which incorporates only the exposure model $\hat \pi$) is corrected using a weighted average of two outcome regression estimates: 
$$\begin{aligned}
\hat \tau_{AIPW} = & \frac{1}{n} \sum_{i=1}^n \left\{ \left[ \frac{D_i Y_i}{\hat \pi(\mathbf X_i)} - \frac{(1-D_i) Y_i}{ 1- \hat \pi(\mathbf X_i)} \right] - \frac{D_i - \hat \pi (\mathbf X_i)}{\hat \pi (\mathbf X_i)(1 - \hat pi (\mathbf X_i))} [(1- \hat \pi (\mathbf X_i)) \hat \mu_1(\mathbf X_i) + \hat \pi (\mathbf X_i) \hat \mu_0(\mathbf X_i)] \right\}.
\end{aligned}$$ 

## Targeted Maximum Likelihood Estimation (TMLE)

Extending and improving previous double robust methods, @vanderlaan_2006_targeted first proposed TMLE using a parametric framework and the efficient influence curve [@hines_2022_demystifying] to obtain estimates and standard errors. Van der Laan has gone on to collaborate on both a gentle introduction [@gruber_2009_targeted], two textbooks [@vanderlaan_2011_targeted; @vanderlaan_2018_targeted], and an R package [@gruber_2012_tmle] for implementing the method. @schuler_2017_targeted and @luque-fernandez_2018_targeted provide introductions for epidemiologists.

TMLE begins by estimating the relevant part of the data-generating distribution $P(Y)$, i.e. the conditional density $Q = P(Y \mid X)$. It next estimates the exposure model. Although any estimation method can be used for these steps, the originators of the method suggest using a "SuperLearner," i.e. ensemble learning with cross-validation [@vanderlaan_2007_super]. Next, the exposure model is used to calculate a "clever covariate," which is similar to an IPW. The coefficient for this clever covariate is estimated using maximum likelihood -- whence the "MLE" in "TMLE." Finally, the estimate of $Q$ is updated in a function involving the clever covariate. This process can be iterated, but usually one iteration is enough. The estimate of the distribution $Q$ can be used to calculate the estimand of interest.  

<!-- Then create the variable $H_{di}$ for targeting step: -->
<!-- $$H_{di} = \frac{I(D_i = 1)}{\hat \pi (\mathbf X_i)} - \frac{I(D_i=0)}{1 - \hat \pi (\mathbf X_i)}.$$ -->

Formally, first generate estimates of $\mu_{d}(\mathbf X_i) = E(Y \mid D=d, \mathbf X_i)$ and $\pi(\mathbf X_i) = P(D=1 \mid \mathbf X_i)$. Next, calculate the clever covariates for each individual in the data. These quantities are similar to inverse probability weights, with $H_{0i}$ for untreated and $H_{1i}$ for treated units:
$$\begin{aligned}
H_{0i}(D=0, \mathbf X = \mathbf x_i) &= \frac{1-d_i}{1- \hat \pi (\mathbf x_i)}, &
H_{1i}(D=1, \mathbf X = \mathbf x_i) = \frac{d_i}{\hat \pi (\mathbf x_i)}.
\end{aligned}$$

In the next step, we estimate fluctuation parameters $\epsilon = (\epsilon_0, \epsilon_1)$ through maximum likelihood of the following logistic regression with fixed intercept $\text{logit}(\mu_{di})$: 
<!-- $$E(Y=1 \mid D, \mathbf X) = \text{logit}(\mu_{di}) + \epsilon H_{di}$$ -->
$$\text{logit}[E(Y=1 \mid D, \mathbf X)] = \text{logit}(\hat \mu_{di}) + \epsilon_0 H_{0i} + \epsilon_1 H_{1i}.$$

Here we are assuming that $Y$ is a dichotomous variable taking the values of 0 or 1; the method is extended to continuous outcomes simply by normalizing the value of $Y$ to fall between 0 and 1. Then we generate updated ("targeted") estimates of potential outcomes:
$$\begin{aligned}
\hat \mu_0^*(\mathbf x_i) &= \text{expit}[\text{logit}(\hat \mu_0(\mathbf x_i)) + \hat \epsilon H_{0i}]\\
\hat \mu_1^*(\mathbf x_i) &= \text{expit}[\text{logit}(\hat \mu_1(\mathbf x_i)) + \hat \epsilon H_{1i}]
\end{aligned}$$
where $\text{expit}(\cdot)$ is the inverse logit function.   

Finally, we estimate the parameter of interest -- in this case, the ATE:
$$\hat \tau_{TMLE} = \frac{1}{n} \sum_{i=1}^n [\hat \mu_1^*(\mathbf x_i) - \hat \mu_0^*(\mathbf x_i)].$$




## Double/Debiased Machine Learning (DML)

The most recently developed of the methods reviewed here, DML was proposed in an econometrics context [@chernozhukov_2018_double] and has since seen a flurry of development [@dukes_2022_doubly; @kennedy_2023_semiparametric; @semenova_2021_debiased; @chernozhukov_2022_long; @farbmacher_2022_causal; @jung_2021_estimating]. The R package `DoubleML` [@bach_2021_doubleml] provides straightforward implementation of the method.  

DML is motivated by the need to handle problems with high-dimensional nuisance parameters, i.e. a large number of measured confounders. Flexible machine learning is appropriate for this task, but such methods suffer from regularization bias, where efforts to control the overfitting of models can bias estimates. DML removes this bias in a two-step procedure. First, it solves the auxiliary problem of estimating the treatment exposure model $E(D \mid X) = \pi(X)$. It then uses this model to remove bias: Neyman orthogonalization allows the creation of an orthogonalized regressor, essentially partialing out the effect of covariates $X$ from treatment $D$. The debiased $D$ is then used to estimate the conditional mean of the outcome $E(Y \mid X) = \mu(X)$, which can be used to calculate the estimand of interest.  

More formally, suppose we want to estimate $\tau$ in the following framework:^[Note that this basic DML setup -- presented in the introduction of @chernozhukov_2018_double -- assumes a partially linear model and targets the ATE. If we are interested in the CATE and heterogeneous effects, Chernozhukov et al. [-@chernozhukov_2018_double, p. C35] present an alternative score function that closely resembles AIPW. In this case, DML and AIPW are identical, except DML also includes sample splitting, accounting for the regularization bias that flexible machine learning estimators may induce. See also @jacob_2021_cate and @nie_2021_quasioracle.]
$$y_i = \tau d_i + g_0(\mathbf x_i) + u_i,$$
$$d_i = m_0(\mathbf x_i) + v_i.$$

The idea is to estimate $g_0$ and $m_0$ separately, then use an orthogonalized or debiased score function -- here, residual-on-residual regression -- to obtain an estimate of $\tau$, which we can designate $\hat \tau$. However, this leaves a term in the asymptotic distribution of $\hat \tau$ that biases the estimate. To avoid this, DML uses sample splitting [@angrist_1995_splitsample].  

We randomly split the sample of $n$ observations into two sets, $I$ and $I^c$, each of size $n/2$.^[In practice, we can split the sample into any number of folds, and more than two sets might be better.] Using any prediction algorithm, we then estimate the response and treatment models using only set $I^c$:  

1) Estimate treatment model $\hat m_0$ in the equation $d_i = \hat m_0(\mathbf x_i) + \hat v_i, \forall i \in I^c$.  
2) Estimate the outcome model $\hat g_0$ in the equation $y_i =  \hat g_0(\mathbf x_i) + \hat u_i, \forall i \in I^c$.

<!-- Note that, in this setup, the estimation of the outcome does not consider treatment assignment $d_i$. This is a key difference between DML and AIPW or TMLE.   -->

Next, we use the estimated models to perform residual-on-residual regression *on the left out set* $I$ to obtain an estimate of $\tau$:
$$\hat \tau(I^c, I) = \left(\sum_{i \in I} \hat v_i d_i \right)^{-1} 
\sum_{i \in I} \hat v_i (y_i - \hat g_0 (\mathbf x_i)),$$
where $\hat v_i = d_i - \hat m_0(\mathbf x_i)$. Using half the sample results in efficiency loss. To rectify this, we repeat the above procedure, switching the split sets. We then have $\hat \tau(I^c, I)$ and $\hat \tau(I, I^c)$. The cross-fitting DML estimator is:
$$\hat \tau_{DML} = \frac{\hat \tau (I^c, I) + \hat \tau (I, I^c)}{2}.$$  







## A simple demonstration using AIPW

To demonstrate double robustness, this section presents one of the simpler double robust estimators, AIPW. As shown above, we can write this estimator as follows:

$$\begin{aligned}
\hat \tau = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) 
- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}$$

For each individual in the sample, this estimator calculates two quantities: The treated potential outcome

\begin{equation}
\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)
(\#eq:treated-aipw)
\end{equation}

and the control potential outcome

\begin{equation}
\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i).
(\#eq:control-aipw)
\end{equation}

Let's focus on the treated model, Equation \@ref(eq:treated-aipw). First, assume that the outcome model $\mu_1(X_i)$ is *correctly* specified and the exposure model $\pi(X_i)$ is *incorrectly* specified. Let's also assume (for now) that we're dealing with a treated unit, i.e. $D_i = 1$. Then
$$E[\hat \mu_1 (X_i)] = E[Y_1 \mid X_i].$$
This means that the model for the outcome and the observed outcome for this treated unit are equal in expectation, so
$$E[Y_i - \hat \mu_1(X_i)] = 0.$$
Plugging into Equation \@ref(eq:treated-aipw), we get 
$$E[\hat Y_{1i}] = 0 + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$
So the model relies *only* on the outcome model. The incorrectly specified exposure model completely disappears from the equation. If we're dealing with a control unit, we get the same result, plugging $D_i=0$ into the same Equation \@ref(eq:treated-aipw):
$$\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$  

Now, what if the *exposure* model $\pi(X_i)$ is correctly specified and the outcome model $\mu_1(X)$ is incorrect? First, we use algebra to rewrite Equation \@ref(eq:treated-aipw) for the treated outcome:
\begin{align}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \nonumber \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \nonumber \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). 
(\#eq:exposure-correct)
\end{align}
Since the exposure model is correctly specified, we have $D_i = \hat \pi(X_i)$ on average, so
$$E[D_i - \hat \pi(X_i)] = 0.$$
This means that the second term in Equation \@ref(eq:exposure-correct) is 0 in expectation, so
$$E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].$$
This shows that when the exposure model is correct, then the estimator depends *only* on the exposure model. We can make similar arguments for the control model for $\hat Y_{0i}$ in Equation \@ref(eq:control-aipw).  

This demonstration shows that this estimator achieves double robustness: The estimator is robust to misspecification of either the exposure or the outcome model (but not both). The other two double robust methods considered in this paper can be shown to have the same property, but demonstrating this is more complicated. Interested readers can refer to the references cited above.

# Simulations






```{r dorie-sim, eval = F}
set.seed(185)

# sims %>%
#   lapply(function(x) append(x, list()))

sim_list <- list()
for(i in 1:20){ # up to 77
  #sim_list_nest <- list()
  for(j in 1:10){ # up to 100
    print(paste0(i,',',j))
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016,
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'main'))
  }
}

sim_linear <- list()
for(i in c(1, 3)){
  for(j in 1:100){ 
    print(paste0(i,',',j))
    sim_linear[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'linear'))
  }
}


saveRDS(append(sim_list, sim_linear), here('files', 'sims.RDS'))
```

```{r large, eval = F}
# dataset 7: polynomial     0.35    one-term exponential      0.75      high
set.seed(185)

sim_list <- list()

for(j in c(2, 5, 10, 20)){ # up to 100
  for(i in 1:20){ # up to 77
  #sim_list_nest <- list()

    print(paste0(i,',',j))
    
    dataset <- aciccomp2016::input_2016 %>% 
      slice(rep(1:n(), each = j))
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(dataset, 7, i, extraInfo = T) %>% 
      append(list(dataset = 7, set = 'large', size = j*4802))
  }
}

saveRDS(sim_list, here('files', 'sims_large.RDS'))

```

```{r sims-small, eval = F}
set.seed(185)

sim_list <- list()
for(j in c(150, 300, 600, 1200, 2400, 4802)){ 
  for(i in 1:20){ 
  #sim_list_nest <- list()

    print(paste0(i,',',j))
  
    sample_index <- sample(1:4802, j, replace = F)
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 7, i, extraInfo = T) %>% 
      append(list(dorie_dataset = 7, set = 'small', size = j))
    
    for(name in names(sim_list[[paste0(i,',',j)]])[1:7]){
      sim_list[[paste0(i,',',j)]][[name]] <- sim_list[[paste0(i,',',j)]][[name]][sample_index]
    }
    
    sim_list[[paste0(i,',',j)]]$x <- sim_list[[paste0(i,',',j)]]$x[sample_index,]
  }
}

saveRDS(sim_list, here('files', 'sims_small.RDS'))
```



```{r sim-test, eval = F}
data_test <- bind_rows(aciccomp2016::input_2016, aciccomp2016::input_2016)

acs <- read_dta('/Users/nathan/Data/ACS/acs_2008_2021.dta')

test <- aciccomp2016::dgp_2016(data_test, parameter_list, 1950, extraInfo = T)
set.seed(185)
test_acs <- aciccomp2016::dgp_2016(filter(acs, year == 2016) %>% drop_na() %>% sample_n(1e5), 
                                   parameter_list, 1950, extraInfo = T)
```


```{r lm-sim, eval = F}
lm_df <- lm_sim(sims) 

write_csv(lm_df, here('files', 'lm_df.csv'))


# lm_df %>%
#   group_by(set) %>%
#   summarize(ate_est = mean(ate), 
#             se = sd(ate))
```


```{r psm-sim, eval = F}
psm_df <- psm_sim(sims) 

write_csv(psm_df, here('files', 'psm_df.csv'))
```


```{r pred-calc, eval = F}

ols_logit_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  ols_logit_pred_list[[i]] <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(ols_logit_pred_list, here('files', 'ols_logit_pred.RDS'))

# tibble(ols_logit_pred_list[[i]]$y,
#              ols_logit_pred_list[[i]]$mu1_pred, 
#              ols_logit_pred_list[[i]]$mu0_pred,
#              ols_logit_pred_list[[i]]$d,
#              hist(ols_logit_pred_list[[12]]$pi_pred)
#              ) 

grf_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  grf_pred_list[[i]] <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(grf_pred_list, here('files', 'grf_pred.RDS'))

 
superlearner_pred_list <- list() 
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  superlearner_pred_list[[i]] <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(superlearner_pred_list, here('files', 'superlearner_pred.RDS'))


```


```{r ipw, eval = F}
ols_logit_pred_list2 <- readRDS(here('files', 'ols_logit_pred.RDS'))
grf_pred_list2 <- readRDS(here('files', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files', 'superlearner_pred.RDS'))


ipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    

    y <- sims[[i]]$y
    D <- sims[[i]]$z == 'trt'
    PrD <- mean(D)
    # ps <- predictions[[j]][[i]]$pi_pred
    ps <- case_when(
      predictions[[j]][[i]]$pi_pred < .01 ~ .01,
      predictions[[j]][[i]]$pi_pred > .99 ~ .99,
      T ~ predictions[[j]][[i]]$pi_pred)
      
    
    # ate_ipw <- sum((D*y)/ps) / sum(D/ps) - sum(((1-D)*y)/(1-ps)) / sum(1-D/(1-ps)) 
    
    # ate_ipw <- mean(y*(D==1*PrD)/ps - y*((D==0)*(1-PrD))/(1-ps))
    
    
    ipw <- (D*PrD)/ps + ((1-D)*(1-PrD))/(1-ps)
    lm_out <- lm(sims[[i]]$y ~ D, weights=ipw)
    ate_ipw <- coefficients(lm_out)[[2]]
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    ipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = ate_ipw, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(ipw_out_list), here('files', 'ipw.csv'))
```

```{r g-comp, eval = F}
grf_pred_list2 <- readRDS(here('files', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files', 'superlearner_pred.RDS'))


gcomp_out_list <- list()
predictions <- list(grf_pred_list2, superlearner_pred_list2)
pred_names <- c('grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    
    gcomp_out <- with(predictions[[j]][[i]], mean(mu1_pred - mu0_pred))
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    gcomp_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = gcomp_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(gcomp_out_list), here('files', 'gcomp.csv'))
```

```{r lin, eval = F}

n <- length(sims)
lin_list <- list()
for(i in 1:n){
  start_time <- Sys.time()
  
  print(paste0(i, ' out of ', n))
  sim_dat <- data.frame(y = sims[[i]]$y, 
                        d = ifelse(sims[[i]]$z == 'trt', 1, 0), 
                        sims[[i]]$x)
  
  ate <- NA
  tryCatch({
    lm_out <- tidy(estimatr::lm_lin(y ~ d, 
                                    covariates = as.formula(paste0('~ ', paste(names(sims[[i]]$x), collapse = ' + '))), 
                                    data = sim_dat))
    ate <- lm_out[[2,2]]
    
    }, error=function(e){
      cat("ERROR :",conditionMessage(e), "\n")
      })
  
  lin_list[[i]] <- data.frame(dataset = i,
                             ate = ate,
                             #se = lm_out[[2,3]],
                             truth = mean(sims[[i]]$y.1) - mean(sims[[i]]$y.0),
                             dorie_dataset = sims[[i]]$dataset,
                             set = sims[[i]]$set,
                             size = sims[[i]]$size,
                             comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
}


write_csv(bind_rows(lin_list), here('files', 'lin.csv'))
```

```{r aipw, eval = F}



aipw_calc2 <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, c(.025)) ~ as.numeric(quantile(pi_pred, c(.025))),
  #   pi_pred > quantile(pi_pred, c(.975)) ~ as.numeric(quantile(pi_pred, c(.975))),
  #   T ~ pi_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)
  
  ipw <- (d*y)/pi_pred - (1-d)*y/(1-pi_pred)
  adjust <- (d-pi_pred)/(pi_pred*(1-pi_pred)) * ((1-pi_pred)*mu1_pred + pi_pred*mu0_pred)
  
  ate <- (1/n) * sum(ipw - adjust)
  
  return(ate)
}


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    #   }
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files', 'aipw.csv'))


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    #   }
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files', 'aipw_trunc.csv'))



```


```{r tmle, eval = F}

tmle_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    # }
    
    start_time <- Sys.time()
    tryCatch({
      tmle_out <- with(predictions[[j]][[i]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
    }, error=function(e){
      tmle_out <- NA
      })
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    tmle_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = denormalize(tmle_out, predictions[[j]][[i]]$y), 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      fail = is.na(tmle_out),
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(tmle_out_list), here('files', 'tmle.csv'))


# logit_logit_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   logit_logit_pred_out <- logit_logit_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(logit_logit_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y)) 
#   truth <- mean(dat$y.1) - mean(dat$y.0)
# 
#   logit_logit_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(logit_logit_pred_list), label = 'ols, logit, tmle')
# 
# 
# #### 
# 
# grf_pred_list <- list()
# 
# for(i in 1:10){ #length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   grf_pred_out <- grf_pred(
#     y = dat$y, # with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   yt <- function(x, y){(x - min(y)) / (max(y) - min(y))}
#   
#   ate <- with(grf_pred_out, 
#               tmle_calc(mu1_pred = yt(mu1_pred, dat$y), 
#                         mu0_pred = yt(mu0_pred, dat$y),
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = yt(y, dat$y)))
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#   
#   grf_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(grf_pred_list), label = 'grf, tmle')


# superlearner_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   superlearner_pred_out <- superlearner_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(superlearner_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y))
#   truth = mean(dat$y.1) - mean(dat$y.0)
#   
#   superlearner_pred_list[[i]] <- data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(superlearner_pred_list), label = 'superlearner, tmle')
```




```{r dml, eval = F}

## DML OLS logit

ols_logit_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
   
  start_time <- Sys.time()
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)

  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
   
  ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(ols_logit_dml_list), 
          here('files', 'dml_ols_logit.csv'))

## DML OLS logit (truncated)
# 
# ols_logit_dml_list <- list()
# 
# set.seed(1285)
# for(i in 1:length(sims)){
#   print(i)
#   dat <- sims[[i]]
#   
#   start_time <- Sys.time()
#   dml_pre_out <- dml_pre(dat$y,
#                        as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#                        x = dat$x)
# 
#   ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
#   
#   ols_logit_dml_out$pi_pred1 <- case_when(
#         ols_logit_dml_out$pi_pred1 < .01 ~ .01,
#         ols_logit_dml_out$pi_pred1 > .99 ~ .99,
#         T ~ ols_logit_dml_out$pi_pred1)
# 
#   ols_logit_dml_out$pi_pred2 <- case_when(
#         ols_logit_dml_out$pi_pred2 < .01 ~ .01,
#         ols_logit_dml_out$pi_pred2 > .99 ~ .99,
#         T ~ ols_logit_dml_out$pi_pred2)
#     
#     
#   
#   dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
#   comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
#   
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#    
#   ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
#                               truth = truth, 
#                               comp_time = comp_time,
#                               dataset = i,
#                               dorie_dataset = dat$dataset,
#                               set = dat$set,
#                               size = dat$size)
# }
# 
# write_csv(bind_rows(ols_logit_dml_list), 
#           here('files', 'dml_ols_logit_trunc.csv'))



# perform(bind_rows(ols_logit_dml_list), label = 'grf, dml')
 

## GRF
grf_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
   
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      grf_dml_out <- do.call(grf_dml, dml_pre_out)

      dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  grf_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(grf_dml_list), 
          here('files', 'dml_grf.csv'))

# perform(bind_rows(grf_dml_list), label = 'grf, dml')

## Superlearner
superlearner_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      superlearner_dml_out <- do.call(superlearner_dml, dml_pre_out)
      dml_post_out <- do.call(dml_post, append(dml_pre_out, superlearner_dml_out ))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  superlearner_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(superlearner_dml_list), 
          here('files', 'dml_superlearner.csv'))



#perform(bind_rows(superlearner_dml_list), label = 'superlearner, dml')

```

```{r aipw-package, eval = F}
aipw_package_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(paste0(i, ' out of ', length(sims)))
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  y <- dat$y
  d <- as.numeric(ifelse(dat$z == 'trt', 1, 0))
  x <- model_matrix(~., dat$x)[,]
  
  grf_model <- grf::causal_forest(X = x, Y = y, W = d, seed = 123)                        
  grf_out <- grf::average_treatment_effect(grf_model, target.sample = 'all', method = 'AIPW')
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  aipw_package_list[[i]] <- data.frame(ate = grf_out[['estimate']], 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size) 
  
  if(i %% 100 == 0){
    write_csv(bind_rows(aipw_package_list), 
          here('files', 'aipw_package.csv'))
  }

}

write_csv(bind_rows(aipw_package_list), 
          here('files', 'aipw_package.csv'))


```


```{r dml-package, eval = F}


dml_package_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(paste0(i, ' out of ', length(sims)))
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_data <- double_ml_data_from_matrix(X = model_matrix(~., dat$x)[,],
           y = dat$y,
           d = as.numeric(ifelse(dat$z == 'trt', 1, 0)))

  graph_ensemble_regr = gunion(list(
      po("learner", lrn("regr.cv_glmnet", s = "lambda.min")),
      po("learner", lrn('regr.xgboost', max_depth = 4)),
      po("learner", lrn("regr.glm"))
    )) %>>%
      po("regravg", 3)
  
  ensemble_pipe_regr = as_learner(graph_ensemble_regr)
  set.seed(123)
  obj_dml_plr_sim_pipe_ensemble = DoubleMLPLR$new(dml_data,
                                                  ml_l = ensemble_pipe_regr,
                                                  ml_m = ensemble_pipe_regr)
  obj_dml_plr_sim_pipe_ensemble$fit()
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  dml_package_list[[i]] <- data.frame(ate = as.vector(obj_dml_plr_sim_pipe_ensemble$coef), 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
  
  if(i %% 100 == 0){
    write_csv(bind_rows(dml_package_list), 
          here('files', 'dml_package.csv'))
  }

}

write_csv(bind_rows(dml_package_list), 
          here('files', 'dml_package.csv'))



```





```{r results-load}

lm_df <- read_csv(here('files', 'lm_df.csv')) %>%
  mutate(method = 'ols')

psm_df <- read_csv(here('files', 'psm_df.csv')) %>%
  mutate(method = 'psm')

lin_df <- read_csv(here('files', 'lin.csv')) %>%
  mutate(method = 'lin')

ipw_df <- read_csv(here('files', 'ipw.csv')) %>%
  mutate(estimator = method,
         method = 'ipw')

gcomp_df <- read_csv(here('files', 'gcomp.csv')) %>%
  mutate(estimator = method,
         method = 'g-comp')

aipw_df <- read_csv(here('files', 'aipw.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

aipw_trunc_df <- read_csv(here('files', 'aipw_trunc.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

tmle_df <- read_csv(here('files', 'tmle.csv')) %>%
  # filter(fail == F) %>%
  select(-fail) %>%
  mutate(estimator = method,
         method = 'tmle')

dml_df <- bind_rows(
  mutate(read_csv(here('files', 'dml_ols_logit.csv')), 
         estimator = 'ols_logit',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files', 'dml_grf.csv')), 
         estimator = 'grf',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files', 'dml_superlearner.csv')), 
         estimator = 'superlearner',
         method = 'dml',
         dataset = row_number())
) %>%
  select(names(tmle_df))

# aipw_package_df <- read_csv(here('files', 'aipw_package.csv')) %>%
#   mutate(estimator = 'grf (package)',
#          method = 'aipw')
# 
# dml_package_df <- read_csv(here('files', 'dml_package.csv')) %>%
#   mutate(estimator = 'superlearner (package)',
#          method = 'dml')


sim_results <- bind_rows(lm_df, psm_df, lin_df, ipw_df, gcomp_df, aipw_trunc_df, tmle_df, dml_df) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'ols', 'logit', 
          'grf', 'grf (package)', 'superlearner', 'superlearner (package)')),
         method = factor(method, levels = c('ols', 'psm', 'ipw', 'g-comp', 'lin', 
                                            'aipw', 'tmle', 'dml', 'aipw (original)', 'aipw (package)'))) %>%
  arrange(method, estimator) %>%
  #mutate(estimator = ifelse(is.na(estimator), 'NA', estimator)) %>%
  mutate(bias = ate - truth,
         method_estimator = ifelse(is.na(estimator), as.character(method), paste0(method, ', ', estimator))) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) 

# write_csv(sim_results, here('files', 'lalonde_means.csv'))
```


These double robust methods have many similarities. How do the results they give compare? This section tests the performance of each in practice, comparing point estimates on simulated data from a causal inference competition. The true treatment effect is known and all potential confounders are observed, so these simulations allow assessment of bias and related quantities.  

In 2016, the Atlantic Causal Inference Conference hosted a competition for causal inference methods that adjust on observables. @dorie_2019_automated published the results of this competition, along with the data used. Below, I test double robust methods on the 20 data generating processes (DGPs) reserved for the "do-it-yourself" part of the competition. The data represent a hypothetical twins study investigating the impact of birth weight on IQ. The data have 4,802 observations and 52 covariates. Treatment is binary and the outcome is continuous. In all DGPs, ignorability holds (all potential confounders are observed), but the authors vary the following: (1) degree of nonlinearity, (2) percentage of treated, (3) overlap for the treatment group, (4) alignment (correspondence in variables used to generate the assignment mechanism and the response surface), and (5) treatment effect heterogeneity.  

The true treatment effect also varies, but as a function of the other DGP characteristics. It has a mean of `r sim_results %>% filter(set == 'main') %>% summarize(round(mean(truth), 1))`, standard deviation of `r sim_results %>% filter(set == 'main') %>% summarize(round(sd(truth), 1))`, and range of `r sim_results %>% filter(set == 'main') %>% pull(truth) %>% min() %>% round(1)` to `r sim_results %>% filter(set == 'main') %>% pull(truth) %>% max() %>% round(1)`. 

## Evaluation Strategy  


Implementing the R code presented in the Appendix, I compare the three double robust methods to two sets of traditional or "single robust" methods used as benchmarks (see Table \@ref(tab:methods-overview) for an overview of all methods used). By "single robust," I mean methods that are not robust to any misspecification. First are one-model methods. The most classic method considered -- linear regression -- models only the response surface. It is estimated using ordinary least squares regression ("OLS"), entering each variable separately without any interactions or higher-order terms. Two other one-model methods model only the treatment assignment mechanism. In propensity score matching (PSM), propensity scores are estimated from logistic regression with each variable entered separately and without any higher order terms, then matched using the `MatchIt` package. Finally, stabilized inverse    probability weights [IPW, @austin_2015_moving, p. 3663] are used for weighted OLS regression. These IPWs are estimated using propensity scores estimated by each of the three under-the-hood estimation techniques described below; extreme propensity scores are truncated so that they range from 0.01 to 0.99.  

The second set of single robust methods are two-model methods, which estimate separate models for treated and untreated units. In theory these could solve the misspecification problem that double robust methods are meant to solve, but they could still suffer from regularization bias. G-computation -- also called regression imputation -- uses some estimation technique to predict outcomes under treatment and control for each unit in the dataset [@robins_1986_new; @snowden_2011_implementation]. This underlying technique can be anything from a traditional method such as OLS regression to a flexible machine learning method. The ATE estimate is the difference in the average prediction under treatment and the average prediction under control: 

$$\begin{aligned}
\hat \tau_{gcomp} = &\frac{1}{n} \sum_{i=1}^n \left[\hat \mu_1 (\mathbf X_i) - \hat \mu_0 (\mathbf X_i) \right].
\end{aligned}$$  

The second two-model method is the Lin estimator [@lin_2013_agnostic]. This method aims to solve issues with the bias induced by OLS regression in a randomization framework by interacting the treatment indicator with mean-centered covariates. @hazlett_2024_demystifying show that this method is equivalent to estimating two separate OLS regression models for the treated and control units -- i.e., G-computation with each of the underlying models estimated by OLS regression.  

Because many of these methods allow the user to choose the underlying estimation method, results compare three estimators. The first estimator uses a logistic regression for the exposure model and an OLS regression for the outcome model. Second is generalized random forests [GRF, @athey_2019_generalized] using the `grf` R package, with separate models for exposure and outcome. The final estimator is the SuperLearner (as promoted by the makers of TMLE) using the `SuperLearner` package [@polley_2023_superlearner], again with separate models for exposure and outcome. GLM, glmnet [a weighted average of lasso and ridge regression, @friedman_2021_package], and XGBoost with a maximum tree depth of 4 [@chen_2016_xgboost] are the models considered for the SuperLearner. These three estimation techniques are used for each of the three double robust methods and for IPW. GRF and SuperLearner are also used for G-computation (OLS predictions with G-computation return identical results to OLS regression).  



```{r methods-overview}

read_csv(here('files/methods.csv')) %>%
  huxtable::hux() %>%
  huxtable::theme_article() %>%
  huxtable::set_width(1) %>%
  # huxtable::set_all_padding(0) %>%
  # huxtable::set_latex_float('h!') %>%
  huxtable::set_caption('Double and single robust methods used for evaluation. The super learner estimator is an ensemble learning method relying on GLM, glmnet, and XGBoost.')
  
  # pander::pander(split.cell = 15, split.table = Inf, 
  #                justify = 'left',
  #                caption = 'Double and single robust methods used for evaluation',
  #                label = 'methods-overview') 
  # kable(booktabs = T, linesep = '', caption = 'Double and single robust methods used for evaluation') %>%
  # kable_styling(latex_options = c("hold_position"))


```

## Main results

I use 10 simulations of each of the 20 DGPs, resulting in 200 data sets. I then calculate bias, percent bias (the estimator's bias as a percentage of its standard error), root mean squared error (rmse), and median absolute error (mae). I also present the number of datasets for which the method fails and the median computation time for each data set, in seconds.^[Simulations were run on a 2020 MacBook Pro laptop computer with a 2 GHz Quad-Core Intel Core i5 Processor and 16 GB of memory.] In the main text I present average bias and RMSE, while the Supplementary Material contains tables with full results.  

Bias results for the full range of simulations are shown in Figure \@ref(fig:dorie-results-bias). Bias is quite low for many of the methods, however IPW (logit) and AIPW (OLS/logit) have high bias and variance, while TMLE (OLS/logit) has moderately high bias and variance. With an average true treatment effect of `r sim_results %>% filter(set == 'main') %>% summarize(round(mean(truth), 1))`, bias with absolute value greater than 1 is substantial. The traditional methods, however, achieve fairly low bias in general.

Figure \@ref(fig:dorie-results-rmse) orders methods by RMSE and presents both bias and RMSE. The lowest RMSE is achieved by three of the methods using SuperLearner estimators (AIPW, TMLE, and G-computation) with values of about 0.35, followed closely by the same three methods using GRF, with values closer to 0.5. The computationally efficient Lin estimator does not do much worse, with an RMSE of 0.6, and OLS and PSM achieve acceptable RMSE of 0.7 to 0.9. Interestingly, DML with the computationally efficient OLS/logit estimators achieves lower RMSE than with GRF or SuperLearner (0.8 compared to 0.9 and 1.6, respectively). The only estimators with RMSE that exceed 2 are TMLE (OLS/logit) with 2.3, IPW (logit) with 8.1, and AIPW (OLS/logit) with 10.1. These methods all use logit models to estimate probability of treatment, and these high error rates are likely due to extreme values of these estimates.  

<!-- Overall, while the double robust methods of AIPW and TMLE achieve slightly lower RMSE than the more traditional, differences are quite small. OLS, PSM, and the Lin estimator all achieve fairly low RMSE and are computed in less than a second. Compare this to the lowest-RMSE method AIPW with SuperLearner, which takes an average of 130 seconds per simulation. -->

Overall, traditional methods perform surprisingly well in comparison with the double robust methods, and flexible single robust methods may be as effective as double robust methods. Even in the full range of datasets -- which include highly nonlinear exposure and outcome data-generating processes -- OLS, propensity score matching, and the Lin estimator obtain some of the smallest bias and RMSE. While double robust methods achieve the lowest RMSE, the choice of underlying estimator appears more important than the choice of method. AIPW and TMLE both do well with a flexible underlying estimator, while DML (in its partially linear form) does worse than OLS. Of the estimators considered, the SuperLearner (which considers GLM, glmnet, and XGBoost models) appears to be best for the double robust methods, with GRF following closely. G-computation does only a hair worse than these two double robust methods without explicitly accounting for regularization bias. Notably, the method with the longest computation time -- DML with a SuperLearner -- takes nearly 2,000 times as long as OLS (an average of 129 seconds per simulation compared to 0.061 seconds).  

```{r dorie-results-bias,  fig.cap = 'Bias of Monte Carlo simulations using the first 20 DGPs from Dorie et al. (2019), 10 replications each. Points represent the bias of estimates on individual datasets, while box plots show the median, 25th and 75th percentiles, and whiskers extending to 1.5 times the interquartile range. Colors correspond to method (in which multiple estimators may be used).'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

sim_results %>% 
  filter(set == 'main') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'Main datasets') +
  scale_y_continuous(trans=ggallin::pseudolog10_trans, breaks = c(-30, -20, -10, -1, 0, 1, 10, 20, 30)) +
  # scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2),
        legend.position = 'none') 
  
ggsave(here('draft/figures', 'dorie-results-bias.png'), width = 7,  height = 5.5, dpi = 600)
```

```{r dorie-results-rmse, fig.cap = 'Root mean squared error and bias for Monte Carlo simulations using the first 20 DGPs from Dorie et al. (2019), 10 replications each. Values greater in absolute value than 4 are plotted at 4 and labeled with their actual value.'}

sim_results_plot <- sim_results %>%
  filter(set == 'main') %>%
  group_by(method_estimator) %>%
  summarize(bias = mean(ate - truth, na.rm = T),
            rmse = sqrt(mean((ate - truth)^2, na.rm = T))
              ) %>%
  arrange(rmse) %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  pivot_longer(c(bias, rmse)) %>%
    mutate(label = ifelse(abs(value) > 4, round(value), NA),
         value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

sim_results_plot %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point() +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', y = '', title = 'Main datasets') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))

ggsave(here('draft/figures', 'dorie-results-rmse.png'), width = 7,  height = 5.5, dpi = 600)
```

## Linear DGPs

Due to their functional form assumptions, traditional methods may perform better when the data generating processes are linear. To test this, I use 100 simulations of each of the two datasets from @dorie_2019_automated with linear data generating processes for both exposure and outcome (numbers 1 and 3). In these two datasets, the average true treatment effect is `r sim_results %>% filter(set == 'linear') %>% summarize(mean(truth))` with a standard deviation of `r sim_results %>% filter(set == 'linear') %>% summarize(sd(truth))`.  

Figure \@ref(fig:dorie-linear-bias) shows the bias from each simulation for each method (table in the Supplementary Material). Similarly to the full set of simulations, bias is fairly low for most methods. IPW (logit) and AIPW (OLS/logit) again suffer from greater bias than other methods, though not to quite as extent as in the full range of simulations. Unsurprisingly, methods that assume linearity -- OLS, PSM, the Lin model -- achieve low bias and variance. IPW does less well than expected, even when treatment assignment is modeled with flexible GRF and SuperLearner.    

Figure \@ref(fig:dorie-linear-rmse) orders the methods by RMSE and presents both RMSE and bias. The methods achieving the lowest RMSE are again three of the SuperLearner methods (TMLE, AIPW, and G-computation) followed by the Lin estimator, DML (OLS/logit), and OLS. The only methods with RMSE above 1 are IPW (logit) and AIPW (OLS/logit), again likely due to unstable logit predictions.  

With these linear DGPs, there seems little reason to sacrifice computational efficiency for a very slight reduction in RMSE. The Lin estimator has an RMSE of 0.28 compared to the lowest-RMSE method, TMLE (SuperLearner), of 0.25, and computes in 0.15 seconds compared to the latter's 126 seconds. Even standard OLS has quite a low RMSE of 0.39. While DML performs better with the linear DGPs than the full range of simulations, it still obtains higher RMSE than at least certain AIPW and TMLE variants. Finally, G-computation again shows its strength, outperforming most other methods when it is estimated using a SuperLearner.




```{r dorie-linear-bias,  fig.cap = 'Bias of Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each. Points represent the bias of estimates on individual datasets, while box plots show the median, 25th and 75th percentiles, and whiskers extending to 1.5 times the interquartile range. Colors correspond to method (in which multiple estimators may be used).'}
sim_results %>% 
  filter(set == 'linear') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'Linear DGPs') +
  scale_y_continuous(trans=ggallin::pseudolog10_trans, breaks = c(-20, -10, -5, -1, 0, 1, 5)) +
  # scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2),
        legend.position = 'none') 

ggsave(here('draft/figures', 'dorie-linear-bias.png'), width = 7,  height = 5.5, dpi = 600)
```


```{r dorie-linear-rmse,  fig.cap = 'Root mean squared error and bias for Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear"). Values greater in absolute value than 4 are plotted at 4 and labeled with their actual value.'}

sim_results_plot <- sim_results %>%
  filter(set == 'linear') %>%
  group_by(method_estimator) %>%
  summarize(bias = mean(ate - truth, na.rm = T),
            rmse = sqrt(mean((ate - truth)^2, na.rm = T))
              ) %>%
  arrange(rmse) %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  pivot_longer(c(bias, rmse)) %>%
  mutate(label = ifelse(abs(value) > 4, round(value), NA),
         value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

sim_results_plot %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point() +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', y = '', title = 'Linear DGPs') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))

ggsave(here('draft/figures', 'dorie-linear-rmse.png'), width = 7,  height = 5.5, dpi = 600)
```

## Do results vary by DGP?
```{r vary-old, eval = F}
sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number()))

dgp_names <- c('Treatment assignment', 'Probability of treatment',
               'Overlap of treatment', 'Response surface',
               'Alignment (same terms for treatment and response)',
               'Treatment effect heterogeneity')

names(dgp_names) <- names(aciccomp2016::parameters_2016)

for(name in colnames(aciccomp2016::parameters_2016[1])){
  plot <- sim_results_dgp %>% 
    mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
    mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
    #filter(method %in% c('ols', 'psm', 'aipw')) %>%
    ggplot(aes(x = method_estimator, y = bias)) +
    geom_jitter(alpha = .2, width = .3, aes(color = method)) +
    geom_boxplot(alpha = .5, outlier.alpha = 0) +
    #facet_wrap(~method, scales = 'free_x') +
    labs(x = '', title = dgp_names[name]) +
    theme(#axis.text.x=element_text(angle=-15), 
          legend.position = 'none') +
    scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
    facet_wrap(vars(!!sym(name)), ncol = 1, scales = 'free') 
  
  print(plot)
}



```

```{r vary-fig, eval = F}
sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number()))

dgp_names <- c('Treatment assignment', 'Probability of treatment',
               'Overlap of treatment', 'Response surface',
               'Alignment (same terms for treatment and response)',
               'Treatment effect heterogeneity')

names(dgp_names) <- names(aciccomp2016::parameters_2016)

for(name in colnames(aciccomp2016::parameters_2016[1])){
  
  sim_results_plot <- sim_results_dgp %>%
    group_by(!!sym(name), method_estimator) %>%
    summarize(bias = round(mean(ate - truth, na.rm = T), 3),
              rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3)
                ) %>%
    arrange(rmse) %>%
    mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
    mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
    pivot_longer(c(bias, rmse)) %>%
    mutate(label = ifelse(abs(value) > 4, round(value), NA),
           value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

  sim_results_plot %>%
    #filter(method %in% c('ols', 'psm', 'aipw')) %>%
    ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
    geom_point() +
    # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
    geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
    geom_hline(yintercept = 0) + 
    #facet_wrap(~method, scales = 'free_x') +
    labs(x = '', y = '', title = dgp_names[name]) +
    theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
    #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
    #scale_y_continuous(trans=ggallin::pseudolog10_trans)
    ylim(c(-4, 4)) +
    facet_wrap(vars(!!sym(name)), ncol = 1, scales = 'free_x') %>%
    print()
  
}



```

```{r dgp}
dgp_names <- c('Treat. assign.', 'Prob. of treat.',
               'Overlap', 'Response surface',
               'Alignment',
               'Treat. heterogeneity')

rename_vec <- names(aciccomp2016::parameters_2016)
names(rename_vec) <- dgp_names

sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number())) %>%
  rename(rename_vec) %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) 


dgp_list <- list()

for(name in dgp_names){
  
  dgp_list[[name]] <- sim_results_dgp %>%
    group_by(!!sym(name), method_estimator) %>%
    summarize(rmse = sqrt(mean((ate - truth)^2, na.rm = T))
                ) %>%
    ungroup() %>%
    group_by(!!sym(name)) %>%
    slice_min(rmse, n = 6) %>%
    mutate(rank = c('Lowest RMSE', 'Second-lowest', 'Third-lowest', 
                    'Fourth-lowest', 'Fifth-lowest', 'Sixth-lowest'),
           `DGP parameter` = name) %>%
    ungroup() %>%
    mutate(rmse = round(rmse, 3),
          value = paste(method_estimator, rmse, sep = ': '),
          dgp_value = as.character(!!sym(name))) %>%
    select(`DGP parameter`, dgp_value, value, rank) %>%
    pivot_wider(names_from = rank) 
  
}

bind_rows(dgp_list) %>%
  select(-c(`Fourth-lowest`:`Sixth-lowest`)) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Data generating process: Three lowest RMSE methods by DGP for Monte Carlo simulations using the first 20 DGPs from Dorie et al. (2019), 10 replications each.')
```

```{r dgp-2}
bind_rows(dgp_list) %>%
  select(-c(`Lowest RMSE`:`Third-lowest`)) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Data generating process: Fourth- to sixth-lowest RMSE methods by DGP for Monte Carlo simulations using the first 20 DGPs from Dorie et al. (2019), 10 replications each.')

```


While the AIPW, TMLE, and G-computation with a SuperLearner may be the top performing methods overall, this does not mean that there are some DGPs where some other method may do better. Across the 20 DGPs, @dorie_2019_automated vary five characteristics: degree of nonlinearity, the percentage treated, overlap for the treatment group, alignment (correspondence in variables used to generate the exposure and response models), and treatment effect heterogeneity. To test how the methods perform across different values of these characteristics, I begin with the 200 simulations from the 20 DGPs used in the main results (Figures \@ref(fig:dorie-results-bias) and \@ref(fig:dorie-results-rmse)). I limit datasets to those generated by a particular value of a DGP characteristic, and I then calculate RMSE for each method for only these datasets. For example, alignment is 0 for datasets 8 and 16 (meaning there is 0 correlation between the terms included in the treatment and outcome models), so for this value RMSE is calculated only for datasets generated from those two DGPs.  

Tables \@ref(tab:dgp) and \@ref(tab:dgp-2) shows the six top-performing methods by lowest RMSE for each each value of the DGP characteristics. (The Supplementary Material presents a table with full results for all DGPs in the simulations.) Across DGPs, the same methods dominate as in the full range of simulations: the SuperLearner with AIPW, TMLE, and G-computation. In fact, these three methods take the top three spots for *every* DGP variation in the simulations.  

Table \@ref(tab:dgp-2) shows the fourth- to sixth-lowest RMSE methods for each DGP variation. GRF with AIPW, G-computation, and TMLE take most of these spots, but the Lin estimator and OLS appear a few times. Notably, in the presence of high treatment effect heterogeneity, flexible machine learning methods take the top six spots, and not all of these are double robust. For the DGPs, allowing for treatment effects to vary appears more important than double robustness. This is supported by the performance of the Lin estimator, which takes the seventh spot (as shown in the Supplementary Material). This estimator is relatively inflexible but robust to heterogeneous effects. DML, which as implemented here is not robust to heterogeneous treatment effects, performs worse than OLS regression.  

Overall, there is little variation across different types of DGPs in which method performs the best. Flexible estimators take the top spots (though notably not those used with DML), and traditional methods do fairly well across the board. 

```{r hetro-rmse, eval = F, fig.height = 7, fig.cap = 'Treatment Effect Heterogeneity: Root mean squared error and bias for Monte Carlo simulations from the first 20 DGPs in Dorie et al. (2019), 10 replications each. Values greater in absolute value than 4 are plotted at 4 and labeled with their actual value.'}


sim_results_plot <- sim_results_dgp %>%
  group_by(`Treat. heterogeneity`, method_estimator) %>%
  summarize(bias = mean(ate - truth, na.rm = T),
            rmse = sqrt(mean((ate - truth)^2, na.rm = T))
              ) %>%
  pivot_longer(c(bias, rmse)) %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(label = ifelse(abs(value) > 4, round(value), NA),
         value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

(sim_results_plot %>%
  filter(`Treat. heterogeneity` == 'high') %>%
  arrange(desc(name), value) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point() +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  # facet_wrap(~`Treat. heterogeneity`, nrow = 2, scales = 'free_x') +
  labs(x = '', y = '', title = 'High Treatment Effect Heterogeneity') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))
  ) /
  (sim_results_plot %>%
  filter(`Treat. heterogeneity` == 'none') %>%
  arrange(desc(name), value) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point()) +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  # facet_wrap(~`Treat. heterogeneity`, nrow = 2, scales = 'free_x') +
  labs(x = '', y = '', title = 'No Treatment Effect Heterogeneity') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))


```


## Do results vary by sample size?

In the above simulations, double robust methods have only slightly outperformed traditional methods. Is the issue with previous results simply that the sample size of the simulation data ($n = 4,802$) is too small for double robust methods to seriously outperform traditional methods? The double robust methods reviewed here have been shown to have lower bias asymptotically, so perhaps their superiority to traditional or single robust methods will be starker in larger samples. To test this, this section uses simulated datasets of varying sizes, from 150 to 96,040 (20 times the original sample size). These datasets are also derived from the @dorie_2019_automated `aciccomp2016` package, using parameter set 7, a fairly nonlinear DGP with high treatment effect heterogeneity. For sample sizes less than 4,802, the sample is randomly drawn from a randomly generated 4,802-unit sample. For sample sizes greater than 4,802, the design matrix (but not the outcome variable) is duplicated, then fed into the `dgp_2016` function. This preserves covariate distributions but retains stochasticity in the outcome.  

Table \@ref(tab:dorie-size) presents the four methods with the lowest RMSE for each sample size (the Supplementary Material presents a table with full results). Again, AIPW, TMLE, and G-computation methods that incorporate the SuperLearner or GRF dominate. There are two exceptions: In the tiny sample of 150, IPW (SuperLearner) and IPW (GRF) figure into the best four methods. PSM is the best-performing method in samples of 1200, but it fails to estimate 17 of the 20 datasets. (With 58 covariates, some of the methods fail in smaller samples.)  

Figure \@ref(fig:dorie-size-fig) presents RMSE for all sample sizes for all methods. In the smallest samples, GRF and SuperLearner are able to provide estimates, while traditional methods fail. Beginning at 1,200 observations, all methods are able to provide estimates for all datasets, with the exception of PSM, which fails to calculate even in some large samples. For most methods, RMSE decreases nearly monotonically as sample size grows. Two exceptions are IPW (logit) and AIPW (OLS/logit), whose error is highest in the maximum sample of 96,040, likely due to extreme logit estimates. In addition, the rank order of methods is nearly constant across sample sizes. DML does not achieve lower RMSE than OLS, PSM, or the Lin estimator in most sample sizes, and methods using the SuperLearner or GRF perform the best across sample sizes.   

In sum, although methods incorporating SuperLearner or GRF do the best in most sample sizes, traditional methods still perform fairly well, achieving low RMSE once the sample size is large enough for them to stably compute.  




```{r dorie-size-fig-old, fig.height = 8, eval = F, fig.cap = 'RMSE of Monte Carlo simulations using DGP 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse)) +
#   geom_point() +
#   facet_wrap(~method_estimator, ncol = 1, scales = 'free')


# # text labels
# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method, estimator, method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(estimator = if_else(is.na(estimator), 'NA', estimator)) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse, color = estimator, label= method)) +
#   geom_text() +
#   scale_y_continuous(trans='log10')


size_plot <- sim_results %>%
  filter(set %in% c('small', 'large')) %>%
  group_by(method, estimator, method_estimator, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = median(comp_time),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  group_by(method_estimator) %>%
  mutate(min_size = min(size*bias/bias, na.rm = T)) %>%
  ungroup() %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
         label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
         label_max = if_else(size == max(as.numeric(size)), 
                             as.character(method_estimator), NA_character_),
         size = factor(size, levels = sort(unique({.$size})))) 

text_size = 3

(size_plot %>%
  filter(method %in% c('ols', 'psm', 'ipw', 'g-comp', 'lin')) %>%
  ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
  geom_line() +
  ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  scale_y_continuous(trans='log10') +
  theme(legend.position = 'none')) /
  (size_plot %>%
    filter(method %in% c('aipw', 'tmle', 'dml')) %>%
    ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
    geom_line() +
    ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                             size = text_size, segment.color = 'transparent') +
    ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                             size = text_size, segment.color = 'transparent') +
    scale_y_continuous(trans='log10') +
    theme(legend.position = 'none'))
```


```{r dorie-size}
# 
# 
# size_plot <- sim_results %>%
#   filter(set %in% c('small', 'large')) %>%
#   group_by(method, estimator, method_estimator, size) %>%
#   summarize(rmse = sqrt(mean((ate - truth)^2, na.rm = T))) %>%
#   group_by(method_estimator) %>%
#   mutate(min_size = min(size*rmse/rmse, na.rm = T)) %>%
#   ungroup() %>%
#   mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
#          method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
#          label_max = if_else(size == max(as.numeric(size)), 
#                              as.character(method_estimator), NA_character_),
#          size = factor(size, levels = sort(unique({.$size})))) 

size_plot <- sim_results %>%
  filter(set %in% c('small', 'large')) %>%
  group_by(method, estimator, method_estimator, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = median(comp_time),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  group_by(method_estimator) %>%
  mutate(min_size = min(size*bias/bias, na.rm = T)) %>%
  ungroup() %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
         label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
         label_max = if_else(size == max(as.numeric(size)), 
                             as.character(method_estimator), NA_character_),
         size = factor(size, levels = sort(unique({.$size})))) 

size_plot %>%
  group_by(size) %>%
  slice_min(rmse, n = 4) %>% 
  # mutate(rank = paste('Rank', row_number())) %>%
  mutate(rank = c('Lowest', 'Second-lowest', 'Third-lowest', 'Fourth-lowest')) %>%
  ungroup() %>%
  mutate(rmse = round(rmse, 3),
         value = paste(method_estimator, rmse, sep = ': ')) %>%
  select(size, value, rank) %>%
  pivot_wider(names_from = rank) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Sample size: Four lowest RMSE methods by sample size for Monte Carlo simulations using DGP 7 from Dorie et al. (2019), 20 replications each')
  

```

```{r dorie-size-fig, fig.height = 8, fig.cap = 'RMSE of Monte Carlo simulations using DGP 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each'}




text_size = 2.5

# best_methods <- size_plot %>%
#   group_by(size) %>%
#   slice_min(rmse, n= 5) %>%
#   pull(method_estimator) %>%
#   unique()

best_methods <- size_plot %>%
  filter(size %in% c(300, 96040)) %>%
  group_by(size) %>%
  slice_min(rmse, n= 10) %>%
  pull(method_estimator) %>%
  unique()

size_plot %>%
  # filter(method_estimator %in% best_methods) %>%
  ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
  geom_line() +
  ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  scale_y_continuous(trans='log10', breaks = c(0.1, 0.5, 1, 10, 100)) +
  theme(legend.position = 'none') +
  labs(x = 'Size (n)')
```








<!-- # LaLonde NSW Data -->

<!-- In the Supplementary Material, I provide another evaluation of these methods, using data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW), as provided by @dehejia_1999_causal. LaLonde compared experimental estimates to control samples drawn from the Panel Study of Income Dynamics (PSID) and Westat's Matched Current Population Survey-Social Security Administration File (CPS). Following @dehejia_1999_causal, I present results for the original samples analyzed by @lalonde_1986_evaluating, but I also include results using a subsample of the experimental group that has 1974 earnings data available.   -->

<!-- Results are somewhat difficult to interpret. In the original LaLonde data, most methods fail to estimate treatment effects in the observational data with the same sign as the experimental estimate. The exception is the PSID-3 sample, which includes only men who were not working in the spring of 1976 or 1975 and is thus more comparable to the individuals were recruited to the NSW study. When 1974 earnings are included as a covariate, many of the methods provide estimates across samples that are close to the experimental estimates. OLS, PSM, G-computation (SuperLearner), AIPW (SuperLearner), TMLE (SuperLearner), and DML (OLS/logit) appear the most reliable, while IPW (logit), IPW (GRF), IPW (SuperLearner), the Lin estimator, and DML (GRF) prove unstable. It is surprising that the Lin estimator provides unstable results in the NSW data, while it is among the lowest-RMSE methods in the simulation study. This highlights the complexity of doing causal inference with observational data where ignorability may not hold.   -->

# Discussion and Conclusion

This paper aims to provide an introduction to and evaluation of double robust methods for covariate adjustment in causal inference. By comparing the double robust methods of AIPW, TMLE, and DML to more traditional statistical methods such as OLS and PSM as well as flexible "single robust" methods such as G-computation and the Lin estimator, it allows evaluation of whether these methods are worth the effort and (computational) time for social scientists to adopt them.  

Results are nuanced. In the full range of simulated data, AIPW and TMLE with a SuperLearner or GRF are able to obtain lower error than OLS or PSM. However these differences are quite small, and non-double-robust G-computation with the same flexible machine learning methods performs just as well as the double robust methods. DML does not perform as well as AIPW or TMLE (however, a version of DML that allows for heterogeneous effects might perform better; see Chernozhukov et al. [-@chernozhukov_2018_double, p. C35]). The Lin estimator performs slightly better than OLS or PSM, without any increase in computation time. Double robust methods relying on logit models to estimate propensity weights have the greatest error, likely due to small estimated propensity scores; researchers should use these methods with caution.  

Methods that come out on top in the full range of simulations also tend to do the best regardless of the data generating process, though the Lin estimator and OLS rise in the rankings when the true treatment and outcome models are linear. As sample size varies, the same rank order generally holds, though traditional methods are unable to produce estimates in small samples when there are many covariates, while the double robust methods are able to do so.  

<!-- In replications of prominent sociology articles, these methods do sometimes change conclusions. In the examples given in this paper, coefficients shift somewhat and sometimes change sign and significance. DML with a SuperLearner shows results that differ more dramatically from the OLS or logistic regressions that the authors originally used, while AIPW with GRF produces estimates that are, for the most part, fairly close to those of the original analyses. This may be related to the higher error shown in the simulations for DML; this method may be somewhat more unstable.   -->

What might explain the variation in performance across methods and estimators? One major reason could be the presence of treatment effect heterogeneity, which is high in 18 out of the 20 DGPs studied here. When the treatment effect does not vary systematically by other covariates, then OLS regression correctly targets the ATE. However, when the treatment effect is heterogeneous -- meaning it depends on the values of other covariates -- the OLS estimate will diverge from the ATE [@angrist_2009_mostly; @hazlett_2024_demystifying]. In this case, it is important to use a method that allows the treatment effect to vary in this way. Two of the double robust methods presented here -- AIPW and TMLE -- allow for this heterogeneity, and they are the top performing methods. The "single robust" G-computation also accounts for heterogeneity by estimating separate models for treated and untreated groups, and it performs nearly as well as AIPW and TMLE. The Lin estimator constitutes another heterogeneity-robust option and performs fairly in these simulations (recall that G-computation with OLS regressions is equivalent to the Lin estimator [@hazlett_2024_demystifying]). Although the Lin estimator's underling OLS regressions make the strong assumption of linearity, this assumption may regularize estimates in a way that actually reduces average prediction error. Overall, these results are in line with recent attention in sociology to heterogeneous treatment effects and modeling them appropriately [@brand_2021_uncovering; @zhou_2022_attendance; @luo_2021_heterogeneous].  

This paper has a number of limitations. First, although it considers some of the most popular double robust, machine learning, traditional, and single robust methods, there are many methods that it does not evaluate, including variations and extensions of the three methods (such as a heterogeneity-robust DML [@chernozhukov_2018_double, p. C35]). Future research could also consider the efficacy of another set of double robust methods: Calibration methods such as Entropy Balancing have been shown to be double robust (under certain assumptions) despite not explicitly modeling the outcome or treatment assignment [@zhao_2017_entropy]. Second, although the simulations cover a wide range of data generating processes, they only consider continuous outcomes and binary treatments; simulations with binary or categorical outcomes and continuous or multi-armed treatments may yield different results. Third, this paper has considered only cross-sectional data; in longitudinal settings with time-varying confounders, double robust methods may more clearly outperform other methods [@tran_2019_double]. Finally, in considering only functional form misspecification, the simulations in this paper do not consider situations where ignorability does not hold. In particular, this paper does not evaluate situations where causal identification is misspecified [@keil_2018_resolving]. Future research should assess violations of this and other assumptions underlying these methods.    

In conclusion, while double robust methods are useful for social scientists to understand due to their increasing popularity, they may not necessarily perform better than other methods. For the greatest accuracy, results here suggest that researchers should opt for AIPW, TMLE, or the non-double-robust G-computation in conjunction with a flexible machine learning algorithm. If computation time is a concern -- for example, if a researcher is calculating bootstrapped estimates, using replicate weights for complex survey data, or dealing with an extremely large sample -- then the Lin estimator is a good choice, with robustness to heterogeneous treatment effects and very low computation time. On the other hand, in most of the simulations presented here, standard OLS or PSM estimates do not diverge widely from top-performing methods. Studies that rely on these traditional methods may still have fairly accurate results.



<!-- Rather than choosing the most flexible or double robust model, researchers should choose a model that is robust to heterogeneous effects, if they have any suspicion that such effects exist. Researchers may opt for AIPW, TMLE, or G-computation with a flexible machine learning algorithm, which produce the most accurate results in this study. -->

<!-- if researchers want small gains in accuracy, they may opt for AIPW, TMLE, or G-computation with a flexible machine learning algorithm. But these methods are computationally costly, taking as long as two minutes per dataset of 4,802 observations and 52 covariates, while OLS, PSM, and the Lin estimator each take a fraction of a second.^[The `grf` and `DoubleML`  packages are able to achieve faster computation time, at under 10 seconds in these samples. See tables of results in the Supplementary Material.] While this computation time may seem trivial, it can balloon in larger samples -- as Table S3 in the Supplemental Material shows, GRF and SuperLearner estimators on samples of 96,040 took over 30 minutes to run, while OLS took only one second. In large complex survey data where proper standard errors computation requires the use of replicate weights, repeated estimation with these flexible machine learning estimators may result in unacceptably long computation time. -->



<!-- In certain circumstances, however, these double robust methods may be a better choice. Especially when paired with a highly flexible estimator like a SuperLearner or GRF, these methods may be slightly more accurate, and they can be useful when the number of covariates is high or even exceeds the number of observations. In longitudinal settings with time-varying confounders, they may be more helpful [@tran_2019_double].   -->







<!-- \newpage -->

<!-- # (APPENDIX) Appendix {-} -->

<!-- # Appendix -->

<!-- ## DML with heterogeneous treatment effects -->


<!-- $$\psi = \mu_1(\mathbf X_i) - \mu_0(\mathbf X_i) + \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)}  -->
<!-- - \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} - \theta$$ -->




\newpage

# (APPENDIX) Appendix {-}

# R Code

## AIPW

The R code below implements AIPW with truncation of extreme weights. As with all of the double robust methods reviewed here, we begin with predicted values (such as from a machine learning algorithm) for the outcome for treated units `mu1_pred` and untreated units `mu0_pred` as well as predicted values for treatment assignment probability `pi_pred`. We also have `d`, the vector of actual treatment assignments, and `y`, the observed outcome values.

```{r aipw-demo, eval = F, echo = T}
require(tidyverse)

aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # Truncate extreme values of the weights
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  # Calculate the predicted outcome value for treated units
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  # Calculate the predicted outcome value for untreated units
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  # Calculate the ATE
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}
```

## TMLE

Here is R code to implement TMLE with predicted outcome values `mu1_pred` and `mu0_pred` and predicted probability of treatment `pi_pred`. Since the outcome is bounded and continuous, it is transformed to fall between 0 and 1 via $\tilde Y_{i} = [Y_i - \min(Y)]/[(\max(Y)-\min(Y)]$.
```{r tmle-demo, eval = F, echo = T}
# Functions for normalization and de-normalization
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  # Normalize the outcome variable
  mu1_pred <- normalize(mu1_pred, y)
  mu0_pred <- normalize(mu0_pred, y)
  y_tilde <- normalize(y, y))
  
  n <- length(y)

  # Calculate clever covariates
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  # Estimate fluctuation parameter through maximum likelihood estimation
  epsilon <- glm(y_tilde ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  # Targeted estimates of the potential outcomes
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H1))
  
  # Estimate ATE
  ATE <- mean(target_1 - target_0)
  return(denormalize(ATE, y))
}
```


## DML

R code to implement DML is shown below. Since DML involves sample splitting, this code is a little different from the above examples. We start with observed outcome values `y`, treatment assignment `d`, and covariate matrix `x`. First, a pre-processing function `dml_pre()` randomly splits the sample, outputting $I$ and $I^c$ sets of each of these variables. The second step predicts outcome values and treatment probabilities for each half of the sample, using models fit to the other half. In the code chunk here, generalized random forests from the `grf` package are used to predict these, but any prediction algorithm can be used. Finally, a post-prediction function `dml_post()` performs the residual-on-residual regression for each half of the sample and finds the average of the two estimates to produce an ATE estimate. 
```{r dml-demo, eval = F, echo = T}
# Pre-processing: sample splitting
dml_pre <- function(y, d, x, seed = 1758){
  set.seed(seed)
  
  n <- length(y)
  n_2 <- round(n/2)
  
  # Split the sample
  random_vec <- sample(1:n, n, replace = F)
  I <- random_vec[1:n_2]
  I_c <- random_vec[(n_2+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,],
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

# Predictor function: in this case, generalized random forests
grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  # Train on the I_c sample, predict on the I sample
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  # Train on the I sample, predict on the I_c sample
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

# Implement DML: takes outputs from pre_dml() and grf_dml()
dml_post <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  # Residual-on-residual regression for each sample separately
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  # Average estimates from each sample
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

dml_pre_out <- dml_pre(y = y, d = d, x = x)
grf_dml_out <- do.call(grf_dml, dml_pre_out)
dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
```

\newpage


# References

<div id="refs"></div>