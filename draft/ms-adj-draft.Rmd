---
output:
  # bookdown::word_document2:
    # reference_docx: "word-template.docx"
  bookdown::pdf_document2:
    toc: no
    number_sections: yes
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
    latex_engine: xelatex
always_allow_html: true
header-includes:
  #- \usepackage{setspace}\doublespace
  # - \usepackage[nolists, fighead, tabhead]{endfloat}
  # - \usepackage{endnotes}
  # - \let\footnote=\endnote
  # - \setlength{\headheight}{14.5pt}
  # - \setlength{\headheight}{13.6pt}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \lhead{N.I. Hoffmann}
  - \rhead{`r format(Sys.time(), '%B %e, %Y')`}
# - \newcommand{\beginsupplement}{\setcounter{table}{0}  
# \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} 
# \renewcommand{\thefigure}{A\arabic{figure}}}

editor_options: 
  chunk_output_type: console


citeproc: no
fontfamily: mathpazo
#fontsize: 11pt
# geometry: margin=.6in
indent: yes
link-citations: yes
linkcolor: blue
lang: 'en-US'

bibliography: "/Users/nathan/Documents/My Library.bib" 
# bibliography: "My Library.bib"  
csl: apa.csl
# csl: american-sociological-association.csl

title: "Double Robust, Flexible Adjustment Methods for Causal Inference: An Overview and an Evaluation"
# subtitle: "American Sociological Association Annual Meeting 2024"
author:  Nathan I. Hoffmann, Department of Sociology, UCLA
date: "`r format(Sys.time(), '%B %e, %Y')`"

abstract: "Double robust methods for flexible covariate adjustment in causal inference have proliferated in recent years. Despite their apparent advantages, these methods remain underutilized by social scientists. It is also unclear whether these methods actually outperform more traditional methods in finite samples. This paper has two aims: It is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, and it compares these methods to more traditional statistical methods. It does this by using both simulated data where the treatment effect estimate is known, and then using comparisons of experimental and observational data from the National Supported Work Demonstration. Methods covered include Augmented Inverse Probability Weighting, Targeted Maximum Likelihood Estimation, and Double/Debiased Machine Learning. Results suggest that these methods do not necessarily outperform OLS regression or matching on propensity score estimated by logistic regression, even in cases where the data generating process is not linear."

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T)
options("yaml.eval.expr" = TRUE)

library(SuperLearner)
library(broom)
library(knitr)
library(kableExtra)
library(here)
library(haven)
library(tidyverse)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


options("yaml.eval.expr" = TRUE, scipen = 3, digits = 2)

uclablue = '#2774AE'
gray = '#808080'
black = '#000000'
ucla_palette = c(black, uclablue, gray)

# theme_set(theme_cowplot(font_family = 'Palatino') + 
theme_set(theme_classic(base_family = 'Palatino') + 
      theme(legend.title=element_blank(), 
         panel.grid.major.y = element_line('grey80'),
         legend.background = element_rect(fill = alpha("white", 0.5))
         ))
ggplot <- function(...) ggplot2::ggplot(...) + 
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")

kable <- function(...) knitr::kable(..., format.args = list(big.mark = ","))
```




```{r load}


unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()
```

```{r load-sims, eval = F}
lalonde_bootstrap <- function(dataset, sample, include_74 = F, iter = 100){
  
  lalonde_variables <- c('age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75', 're75_0')
  if(include_74 == F){
        dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
        
      } else {
        lalonde_variables <- c(lalonde_variables, 're74', 're74_0')
        dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
      }
  
  bootstrap_list <- list()
  
  for(i in 1:iter){
    dataset_bootstrap <- sample_n(dataset, nrow(dataset), replace = T)
    
    bootstrap_list[[i]] <- list(y = dataset_bootstrap$re78,
                  z = ifelse(dataset_bootstrap$treat == 1, 'trt', 'ctl'),
                  x = dataset_bootstrap[,lalonde_variables],
                  y.1 = NA, y.0 = NA,
                  dataset = i,
                  set = sample,
                  size = nrow(dataset_bootstrap))
  }
  
  return(bootstrap_list)
}

set.seed(1859)
sims_lalonde <- c(
  lalonde_bootstrap(filter(lalonde_exp, treat == 0), sample = 'lalonde experimental original'),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 original'),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 original'),
  lalonde_bootstrap(filter(lalonde_exp_74, treat == 0), sample = 'lalonde experimental 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 74', include_74 = T))

# sims <- readRDS(here('files', 'sims.RDS'))

sims_main <- readRDS(here('files_main', 'sims.RDS'))
sims_large <- readRDS(here('files_large', 'sims_large.RDS'))
sims_small <- readRDS(here('files_small', 'sims_small.RDS'))

sims <- sims_main %>% append(sims_small) %>% append(sims_large) %>% append(sims_lalonde)

# sims <- sims %>% append(sims_large)

# sims <- readRDS(here('files', 'sims_small.RDS'))
 
# for(i in 1:length(sims_small)){
#   # sims_large[[i]]$size <- nrow(sims_large[[i]]$x)
#   sims_small[[i]]$dataset <- 7
# }
# saveRDS(sims_large, here('files_large', 'sims_large.RDS'))
# 
# 
# sims_combined <- sims %>% append(sims_small) %>% append(sims_large)
# 
# index_df <- data.frame(dorie_dataset = sapply(sims_combined, function(x) x$dataset),
#                        set = sapply(sims_combined, function(x) x$set),
#                        size = sapply(sims_combined, function(x) x$size)) %>%
#   mutate(dataset = c(1:length(sims), 1:length(sims_small), 1:length(sims_large)))
# 
# lm_df <- read_csv(here('files_main', 'lm_df.csv')) %>% 
#   mutate(set = c(rep('main', 200), rep('linear', 200))) %>%
#   bind_rows(read_csv(here('files_small', 'lm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'lm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# 
# write_csv(lm_df, here('files', 'lm_df.csv'))
# 
# psm_df <- read_csv(here('files_main', 'psm_df.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small', 'psm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'psm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(lm_df, here('files', 'psm_df.csv'))
# 
# 
# aipw_df <- read_csv(here('files_main/homemade', 'aipw.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_df, here('files/homemade', 'aipw.csv'))
# 
# 
# aipw_trunc_df <- read_csv(here('files_main/homemade', 'aipw_trunc.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_trunc_df, here('files/homemade', 'aipw_trunc.csv'))
# 
# tmle_df <- read_csv(here('files_main/homemade', 'tmle.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'tmle.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'tmle.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(tmle_df, here('files/homemade', 'tmle.csv'))
# 
# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files/homemade', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files/homemade', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files/homemade', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files/homemade', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files/homemade', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files/homemade', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))



## add lalonde ####
# read_csv(here('files_combined', 'lm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lm_df.csv'))) %>%
#   write_csv(here('files', 'lm_df.csv'))
# 
# 
# read_csv(here('files_combined', 'psm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'psm_df.csv'))) %>%
#   write_csv(here('files', 'psm_df.csv'))
# 
# read_csv(here('files_combined', 'lin.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lin.csv'))) %>%
#   write_csv(here('files', 'lin.csv'))
# 
# read_csv(here('files_combined', 'gcomp.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'gcomp.csv'))) %>%
#   write_csv(here('files', 'gcomp.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw.csv'))) %>%
#   write_csv(here('files/homemade', 'aipw.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw_trunc.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw_trunc.csv'))) %>%
#   write_csv(here('files/homemade', 'aipw_trunc.csv'))
# 
# read_csv(here('files_combined/homemade', 'tmle.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'tmle.csv'))) %>%
#   write_csv(here('files/homemade', 'tmle.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_ols_logit.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_ols_logit.csv'))) %>%
#   write_csv(here('files/homemade', 'dml_ols_logit.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_grf.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_grf.csv'))) %>%
#   write_csv(here('files/homemade', 'dml_grf.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_superlearner.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_superlearner.csv'))) %>% 
#   write_csv(here('files/homemade', 'dml_superlearner.csv'))
# 
# 
# readRDS(here('files_combined/homemade', 'ols_logit_pred.RDS')) %>%
#   bind_rows(readRDS(here('files_lalonde/homemade', 'ols_logit_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'ols_logit_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'grf_pred.RDS')) %>%
#   bind_rows(readRDS(here('files_lalonde/homemade', 'grf_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'grf_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'superlearner_pred.RDS')) %>%
#   bind_rows(readRDS(here('files_lalonde/homemade', 'superlearner_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'superlearner_pred.RDS'))




# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files/homemade', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files/homemade', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files/homemade', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files/homemade', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files/homemade', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files/homemade', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))

```

```{r sim-homemade, eval = F}
sim_homemade <- function(n, m = NULL){
  if(is.null(m)){
    m <- round(n/100)
  }
  
  dat <- replicate(m, rnorm(n, 0, 5)) %>% 
    as.data.frame() 
  dat_original <- dat
  
  # first make treatment indicator
  dat <- data.frame(1, dat)
  coefs <- rnorm(m+1, 0, 2)
  d <- round(1/(1+exp(-as.matrix(dat) %*% as.matrix(coefs))))
  
  dat <- data.frame(dat, d)
  
  # Set treatment effect to 2
  coefs <- c(rnorm(m+1, 0, 2), 2)
  y <- as.matrix(dat) %*% as.matrix(coefs) + rnorm(1)
  
  dat <- data.frame(dat, y)
  
  
  dat_new <- dat_original
  
  transformers <- rnorm(m, 0, 2)
  
  for(i in 1:m){
    dat_new[,i] <- dat_new[,i] * transformers[i]
  }
  
  # m/4 squared terms
  dat_new <- dat_new %>% 
    mutate(across(sample(1:m, m/4, replace = F), function(x) x^2))
  
  # m/4 exponentiated 
  dat_new <- dat_new %>% 
    mutate(across(sample(1:m, m/4, replace = F), function(x) exp(x)))
  
  # m/4 sin 
  dat_new <- dat_new %>% 
    mutate(across(sample(1:m, m/4, replace = F), function(x) sin(x)))
  
  # m/2 2-way interactions
  for(i in 1:round(m/2)){
    index <- sample(1:m, 2, replace = F)
    dat_new[, index[1]] <- dat_new[,index[1]] * dat_new[,index[2]]
  }
  
  # m/4 3-way interactions
  for(i in 1:round(m/4)){
    index <- sample(1:m, 3, replace = F)
    dat_new[, index[1]] <- dat_new[,index[1]] * dat_new[,index[2]] * dat_new[,index[3]]
  }
  
  dat_final <- data.frame(dat_new, d = dat[,'d'], y = dat[,'y'])
  return(dat_final)
}

# 500, 1000, 5000, 10k, 50k, 100k
# Set m = 1% of n
# 50 of each 
# then set n = 5000 and m = 10, 50, 200, 1000
set.seed(185)
sim_homemade_list <- list()
for(n in c(500, 1000, 5000, 1e3, 5e3, 1e5)){
  print(paste0('n = ', n))
  for(i in 1:50){
    print(i)
    sim_homemade_list[[paste0(n, i)]] <- sim_homemade(n) %>%
      mutate(n = n, m  = n/100, dataset = i, simulation = 'n')
  }
}
for(m in c(10, 50, 200, 500, 1000)){
  print(paste0('m = ', m))
  for(i in 1:50){
    print(i)
    sim_homemade_list[[paste0(n, i)]] <- sim_homemade(n = n, m = m) %>%
      mutate(n = n, m  = m, dataset = i, simulation = 'm')
  }
}

saveRDS(sim_homemade_list, here('files', 'sim_homemade.RDS'))


```

```{r github-datageneration, eval = F}
# from https://github.com/QuantLet/DataGenerationForCausalInference

### This generates the Simulation Data as a dataframe
# by Daniel Jacob (daniel.jacob@hu-berlin.de) 

# Arguments to specify are: 
# Y = Outcome (dependend variable). Either continuous or binary. 
# N = Number of observations (real number)
# k = Number of covariates (real number). At least 10 
# random_d = treatment assignment: (Either T for random assignment or F for confounding on X)
# theta = treatment effect: (Either real number for only one theta, or "binary" {0.1,0.3}, "con" for continuous values (0.1,0.3) or "big" for {1,0.4})
# var = Size of the variance (Noise-level)

#Required Packages
if(!require("clusterGeneration")) install.packages("clusterGeneration"); library("clusterGeneration")
if(!require("mvtnorm")) install.packages("mvtnorm"); library("mvtnorm")



datagen <- function(N,y,k,random_d,theta,var) {
  
  N = N
  k = k
  b = 1 / (1:k)
  # = Generate covariance matrix of z = #
  sigma <- genPositiveDefMat(k, "unifcorrmat")$Sigma
  sigma <- cov2cor(sigma)
  
  
  z <- rmvnorm(N, sigma = sigma) # = Generate z = #
  
  
  ### Options for D (m_(X))
  if (random_d == T) {
    d <- rep(c(0, 1), length.out = N)
  } else {
    d_prop <- pnorm(z %*% b) # D is dependent on Za
    d <- as.numeric(rbinom(N, prob = d_prop, size = 1))
  }
  
  
  ### Options for theta
  if (theta == "con") {
    theta_s <- as.vector(sin(z %*% b) ^ 2)
    theta <-
      (theta_s - min(theta_s)) * (0.3 - 0.1) / (max(theta_s) - min(theta_s)) +
      0.1
  } else if (theta == "binary") {
    theta_low <-
      rbinom(N, pnorm((z[, 6] * (z[, 1] %*% t(
        z[, 5]
      )) * z[, 2]) ^ 2), size = 1)
    theta <-
      ifelse(theta_low == 1, 0.3, 0.1)
  } else if (theta == "big") {
    theta_big <-
      rbinom(N, pnorm((z[, 6] * (z[, 1] %*% t(
        z[, 5]
      )) * z[, 2]) ^ 2), size = 1)
    theta <- ifelse(theta_big == 1, 1, 0.4)
  }  else {
    theta == theta
  }
  
  
  g <- as.vector(cos(z %*% b) ^ 2)
  
  if(y=="binary") {
    y1 <- theta * d + g 
    y1.1 <- rbinom(N,prob=pnorm(scale(y1)),size=1)
    y <- y1.1
  } else {y <- theta * d + g + rnorm(N,0,var)}
  
  data <- as.data.frame(y)
  data <- cbind(data, theta, d, z)
  colnames(data) <- c("y", "theta", "d", c(paste0("V", 1:k)))
  
  return(data)
}


### Example
dataset <- datagen(y="binary",N = 2000, k = 20, random_d = F, theta = "binary", var = 1)
summary(dataset)
str(dataset)

glm(y ~ ., data = dataset, family = binomial(link = 'logit')) %>%
  summary()
```


```{r trash, eval = F}
dat_transform <- function(dat){
  # first make treatment indicator
  coefs <- runif(ncol(dat), -1, 1)
  
  
  # m/4 squared terms
  dat <- dat %>% 
    mutate(across(sample(1:m, m/4, replace = F), function(x) x^2))
  
  # m/4 exponentiated 
  dat <- dat %>% 
    mutate(across(sample(1:ncol(dat), m/4, replace = F), function(x) exp(x)))
  
  # m/4 sin 
  dat <- dat %>% 
    mutate(across(sample(1:ncol(dat), m/4, replace = F), function(x) sin(x)))
  
  # m/2 2-way interactions
  dat <- bind_cols(dat,
    replicate(m/2, {
      index <- sample(1:m, 2, replace = F)
      dat[,index[1]] * dat[,index[2]]
      }, simplify = T))
  
  # m/4 3-way interactions
  dat <- bind_cols(dat,
    replicate(m/4, {
      index <- sample(1:m, 3, replace = F)
      dat[,index[1]] * dat[,index[2]] * dat[,index[3]]
      }, simplify = T))
  
  
  
  coefs <- runif(ncol(dat), -1, 1)
  
  y <- as.matrix(dat) %*% as.matrix(coefs)
  
  return(y)
}

sim_homemade <- function(m, n){
  m <- m-2
  dat <- replicate(m, rnorm(n, 0, 5)) %>%
    cbind(replicate(m/4, rbinom(n, 1, .5))) %>%
    as.data.frame()
  dat_original <- dat
  
  # first make treatment indicator
  dat <- data.frame(1, dat)
  coefs <- rnorm(ncol(dat), 0, 2)
  d <- round(1/(1+exp(-as.matrix(dat) %*% as.matrix(coefs))))
  
  dat <- data.frame(dat, d)
  
  # Set treatment effect to 2
  coefs <- c(rnorm(m+1, 0, 2), 2)
  y <- as.matrix(dat) %*% as.matrix(coefs) + rnorm(1)
  
  dat <- data.frame(dat, y)
  
  
  dat_new <- dat_original
  
  transformers <- rnorm(m, 0, 2)
  
  for(i in 1:m){
    dat_new[,i] <- dat_new[,i] * transformers[i]
  }
  
  # m/4 squared terms
  dat_new <- dat_new %>% 
    mutate(across(sample(1:m, m/4, replace = F), function(x) x^2))
  
  # m/4 exponentiated 
  dat_new <- dat_new %>% 
    mutate(across(sample(1:ncol(m), m/4, replace = F), function(x) exp(x)))
  
  # m/4 sin 
  dat_new <- dat_new %>% 
    mutate(across(sample(1:ncol(m), m/4, replace = F), function(x) sin(x)))
  
  # m/2 2-way interactions
  for(i in 1:round(m/2)){
    index <- sample(1:m, 2, replace = F)
    dat_new[, index[1]] <- dat_new[,index[1]] * dat_new[,index[2]]
  }
  
  # m/4 3-way interactions
  for(i in 1:round(m/4)){
    index <- sample(1:m, 3, replace = F)
    dat_new[, index[1]] <- dat_new[,index[1]] * dat_new[,index[2]] * dat_new[,index[3]]
  }
  
  dat_final <- data.frame(dat_new, d = dat[,'d'], y = dat[,'y'])
  return(dat_final)
}




```


```{r functions}
## Pred functions ####
ols_logit_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  mu_mod <- lm(y ~  d + ., data.frame(y, d, x))
  mu1_pred <- predict(mu_mod, newdata = data.frame(y, d = 1, x))
  mu0_pred <- predict(mu_mod, newdata = data.frame(y, d = 0, x))
  
  pi_mod <- glm(d ~ ., data.frame(y, x), family = binomial(link = 'logit'))
  pi_pred <- predict(pi_mod, type = 'response')

  
  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}


grf_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  forest_mu <- grf::regression_forest(X = data.frame(d, x), Y = y, 
                                 tune.parameters = "all")
  mu0_pred <- predict(forest_mu, newdata = data.frame(d = 0, x))$predictions
  mu1_pred <- predict(forest_mu, newdata = data.frame(d = 1, x))$predictions
  
  forest_pi <- grf::regression_forest(X = x, Y = d, tune.parameters = "all")
  pi_pred <- predict(forest_pi, newdata = x)$predictions

  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}

superlearner_pred <- function(y, d, x, folds = 5, seed = 158){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  set.seed(seed)
  mu_fit <- SuperLearner(
    Y = y,
    X = data.frame(d, x),
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= gaussian()
  )
  
  pi_fit <- SuperLearner(
    Y = d,
    X = x,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= binomial()
  )
  

  return(list(
    mu0_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 0, x), type = 'response')$library.predict %*% mu_fit$coef),
    mu1_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 1, x), type = 'response')$library.predict %*% mu_fit$coef),
    pi_pred = as.numeric(predict(pi_fit, type = 'response')$pred),
    d = d,
    y = y))
}

## Methods ####

lm_sim <- function(dat){
  
  n <- length(dat)
  lm_list <- list()
  for(i in 1:n){
    start_time <- Sys.time()
    
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    tryCatch({
      lm_out <- tidy(lm(y ~ d + ., sim_dat))
      ate <- lm_out[[2,2]]
      
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
    
    lm_list[[i]] <- data.frame(dataset = i,
                               ate = ate,
                               #se = lm_out[[2,3]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                               dorie_dataset = dat[[i]]$dataset,
                               set = dat[[i]]$set,
                               size = dat[[i]]$size,
                               comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
    
  }
  
  # fail_count <- sum(sapply(lm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(lm_list) == n, fail_count, fail_count + (n - length(lm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(lm_list))
  
  # return(list(
  #   est_df = bind_rows(lm_list)
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  #   ))
}


psm_sim <- function(dat){
  
  n <- length(dat)
  psm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    
    start_time <- Sys.time()
    
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    
    tryCatch({
      form <- as.formula(paste0('d ~ ', paste(names(dat[[i]]$x), collapse = '+')))
      match_out <- MatchIt::matchit(form,
                             data = sim_dat,
                             method = 'nearest',
                             distance = 'glm') 
      
      # match_data <- MatchIt::match.data(match_out) 
      # apply(match_data, 1, unique)
      form2 <- as.formula(paste0('y ~ d + ', paste(names(dat[[i]]$x), collapse = '+')))
      
    
      psm_out <- lm(form2, 
                    MatchIt::match.data(match_out), 
                    weights = weights) %>%
        tidy()
      
      ate <- psm_out[[2,2]]
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
    
    psm_list[[i]] <- data.frame(dataset = i,
                                ate = ate,
                                # se = psm_out[[2,3]],
                                truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                                dorie_dataset = dat[[i]]$dataset,
                                set = dat[[i]]$set,
                                size = dat[[i]]$size,
                                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
  }
  
  # fail_count <- sum(sapply(psm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(psm_list) == n, fail_count, fail_count + (n - length(psm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(psm_list))
  
  # return(list(
  #   est_df = bind_rows(psm_list),
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  # )
}


aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}

aipw_calc_trunc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, .025) ~ quantile(pi_pred, .025),
  #   pi_pred > quantile(pi_pred, .975) ~ quantile(pi_pred, .975),
  #   T ~ pi_pred)
  
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}


tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(y)
  # H <- (d == 1)/pi_pred - (d==0)/(1-pi_pred)
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  epsilon <- glm(y ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  H_0 = (1-d)/(1-pi_pred)
  H_1 = d/pi_pred
  
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H_0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H_1))
  
  ATE <- mean((target_1 - target_0), na.rm = T)
  return(ATE)
}

dml_pre <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  n <- length(y)
  n_2 <- n/2
  n_2_1 = ifelse(round(n_2) == n_2, n_2, round(n_2))
  n_2_2 = ifelse(round(n_2) == n_2, n_2, round(n_2)+1)
  
  # split the sample
  random_vec <- sample(1:n, n)
  I <- random_vec[1:n_2_1]
  I_c <- random_vec[(n_2_1+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,], 
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

dml_post <- function(y_I, d_I, x_I = NULL, y_I_c, d_I_c, x_I_c = NULL,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

## Predictor functions
ols_logit_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- lm(y ~ ., data.frame(y = y_I_c, x_I_c))
  mu_pred1 <- predict(mu_mod1, newdata = data.frame(y = y_I, x_I))

  pi_mod1 <- glm(d ~ ., data.frame(d = d_I_c, x_I_c), 
                family = binomial(link = 'logit'))
  pi_pred1 <- predict(pi_mod1, 
                     newdata = data.frame(d = d_I, x_I), 
                     type = 'response')
  
  mu_mod2 <- lm(y ~ ., data.frame(y = y_I, x_I))
  mu_pred2 <- predict(mu_mod2, newdata = data.frame(y = y_I_c, x_I_c))

  pi_mod2 <- glm(d ~ ., data.frame(d = d_I, x_I), 
                family = binomial(link = 'logit'))
  pi_pred2 <- predict(pi_mod2, 
                     newdata = data.frame(d = d_I_c, x_I_c), 
                     type = 'response')
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, 
                                       tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, 
                                       tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

superlearner_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                             folds = 5){
  
  
  mu_mod1 <- SuperLearner(
    Y = y_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod1 <- SuperLearner(
    Y = d_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  
  mu_pred1 <- predict(mu_mod1, newdata = x_I, type = 'response')$library.predict %*% mu_mod1$coef
  pi_pred1 <- predict(pi_mod1, newdata = x_I, type = 'response')$pred
  
  mu_mod2 <- SuperLearner(
    Y = y_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod2 <- SuperLearner(
    Y = d_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c, type = 'response')$library.predict %*% mu_mod2$coef
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c, type = 'response')$pred
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

## Functions using double robust packages ####
aipw_sim <- function(dat, seed = 185){

  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  aipw_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = as.numeric(ifelse(dat[[i]]$z == 'trt', 1, 0)), 
                          dat[[i]]$x)
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, 
                                   W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #                              Y = sim_dat$y, W = sim_dat$d)
      
      aipw_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'AIPW')
      
      aipw_list[[i]] <- data.frame(d = aipw_out[[1]],
                                   se = aipw_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(aipw_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(aipw_list) == n, fail_count, fail_count + (n - length(aipw_list)))
  
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(aipw_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

tmle_sim <- function(dat, seed = 185){
  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  tmle_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #   Y = sim_dat$y, W = sim_dat$d)
      
      tmle_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'TMLE')
      
      tmle_list[[i]] <- data.frame(d = tmle_out[[1]],
                                   se = tmle_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(tmle_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(tmle_list) == n, fail_count, fail_count + (n - length(tmle_list)))
  
  end_time <- Sys.time()
  
 return(list(
      est_df = bind_rows(tmle_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

dml_sim <- function(dat, seed = 185){
  library(mlr3)
  library(mlr3learners)
  
  start_time <- Sys.time()
  
  set.seed(seed)
  
  n <- length(dat)
  
  dml_list <- list()
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    lgr::get_logger("mlr3")$set_threshold("warn")
    
    learner = lrn("regr.ranger", num.trees=500, 
                  max.depth=5, min.node.size=2)
    ml_l = learner$clone()
    ml_m = learner$clone()

    tryCatch({
      dml_out <- DoubleML::DoubleMLPLR$new(
        DoubleML::DoubleMLData$new(sim_dat,
                                 y_col = 'y',
                                 d_cols = 'd',
                                 x_cols = names(dat[[i]]$x)), 
        ml_l=ml_l, ml_m=ml_m)
      
      dml_out$fit()
      dml_list[[i]] <- data.frame(d = dml_out$all_coef[[1,1]],
                                  se = dml_out$all_se[[1,1]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(dml_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(dml_list) == n, fail_count, fail_count + (n - length(dml_list)))
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(dml_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
      )
}



## Other functions ####
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

perform <- function(est_df, label){
  est_df %>%
    # mutate(d = d/truth,
    #        truth = 1) %>%
    summarize(bias = mean(d - truth),
              percent_bias = bias/sd(d),
              rmse = sqrt(mean((ate - truth)^2)),
              mae = median(abs(d - truth))
              # fail_count = first(fail_count),
              # comp_time = first(comp_time)
              ) %>%
    mutate(label = label,
           n = nrow(est_df) + fail_count) %>%
    select(label, everything()) %>%
    return()
}
```


<!-- # Abstract -->

<!-- This thesis aims to compare different advanced adjustment methods. It will compare Imben & Rubins propensity matching, stabilized IPWs, TMLE, double machine learning (Chernozhukov et al 2018), Ratkovic (2021), and BART. -->

# Introduction

Statistical methods for flexible covariate adjustment in causal inference have proliferated in recent years. These methods have a number of strengths over traditional regression methods: They make few functional form assumptions, can accommodate large numbers of covariates, and produce easily interpretable treatment effect estimates. Many of these methods also have a "double robust" property: They estimate one model for the treatment exposure and another for the outcome, and as long as at least one is correctly specified, then the treatment effect will be estimated consistently. Despite their apparent advantages, these methods remain underutilized by social scientists. Part of the barrier has been lack of familiarity with these methods. It has also been unclear how these methods compare, or whether such methods actually perform better than traditional methods in finite samples. 
<!-- And when researchers want to use an existing R package for these methods, the array of possible estimation methods and other options can be dizzying.   -->

This paper makes advances on these fronts. First, it is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, explaining the methods to a social scientist audience. Second, it compares these methods to more traditional statistical methods using both simulations [@dorie_2019_automated] and the National Support for Work Demonstration (NSW) originally analyzed by @lalonde_1986_evaluating.  

Methods covered include Augmented Inverse Probability Weighting [AIPW, @glynn_2010_introduction], Targeted Maximum Likelihood Estimation [TMLE, @vanderlaan_2006_targeted], and Double or Debiased Machine Learning [DML, @chernozhukov_2018_double]. This paper reviews the theory behind these methods as well as simple R implementations of them on simulations and real data. These methods are compared to two methods commonly used by social scientists: ordinary least squares (OLS) regression, and matching on propensity scores estimated from logistic regression (PSM).  

# Motivation

## Literature Review

Although previous work has compared different methods for covariate adjustment, there has not yet been a thorough evaluation and comparison of the recent and popular double robust methods of AIPW, TMLE, and DML. @austin_2014_comparison compares methods for matching on the propensity score. @dorie_2019_automated report the results for the Atlantic Causal Inference Conference 2016 competition. Some of the methods used incorporate TMLE but not AIPW or DML.  @cousineau_2022_estimating evaluates the performance of optimization-baseds methods for causal inference, but these do not include the double robust methods covered in the current paper. @kang_2007_demystifying provide an overview and evaluation of double robust methods in the context of missing data. The authors consider AIPW but not TMLE or DML. @chatton_2020_gcomputation compare four methods: G-computation, IPW, full matching, and TMLE. These authors only consider one double robust method, and their focus is on omitted variable bias rather than determining which method is the most useful. @knaus_2022_double reviews double robust methods in an econometrics setting but does not compare them to traditional statistical methods for covaraite adjustment.



## Historical Overview 

According to @bang_2005_doubly, double robust methods have their origins in missing data models. @robins_1994_estimation and @rotnitzky_1998_semiparametric developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models, and @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference (since causal inference is fundamentally a missing data problem).

But @kang_2007_demystifying argue that double robust methods are older. They cite work by @cassel_1976_results, who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated. Arguably, double robust methods go back even further than this. The form of double robust methods is similar to residual-on-residual regression, which dates back to @frisch_1933_partial famous FWL theorem:

$$\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)}$$

where $\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$. This formulation writes the regression coefficient as composed of an outcome model ($\tilde Y_i$) and exposure model ($\tilde D_i$), the two models used in double robust estimators.

There are also links between double robust methods and matching with regression adjustment. This work goes back to at least @rubin_1973_use, who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.  

Today, double robust methods abound [e.g. @ratkovic_2023_relaxing; @sloczynski_2018_general; @arkhangelsky_2021_doublerobust; @dukes_2022_doubly; @kennedy_2023_semiparametric]. This paper focuses on three of the more popular or fundamental methods.

## Aims of Double Robust Methods
Double robust methods for covariate adjustment aim to overcome what many consider to be the downsides of both traditional statistical methods and flexible machine learning methods. Statistical methods that are popular with social scientists -- such as OLS regression and matching on propensity scores from logistic regression -- have two main weaknesses that double robust methods address. First, they assume simple (linear or transformed linear) functional forms. In the presence of highly nonlinear data generating processes, they may provide biased estimates. Second, these methods cannot handle large numbers of covariates relative to sample size. While some machine learning methods can produce estimates even when the number of covariates exceeds the number of observations (such as lasso), OLS fails in this case due to the $X^\top X$ matrix not being of full rank and hence not invertible. In cases with many covariates, but not more than the number of observations, estimation is unstable with many traditional statistical methods.  

Flexible machine learning methods also have their drawbacks. First, naive application of these methods can result in overfitting, with predictive accuracy maximized in sample but treatment effect estimation being biased. Furthermore, results of these machine learning methods can be difficult to interpret without further processing. Machine learning methods have often been developed with a focus on prediction rather than on producing treatment effect point estimates.  

Double robust methods for covariate adjustment are also motivated by the idea that many older methods ignore information present in the data. Methods tend to model either the outcome -- as in OLS regression and G-computation -- or only the treatment assignment -- as in propensity score matching or inverse probability weighting. Double robust methods, on the other hand, model both of these.


# Conceptual Overview

Double robust methods estimate two models:  

- an *outcome model*

$$\mu_d(X_i) = E(Y_i \mid D_i = d, X_i)$$

- and an *exposure model* (or treatment model or propensity score):

$$\pi(X_i) = E(D_i \mid X_i)$$

where $\mu_d(\cdot)$ is the model of control or treatment $D_i = d=\{0, 1\}$, $X_i$ is a vector of covariates for unit $i = 1, \ldots, N$ for treatment (1) and control (0), $Y_i$ is the outcome, and $\pi(\cdot)$ is the exposure model. The covariates included in $X_i$ can be different for the two models. 

An estimator is called "double robust" if it achieves consistent estimation of the ATE (or whatever estimand the researcher is interested in) as long as *at least one* of these two models is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  




## Assumptions

Most double robust methods require almost all of the standard assumptions necessary formost methods that depend on selection on observables. Although some double robust methods relax one or two of these, the six standard assumptions are:  

1. Consistency
2. Positivity/overlap
3. One version of treatment
4. No interference
5. IID observations
6. Conditional ignorability: $\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid X_i$

Special attention should be paid to Assumption 6: double robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the double robust methods covered in this tutorial make no functional form assumptions. These methods are designed to incorporate flexible machine learning algorithms to estimate both the outcome and exposure models, with regularization (often through cross-fitting) to avoid overfitting. 

# Overview of Techniques  
Each of the methods reviewed in this paper can be thought of as a collection of estimation techniques. Each involves a model for the outcome and another for the treatment exposure, but the ways these relate and are combined varies from method to method. Choice of estimation technique for these two models is left to the discretion of the user; often ensemble learning is recommended, but in practice simpler methods can also work well.  

## Augmented Inverse Probability Weighting (AIPW)

The oldest of these modern methods, AIPW arose in the context of missing data imputation [@robins_1994_estimation]. @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference. Introductions to AIPW exist in the contexts of political science [@glynn_2010_introduction] and econometrics [@funk_2011_doubly]. The `AIPW` R package provides a simple implementation of the method.   

AIPW combines estimates from a model for the treatment exposure, $\pi(X)$, and a model for the outcome, $\mu(X)$. The name comes from the close similarity to inverse probability weights (IPW), but whereas IPW only weights for probability of treatment, AIPW "augments" these weights with an estimate of the response surface.    

Formally, the model can be written as the difference between an estimated outcome for treated units and the estimated outcome for untreated units (see the demonstration below):

$$\begin{aligned}
\widehat{ATE}_{AIPW} = &\frac{1}{n} \sum_{i=1}^n \left( \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)} + \hat \mu_1(\mathbf X_i) \right) 
- \frac{1}{n} \sum_{i=1}^n \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} + \hat \mu_0(\mathbf X_i) \right)
\end{aligned}$$

In practice, AIPW weights may be very small or very large, a problem that inverse probability weights also suffer from. This can make AIPW prone to high variance. To remedy this, the predicted probabilities of treatment are often truncated, setting extremely small or large weights to some less extreme value [as in the `AIPW` R package, @zhong_2021_aipw].  

Below is R code to implement AIPW with truncation of extreme weights. As with all of the double robust methods reviewed here, we begin with predicted values (such as from a machine learning algorithm) for the outcome for treated units `mu1_pred` and untreated units `mu0_pred` as well as predicted values for treatment assignment probability `pi_pred`. We also have `d`, the vector of actual treatment assignments, and `y`, the observed outcome values.

```{r aipw-demo, eval = F, echo = T}
require(tidyverse)

aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # Truncate extreme values of the weights
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  # Calculate the predicted outcome value for treated units
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  # Calculate the predicted outcome value for untreated units
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  # Calculate the ATE
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}
```

@glynn_2010_introduction provide an alternate but equivalent formula, where the basic inverse probability weight (IPW) estimator (which incorporates only the exposure model $\hat \pi$) is corrected using a weighted average of two outcome regression estimates: 
$$\begin{aligned}
\widehat{ATE}_{AIPW} = & \frac{1}{n} \sum_{i=1}^n \left\{ \left[ \frac{D_i Y_i}{\hat \pi(\mathbf X_i)} - \frac{(1-D_i) Y_i}{ 1- \hat \pi(\mathbf X_i)} \right] - \frac{D_i - \hat \pi (\mathbf X_i)}{\hat \pi (\mathbf X_i)(1 - \hat pi (\mathbf X_i))} [(1- \hat \pi (\mathbf X_i)) \hat \mu_1(\mathbf X_i) + \hat \pi (\mathbf X_i) \hat \mu_0(\mathbf X_i)] \right\}
\end{aligned}$$ 

## Targeted Maximum Likelihood Estimation (TMLE)

@vanderlaan_2006_targeted first proposed TMLE an extension and improvement of previous double robust methods, using a parametric framework and the efficient influence curve  [@hines_2022_demystifying] to obtain estimates and standard errors. Mark van der Laan has gone on to collaborate on both a gentle introduction [@gruber_2009_targeted], a textbook [@vanderlaan_2011_targeted], and an R package [@gruber_2012_tmle] for implementing the method. @schuler_2017_targeted and @luque-fernandez_2018_targeted provide introductions for epidemiologists.

TMLE begins by estimating the relevant part of the data-generating distribution $P(Y)$, i.e. the conditional density $Q = P(Y \mid X)$. It next estimates the exposure model. Although any estimation method can be used for these steps, the originators of the method suggest using a "super learner," i.e. ensemble learning with cross-validation [@laan_2007_super]. Next, the exposure model is used to calculate a "clever covariate," which is similar to an IPW. The coefficient for this clever covariate is estimated using maximum likelihood -- whence the "MLE" in "TMLE." Finally, the estimate of $Q$ is updated in a function involving the clever covariate. This process can be iterated, but usually one iteration is enough. The estimate of the distribution $Q$ can be used to calculate the estimand of interest.  

Formally, first generate estimates of $\mu_{d}(\mathbf X_i) = E(Y \mid D=d, \mathbf X_i)$ and $\pi(\mathbf X_i) = P(D=1 \mid \mathbf X_i)$. Then create the variable $H_{di}$ for targeting step:
$$H_{di} = \frac{I(D_i = 1)}{\hat \pi (\mathbf X_i)} - \frac{I(D_i=0)}{1 - \hat \pi (\mathbf X_i)}$$

Next, calculate the clever covariates for each individual in the data. These quantities are similar to inverse probability weights, with $H_{0i}$ for untreated and $H_{0i}$ for treated units:
$$\begin{aligned}
H_{0i}(D=0, \mathbf X_i) &= \frac{1-d_i}{1- \hat \pi (\mathbf X_i)}, &
H_{1i}(D=1, \mathbf X_i) = \frac{d_i}{\hat \pi (\mathbf X_i)}.
\end{aligned}$$

In the next step, we estimate fluctuation parameters $\epsilon = (\epsilon_0, \epsilon_1)$ through maximum likelihood of the following logistic regression with fixed intercept $\text{logit}(\mu_{di})$: 
<!-- $$E(Y=1 \mid D, \mathbf X) = \text{logit}(\mu_{di}) + \epsilon H_{di}$$ -->
$$\text{logit}[E(Y=1 \mid D, \mathbf X)] = \text{logit}(\hat \mu_{di}) + \epsilon_0 H_{0i} + \epsilon_1 H_{1i}$$

Here we are assuming that $Y$ is a dichotomous variable taking the values of 0 or 1; the method is extended to continuous outcomes simply by normalizing the value of $Y$ to fall between 0 and 1. Then generate updated ("targeted") estimates of potential outcomes:
$$\begin{aligned}
\hat \mu_0^*(\mathbf X_i) &= \text{expit}[\text{logit}(\hat \mu_0(\mathbf X_i)) + \hat \epsilon H_{0i}]\\
\hat \mu_1^*(\mathbf X_i) &= \text{expit}[\text{logit}(\hat \mu_1(\mathbf X_i)) + \hat \epsilon H_{1i}]
\end{aligned}$$

Finally, estimate the parameter of interest -- in this case, the ATE:
$$\widehat{ATE}_{TMLE} = \frac{1}{n} \sum_{i=1}^n [\hat \mu_1^*(\mathbf X_i) - \hat \mu_0^*(\mathbf X_i)]$$

Here is R code to implement TMLE, again with predicted outcome values `mu1_pred` and `mu0_pred` and predicted probability of treatment `pi_pred`. Since the outcome is bounded and continuous, it is transformed to fall between 0 and 1 via $\tilde Y_{i} = Y_i - \min(Y)/(\max(Y)-\min(Y)$.
```{r tmle-demo, eval = F, echo = T}
# Functions for normalization and de-normalization
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  # Normalize outcome variable
  mu1_pred <- normalize(mu1_pred, y)
  mu0_pred <- normalize(mu0_pred, y)
  y_tilde <- normalize(y, y))
  
  n <- length(y)

  # Calculate clever covariates
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  # Estimate fluctuation parameter through maximum likelihood estimation
  epsilon <- glm(y_tilde ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  # Targeted estimates of the potential outcomes
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H1))
  
  # Estimate ATE
  ATE <- mean(target_1 - target_0)
  return(denormalize(ATE, y))
}
```


## Double/Debiased Machine Learning (DML)

The most recently developed of the methods reviewed here, DML was proposed in an econometrics context [@chernozhukov_2018_double] and has since seen a flurry of development [@dukes_2022_doubly; @kennedy_2023_semiparametric; @semenova_2021_debiased; @chernozhukov_2022_long; @farbmacher_2022_causal; @jung_2021_estimating]. The R package `DoubleML` [@bach_2021_doubleml] provides straightforward implementation of the method.  

DML is motivated by the need to handle problems with high-dimensional nuisance parameters, i.e. a large number of measured confounders. Flexible machine learning is appropriate for this task, but such methods suffer from regularization bias. DML removes this bias in a two-step procedure. First, it solves the auxiliary problem of estimating the treatment exposure model $E(D|X) = \pi(X)$. It then uses this model to remove bias: Neyman orthogonalization allows the creation of an orthogonalized regressor, essentially partialing out the effect of covariates $X$ from treatment $D$. The debiased $D$ is then used to estimate the conditional mean of the outcome $E(Y \mid X) = \mu(X)$, which can be used to calculate the estimand of interest.  

More formally, suppose we want to estimate $\delta$ in the following framework:
$$y_i = \delta d_i + g_0(\mathbf x_i) + u_i,$$
$$d_i = m_0(\mathbf x_i) + v_i.$$

The idea is to estimate $g_0$ and $m_0$ separately, then use residual-on-residual regression to obtain an estimate of $\delta$, which we can designate $\hat \delta$. However this leaves a term in the asymptotic distribution of $\hat \delta$ that biases the estimate. To avoid this, DML uses sample splitting [@angrist_1995_splitsample].  

We randomly split the sample of $n$ observations into two sets, $I$ and $I^c$, each of size $n/2$. Using any prediction algorithm, we then estimate the response and treatment models using only set $I^c$:  

1) Estimate treatment model $\hat m_0$ in the equation $d_i = \hat m_0(\mathbf x_i) + \hat v_i, i \in I^c$.  
2) Estimate the outcome model $\hat g_0$ in the equation $y_i =  \hat g_0(\mathbf x_i) + \hat u_i, i \in I^c$.

Note that the estimation of the outcome does not consider treatment assignment $d_i$. This is a key difference between DML and AIPW or TMLE.  

Next, we use the estimated models to perform residual-on-residual regression *on the left out set* $I$ to obtain an estimate of $\delta$:
$$\hat \delta(I^c, I) = \left(\sum_{i \in I} \hat v_i d_i \right)^{-1} 
\sum_{i \in I} \hat v_i (y_i - \hat g_0 (\mathbf x_i)),$$

where $\hat v_i = d_i - \hat m_0(\mathbf x_i)$. Using half the sample results in efficiency loss. To rectify this, we repeat the above procedure, switching the split sets. We then have $\delta(I^c, I)$ and $\delta(I, I^c)$. The cross-fitting DML estimator is:
$$\widehat {ATE}_{DML} = \frac{\hat \delta (I^c, I) + \hat \delta (I, I^c)}{2}.$$
R code to implement DML is shown below. Since DML involves sample splitting, this code is a little different from the above examples. We start with observed outcome values `y`, treatment assignment `d`, and covariate matrix `x`. First, a pre-processing function `dml_pre()` randomly splits the sample, outputting $I$ and $I^c$ sets of each of these variables. The second step predicts outcome and treatment probability for each half of the sample, using models fit to the other half. In the code chunk here, generalized random forests from the `grf` package are used to predict these, but any prediction algorithm can be used. Finally, a post-prediction function `dml_post()` performs the residual-on-residual regression for each half of the sample and finds the average of the two estimates to produce an ATE estimate. 
```{r dml-demo, eval = F, echo = T}
# Pre-processing: sample splitting
dml_pre <- function(y, d, x, seed = 1758){
  set.seed(seed)
  
  n <- length(y)
  n_2 <- round(n/2)
  
  # Split the sample
  random_vec <- sample(1:n, n, replace = F)
  I <- random_vec[1:n_2]
  I_c <- random_vec[(n_2+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,],
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

# Predictor function: in this case, generalized random forests
grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  # Train on the I_c sample, predict on the I sample
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  # Train on the I sample, predict on the I_c sample
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

# Implement DML: takes outputs from pre_dml() and grf_dml()
dml_post <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  # Residual-on-residual regression for each sample separately
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  # Average estimates from each sample
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

dml_pre_out <- dml_pre(y = y, d = d, x = x)
grf_dml_out <- do.call(grf_dml, dml_pre_out)
dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
```


## A simple demonstration using AIPW

To demonstrate double robustness, this section presents one of the simpler double robust estimators, AIPW [@glynn_2010_introduction]. As shown above, we can write this estimator as follows:

$$\begin{aligned}
\widehat{ATE} = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) 
- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}$$

For each individual in the sample, this estimator calculates two quantities:

- The treated potential outcome

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

- The control potential outcome

$$\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i)$$

Let's focus on the treated model:

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

First, assume that the outcome model $\mu_1(X_i)$ is *correctly* specified and the exposure model $\pi(X_i)$ is *incorreclty* specified. Let's also assume (for now) that we're dealing with a treated unit, i.e. $D_i = 1$. Then

$$\hat \mu_1 (X_i) = Y_i$$

and hence

$$\hat Y_{1i} = \frac{D_i(0)}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

So the model relies *only* on the outcome model. The incorrectly specified exposure model completely disappears from the equation. If we're dealing with a control unit ($D_i=0$), we get the same result:

$$\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

Now, what if the *exposure* model $\pi(X_i)$ is correctly specified and the outcome model $\mu_1(X)$ is incorrect? First, we rewrite the estimator for the treated outcome:

$$\begin{aligned}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). &&(*)
\end{aligned}$$

Since the exposure model is correclty specified, we have $D_i = \hat \pi(X_i)$ on average, so

$$E[D_i - \hat \pi(X_i)] = 0.$$

This means that the second term in equation $(*)$ is 0, so

$$E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].$$

This shows that when the exposure model is correct, then the estimator depends *only* on the exposure model. We can make similar arguments for the control model $\hat Y_{0i}$.

This demonstration shows that this estimator achieves double robustness: The estimator is robust to misspecification of either the exposure or the outcome model (but not both).

# Methods  

These double robust methods have many similarities. How do the results they give compare? This section tests the performance of each in practice using two strategies. First, results are compared using simulated data from a causal inference competition (@dorie_2019_automated). The true treatment effect is known, so these simulations allow assessment of accuracy. Second, these methods are applied to data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW). The NSW randomly provided training to disadvantaged workers, allowing an experimental estimate of the effect of the intervention, and data assembled by @dehejia_1999_causal allows these experimental estimates to be compared to observational ones.  

The three double robust methods are compared to two sets of traditional methods used as benchmarks. First are one-model methods. The most classic method considered -- linear regression -- models only the response surface. Here it is estimated using ordinary least squares regression ("OLS"), entering each variable separately without any interactions or higher-order terms. Two other one-model methods model only the treatment assignment mechanism. Propensity score are estimated from logistic regression with each variable entered separately and without any higher order terms, then matched using the `MatchIt` package ("PSM"). Finally, stabilized inverse probability weights [IPW, @austin_2015_moving, p. 3663] are used for weighted OLS regression.  These IPWs are estimated using each of the three under-the-hood estimation methods described below.  

The second set of traditional methods are two-model methods, which estimate separate models for treated and untreated units. In theory these could solve the misspecification problem that double robust methods are meant to solve, but they could still suffer from regularization bias. G-computation [@robins_1986_new; @hernan_2020_causal p. 163; @snowden_2011_implementation] uses some estimation technique to predict outcomes under treatment and control for each unit in the dataset. The ATE estimate is the difference in the average prediction under treatment and the average prediction under control. The second two-model method is the Lin estimator [@lin_2013_agnostic]. This method aims to solve issues with the bias induced by OLS regression in a randomization framework by interacting the treatment indicator with mean-centered covariates.  

Many of these methods allow the user to choose the underlying estimation method, results compare three techniques. First is using a logistic regression for the exposure model and an OLS regression for the outcome model. Second is generalized random forests [GRF, @athey_2019_generalized] using the `grf` R package, with separate models for exposure and outcome. The final technique is the SuperLearner (as promoted by the makers of TMLE) using the `SuperLearner` package, again with separate models for exposure and outcome. GLM, glmnet (lasso), and XGBoost models are considered for the SuperLearner. These three estimation techniques are used for each of the three double robust methods, for IPW, and for G-computation.  

For the simulations, results compare only point estimates from these methods. For evaluation of the experimental LaLonde data, I use bootstrapping with 100 samples to obtain estimates and standard errors.


# Results


```{r dorie-sim, eval = F}
set.seed(185)

# sims %>%
#   lapply(function(x) append(x, list()))

sim_list <- list()
for(i in 1:20){ # up to 77
  #sim_list_nest <- list()
  for(j in 1:10){ # up to 100
    print(paste0(i,',',j))
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016,
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'main'))
  }
}

sim_linear <- list()
for(i in c(1, 3)){
  for(j in 1:100){ 
    print(paste0(i,',',j))
    sim_linear[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'linear'))
  }
}


saveRDS(append(sim_list, sim_linear), here('files', 'sims.RDS'))
```

```{r large, eval = F}
# dataset 7: polynomial     0.35    one-term exponential      0.75      high
set.seed(185)

sim_list <- list()

for(j in c(2, 5, 10, 20)){ # up to 100
  for(i in 1:20){ # up to 77
  #sim_list_nest <- list()

    print(paste0(i,',',j))
    
    dataset <- aciccomp2016::input_2016 %>% 
      slice(rep(1:n(), each = j))
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(dataset, 7, i, extraInfo = T) %>% 
      append(list(dataset = 7, set = 'large', size = j*4802))
  }
}

saveRDS(sim_list, here('files', 'sims_large.RDS'))

```

```{r sims-small, eval = F}
set.seed(185)

sim_list <- list()
for(j in c(150, 300, 600, 1200, 2400, 4802)){ 
  for(i in 1:20){ 
  #sim_list_nest <- list()

    print(paste0(i,',',j))
  
    sample_index <- sample(1:4802, j, replace = F)
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 7, i, extraInfo = T) %>% 
      append(list(dorie_dataset = 7, set = 'small', size = j))
    
    for(name in names(sim_list[[paste0(i,',',j)]])[1:7]){
      sim_list[[paste0(i,',',j)]][[name]] <- sim_list[[paste0(i,',',j)]][[name]][sample_index]
    }
    
    sim_list[[paste0(i,',',j)]]$x <- sim_list[[paste0(i,',',j)]]$x[sample_index,]
  }
}

saveRDS(sim_list, here('files', 'sims_small.RDS'))
```



```{r sim-test, eval = F}
data_test <- bind_rows(aciccomp2016::input_2016, aciccomp2016::input_2016)

acs <- read_dta('/Users/nathan/Data/ACS/acs_2008_2021.dta')

test <- aciccomp2016::dgp_2016(data_test, parameter_list, 1950, extraInfo = T)
set.seed(185)
test_acs <- aciccomp2016::dgp_2016(filter(acs, year == 2016) %>% drop_na() %>% sample_n(1e5), 
                                   parameter_list, 1950, extraInfo = T)
```


```{r lm-sim, eval = F}
lm_df <- lm_sim(sims) 

write_csv(lm_df, here('files', 'lm_df.csv'))


lm_df %>%
  group_by(set) %>%
  summarize(ate_est = mean(ate), 
            se = sd(ate))
```


```{r psm-sim, eval = F}
psm_df <- psm_sim(sims) 

write_csv(psm_df, here('files', 'psm_df.csv'))
```


```{r pred-calc, eval = F}

ols_logit_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  ols_logit_pred_list[[i]] <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(ols_logit_pred_list, here('files/homemade', 'ols_logit_pred.RDS'))

# tibble(ols_logit_pred_list[[i]]$y,
#              ols_logit_pred_list[[i]]$mu1_pred, 
#              ols_logit_pred_list[[i]]$mu0_pred,
#              ols_logit_pred_list[[i]]$d,
#              hist(ols_logit_pred_list[[12]]$pi_pred)
#              ) 

grf_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  grf_pred_list[[i]] <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(grf_pred_list, here('files/homemade', 'grf_pred.RDS'))

 
superlearner_pred_list <- list() 
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  superlearner_pred_list[[i]] <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(superlearner_pred_list, here('files/homemade', 'superlearner_pred.RDS'))


```


```{r ipw, eval = F}
ols_logit_pred_list2 <- readRDS(here('files/homemade', 'ols_logit_pred.RDS'))
grf_pred_list2 <- readRDS(here('files/homemade', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files/homemade', 'superlearner_pred.RDS'))


ipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    

    D <- sims[[i]]$z == 'trt'
    PrD <- mean(D)
    ps <- predictions[[j]][[i]]$pi_pred
    ipw <- D*PrD/ps + (1-D)*(1-PrD)/(1-ps)
    
    lm_out <- lm(sims[[i]]$y ~ D, weights=ipw) 
    ate_ipw <- coefficients(lm_out)[[2]]
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    ipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = ate_ipw, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(ipw_out_list), here('files', 'ipw.csv'))
```

```{r g-comp, eval = F}
ols_logit_pred_list2 <- readRDS(here('files/homemade', 'ols_logit_pred.RDS'))
grf_pred_list2 <- readRDS(here('files/homemade', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files/homemade', 'superlearner_pred.RDS'))


gcomp_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    
    gcomp_out <- with(predictions[[j]][[i]], mean(mu1_pred - mu0_pred))
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    gcomp_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = gcomp_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(gcomp_out_list), here('files', 'gcomp.csv'))
```

```{r lin, eval = F}

n <- length(sims)
lin_list <- list()
for(i in 1:n){
  start_time <- Sys.time()
  
  print(paste0(i, ' out of ', n))
  sim_dat <- data.frame(y = sims[[i]]$y, 
                        d = ifelse(sims[[i]]$z == 'trt', 1, 0), 
                        sims[[i]]$x)
  
  ate <- NA
  tryCatch({
    lm_out <- tidy(estimatr::lm_lin(y ~ d, 
                                    covariates = as.formula(paste0('~ ', paste(names(sims[[i]]$x), collapse = ' + '))), 
                                    data = sim_dat))
    ate <- lm_out[[2,2]]
    
    }, error=function(e){
      cat("ERROR :",conditionMessage(e), "\n")
      })
  
  lin_list[[i]] <- data.frame(dataset = i,
                             ate = ate,
                             #se = lm_out[[2,3]],
                             truth = mean(sims[[i]]$y.1) - mean(sims[[i]]$y.0),
                             dorie_dataset = sims[[i]]$dataset,
                             set = sims[[i]]$set,
                             size = sims[[i]]$size,
                             comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
}


write_csv(bind_rows(lin_list), here('files', 'lin.csv'))
```

```{r aipw, eval = F}



aipw_calc2 <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, c(.025)) ~ as.numeric(quantile(pi_pred, c(.025))),
  #   pi_pred > quantile(pi_pred, c(.975)) ~ as.numeric(quantile(pi_pred, c(.975))),
  #   T ~ pi_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)
  
  ipw <- (d*y)/pi_pred - (1-d)*y/(1-pi_pred)
  adjust <- (d-pi_pred)/(pi_pred*(1-pi_pred)) * (1-pi_pred)*mu1_pred + pi_pred*mu0_pred
  
  ate <- (1/n) * sum(ipw - adjust)
  
  return(ate)
}


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade', 'aipw.csv'))


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade', 'aipw_trunc.csv'))



```


```{r tmle, eval = F}

tmle_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')

for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    start_time <- Sys.time()
    tryCatch({
      tmle_out <- with(predictions[[j]][[i]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
    }, error=function(e){
      tmle_out <- NA
      })
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    tmle_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = denormalize(tmle_out, predictions[[j]][[i]]$y), 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      fail = is.na(tmle_out),
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(tmle_out_list), here('files/homemade', 'tmle.csv'))


# logit_logit_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   logit_logit_pred_out <- logit_logit_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(logit_logit_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y)) 
#   truth <- mean(dat$y.1) - mean(dat$y.0)
# 
#   logit_logit_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(logit_logit_pred_list), label = 'ols, logit, tmle')
# 
# 
# #### 
# 
# grf_pred_list <- list()
# 
# for(i in 1:10){ #length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   grf_pred_out <- grf_pred(
#     y = dat$y, # with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   yt <- function(x, y){(x - min(y)) / (max(y) - min(y))}
#   
#   ate <- with(grf_pred_out, 
#               tmle_calc(mu1_pred = yt(mu1_pred, dat$y), 
#                         mu0_pred = yt(mu0_pred, dat$y),
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = yt(y, dat$y)))
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#   
#   grf_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(grf_pred_list), label = 'grf, tmle')


# superlearner_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   superlearner_pred_out <- superlearner_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(superlearner_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y))
#   truth = mean(dat$y.1) - mean(dat$y.0)
#   
#   superlearner_pred_list[[i]] <- data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(superlearner_pred_list), label = 'superlearner, tmle')
```




```{r dml, eval = F}

## DML OLS logit

ols_logit_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)

  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
   
  ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(ols_logit_dml_list), 
          here('files/homemade', 'dml_ols_logit.csv'))



# perform(bind_rows(ols_logit_dml_list), label = 'grf, dml')
 

## GRF
grf_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
   
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      grf_dml_out <- do.call(grf_dml, dml_pre_out)

      dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  grf_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(grf_dml_list), 
          here('files/homemade', 'dml_grf.csv'))

# perform(bind_rows(grf_dml_list), label = 'grf, dml')

## Superlearner
superlearner_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      superlearner_dml_out <- do.call(superlearner_dml, dml_pre_out)
      dml_post_out <- do.call(dml_post, append(dml_pre_out, superlearner_dml_out ))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  superlearner_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(superlearner_dml_list), 
          here('files/homemade', 'dml_superlearner.csv'))



#perform(bind_rows(superlearner_dml_list), label = 'superlearner, dml')

```

## Simluations with Dorie et al. (2019) Data

In 2016, the Atlantic Causal Inference Conference hosted a competition for causal inference methods that adjust on observables. @dorie_2019_automated published the results of this competition, along with the data used in the competition. Below, I test double robust methods on the 20 data sets used for the "do-it-yourself" part of the competition. The data represent a hypothetical twins study investigating the impact of birth weight on IQ. The data have 4802 observations and 52 covariates. The authors of the study specify a different data generating process for the potential outcomes in each data set. In all cases, ignorability holds, but the authors vary the following:  

- degree of nonlinearity
- percentage of treated
- overlap for the treatment group
- alignment (correspondence between the assignment mechanism and the response surface)
- treatment effect heterogeneity
- overall magnitude of the treatment effect  

### Main results

The 20 data sets used here cover a range of these attributes; see the supplemental material from @dorie_2019_automated for details. I use 10 simulations of each data set, resulting in 200 data sets. I then calculate bias, percent bias (the estimator's bias as a percentage of its standard error), root mean squared error (rmse), and median absolute error (mae). I also present the number of datasets for which the method fails and the average computation time for each data set, in seconds.  

Results for the full range of simulations are shown in Table \@ref(tab:dorie-results) and Figure \@ref(fig:dorie-results-fig). Using AIPW as written above results in some wildly biased estimates for the ols_logit estimator, due to dividing by some very small propensity scores. Hence I present estimates from a trimmed AIPW estimator, where predicted exposure model values are set to 0.01 if they are less than 0.01 and to 0.99 if they are greater than 0.99.  

The lowest bias is achieved by AIPW with GRF, followed by propensity score matching and then OLS. The lowest root mean squared error is obtained by AIPW with GRF, followed by TMLE with GRF, and next by OLS regression. Results are similar if we consider percent bias or mean absolute error. DML does not perform particularly well and also has the highest computation times due to sample splitting.  




```{r dorie-results}

lm_df <- read_csv(here('files', 'lm_df.csv')) %>%
  mutate(method = 'ols')

psm_df <- read_csv(here('files', 'psm_df.csv')) %>%
  mutate(method = 'psm')

lin_df <- read_csv(here('files', 'lin.csv')) %>%
  mutate(method = 'lin')

ipw_df <- read_csv(here('files', 'ipw.csv')) %>%
  mutate(estimator = method,
         method = 'ipw')

gcomp_df <- read_csv(here('files', 'gcomp.csv')) %>%
  mutate(estimator = method,
         method = 'g-comp')

aipw_df <- read_csv(here('files/homemade', 'aipw.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

aipw_trunc_df <- read_csv(here('files/homemade', 'aipw_trunc.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

tmle_df <- read_csv(here('files/homemade', 'tmle.csv')) %>%
  # filter(fail == F) %>%
  select(-fail) %>%
  mutate(estimator = method,
         method = 'tmle')

dml_df <- bind_rows(
  mutate(read_csv(here('files/homemade', 'dml_ols_logit.csv')), 
         estimator = 'ols_logit',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_grf.csv')), 
         estimator = 'grf',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_superlearner.csv')), 
         estimator = 'superlearner',
         method = 'dml',
         dataset = row_number())
) %>%
  select(names(tmle_df))

sim_results <- bind_rows(lm_df, psm_df, lin_df, ipw_df, gcomp_df, aipw_trunc_df, tmle_df, dml_df) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'ols', 'logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'lin', 'ipw', 'g-comp', 'aipw', 'tmle', 'dml'))) %>%
  arrange(method, estimator) %>%
  #mutate(estimator = ifelse(is.na(estimator), 'NA', estimator)) %>%
  mutate(bias = ate - truth,
         method_estimator = ifelse(is.na(estimator), as.character(method), paste0(method, ', ', estimator))) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) 
```


```{r dorie-results-fig, fig.height = 6, fig.cap = 'Results of Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each.'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

sim_results %>% 
  filter(set == 'main') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'All datasets') +
  theme(#axis.text.x=element_text(angle=-15), 
        legend.position = 'none') +
  scale_x_discrete(labels=function(x){sub(", ", "\n", x)})
```

### Linear DGPs

Traditional methods may perform better when the data generating processes are linear. To test this, I use 100 simulations of each of the two datasets from @dorie_2019_automated with linear data generating processes (numbers 1 and 3) to show how these methods perform when the underlying models are linear. 

If we consider only the linear datasets (Table \@ref(tab:dorie-results-linear) and Figure \@ref(fig:dorie-results-linear-fig)), OLS obtains the lowest bias, followed by AIPW with GRF and then propensity score matching. The lowest root mean squared error is achieved by OLS, then AIPW with GRF, and then TMLE with GRF. Percent bias and mean absolute error show similar results.  

Overall, traditional methods perform surprisingly well in comparison with the double robust methods. Even in the full range of datasets -- which include highly nonlinear exposure and outcome data-generating processes -- OLS and propensity score matching obtain some of the smallest bias. Generalized random forests appear to be the best estimators for the double robust methods, though a SuperLearner than incorporates GRF has the potential to outperform GRF alone, at the expense of computation time. Notably, the method with the longest computation time -- DML with a SuperLearner -- takes over 2,000 times as along as OLS.  



```{r dorie-results-linear-fig, fig.height = 6, fig.cap = 'Results of Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear").'}
sim_results %>% 
  filter(set == 'linear') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'Linear DGPs') +
  theme(#axis.text.x=element_text(angle=-15), 
        legend.position = 'none') +
  scale_x_discrete(labels=function(x){sub(", ", "\n", x)})
```

### Do results vary by DGP?
```{r vary}
sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number()))

dgp_names <- c('Treatment assignment', 'Probability of treatment',
               'Overlap of treatment', 'Response surface',
               'Alignment (same terms for treatment and response)',
               'Treatment effect heterogeneity')

names(dgp_names) <- names(aciccomp2016::parameters_2016)

for(name in colnames(aciccomp2016::parameters_2016[1])){
  plot <- sim_results_dgp %>% 
    mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
    mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
    #filter(method %in% c('ols', 'psm', 'aipw')) %>%
    ggplot(aes(x = method_estimator, y = bias)) +
    geom_jitter(alpha = .2, width = .3, aes(color = method)) +
    geom_boxplot(alpha = .5, outlier.alpha = 0) +
    #facet_wrap(~method, scales = 'free_x') +
    labs(x = '', title = dgp_names[name]) +
    theme(#axis.text.x=element_text(angle=-15), 
          legend.position = 'none') +
    scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
    facet_wrap(vars(!!sym(name)), ncol = 1, scales = 'free') 
  
  print(plot)
}
```

### Do results vary by sample size?

The double robust methods reviewed here have been shown to have lower bias asymptotically. Is the issue with previous results simply that the sample size of the simulation data ($n$ = 4,802) is too small for double robust methods to outperform traditional methods? To test this, this section uses simulated datasets of varying sizes, from 150 to 96,040 (20 times the original sample size). These datasets are also derived from the @dorie_2019_automated `aciccomp2016` package, using parameter set 7, a fairly nonlinear DGP with high heterogeneity. For sample sizes less than 4,802, the sample is randomly drawn from a randomly generated 4,802-unit sample. For sample sizes greater than 4,802, the design matrix (but non the outcome variable) is duplicated, then fed into the `dgp_2016` function. This preserves covariate distributions but retains stochasticity in the outcome.  

Results are presented in Table \@ref(tab:dorie-results-size) and Figure \@ref(fig:dorie-size-fig). 



```{r dorie-size-fig, fig.height = 6, fig.cap = 'RMSE of Monte Carlo simulations using dataset 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse)) +
#   geom_point() +
#   facet_wrap(~method_estimator, ncol = 1, scales = 'free')


# # text labels
# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method, estimator, method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(estimator = if_else(is.na(estimator), 'NA', estimator)) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse, color = estimator, label= method)) +
#   geom_text() +
#   scale_y_continuous(trans='log10')


sim_results %>%
  filter(set %in% c('small', 'large')) %>%
  group_by(method, estimator, method_estimator, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = sum(comp_time)/n(),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  group_by(method_estimator) %>%
  mutate(min_size = min(size*bias/bias, na.rm = T)) %>%
  ungroup() %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
         label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
         label_max = if_else(size == max(as.numeric(size)), 
                             as.character(method_estimator), NA_character_),
         size = factor(size, levels = sort(unique({.$size})))) %>%
  ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
  geom_line() +
  ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, segment.color = 'transparent') +
  ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, segment.color = 'transparent') +
  scale_y_continuous(trans='log10') +
  theme(legend.position = 'none')
```

## Comparisons using LaLonde NSW Data


As another test, I use data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW), as provided by @dehejia_1999_causal. Between March 1975 and July 1977, the NSW randomly provided training to disadvantaged workers. LaLonde used earnings in 1978 as the outcome of interest; comparing earnings in this year for treated and untreated workers allows an experimental estimate of the effect of the intervention. Restricting the sample to men, this study had `r nrow(filter(lalonde_exp, treat == T))` treated and `r nrow(filter(lalonde_exp, treat == F))` control participants. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, and not having a high school degree. Following @dehejia_1999_causal, I add a variable indicating whether each respondent's earnings in 1975 was $0 -- i.e., they were unemployed.     

LaLonde compared these experimental estimates to control samples drawn from the Panel Study of Income Dynamics (PSID) and Westat's Matched Current Population Survey-Social Security Administration File (CPS). The PSID-1 sample (*n* = `r nrow(lalonde_psid1_controls)`) contains all male household heads under 55 who did not classify themselves as retired in 1975, and the PSID-3 sample (*n* = `r nrow(lalonde_psid3_controls)`) further restricts this to men who were not working in spring of 1976 or 1975. The CPS-1 sample (*n* = `r nrow(lalonde_cps1_controls)`) includes all CPS males under 55, and CPS-3 (*n* = `r nrow(lalonde_cps3_controls)`) restricts this two those who were not working in March 1976 whose earnings in 1975 was below the poverty level. Restricting these observational samples gets closer to the group eligible for the NSW.  

Following @dehejia_1999_causal, I present results for the original samples analyzed by @lalonde_1986_evaluating, but I also include results using a subsample of the experimental group that has 1974 earnings data available (`r nrow(filter(lalonde_exp_74, treat == T))` treated and `r nrow(filter(lalonde_exp_74, treat == F))` control participants) and include this additional covariate, along with an indicator variable for no earnings in 1974.  

I again compare the three double robust methods to a linear model fitted by OLS ("ols") and propensity score matching using logistic regression ("psm"). Results are presented in Table \@ref(tab:lalonde), Figure \@ref(fig:lalonde-combined), and Figure \@ref(fig:lalonde-combined-1974).  

The "experimental" column provides a baseline for the comparison, suggesting that the program resulted in an earnings gain of about \$700 to \$800. If selection on observables holds, then we should be able to recover these estimates from the non-experimental control groups. Most of the methods do not perform very well; the only ones to estimate a positive treatment effects are OLS for the PSID-3 sample and TMLE for the CPS samples, and these are still smaller than the experimental baselines.  

Including 1974 earnings data results in much better estimates with the observational control groups. For PSID-1, TMLE provides estimates closest to the experimental ones, while DML performs much worse than even OLS. For PSID-3, OLS and TMLE both perform well. For CPS-1 and CPS-3, TMLE performs the best.  

These results highlight the importance of selection on observables holding. Without including 1974 earnings as a covariate, it appears that selection on observables does not hold, as most methods provide highly inaccurate estimates with the wrong sign. Once 1974 earnings are included, all of the methods provide estimates much closer to the experimental values. PSM, AIPW, and TMLE all do fairly well, while OLS and DML are more unstable across samples.



```{r lalonde-calc-homemade, eval = F}

lalonde_func <- function(dataset, sample, include_74 = F){
  set.seed(872)
  
  
  if(include_74 == F){
    lalonde_variables <- names(lalonde_exp)[3:9]
    dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
    
  } else {
    lalonde_variables <- names(lalonde_exp_74)[3:10]
    dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
  }
  
  dat <- list(y = dataset$re78,
                z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                x = dataset[,lalonde_variables])
  
  
  out_list <- list()
  
  out_list[['ols']] <- data.frame(
      method = 'ols',
      estimator = NA,
      ate = lm_sim(list(dat))$est_df$ate) 
  
  out_list[['psm']] <- data.frame(
      method = 'psm',
      estimator = NA,
      ate = lm_sim(list(dat))$est_df$ate)
  
  ols_logit_pred_out <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 

  
  grf_pred_out <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 

  superlearner_pred_out <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 
  
  predictions <- list(ols_logit_pred_out, grf_pred_out, superlearner_pred_out)
  pred_names <- c('ols_logit', 'grf', 'superlearner')
  for(j in 1:length(predictions)){
    aipw_out <- with(predictions[[j]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    
    out_list[[paste0('aipw', j)]] <- data.frame(
      method = 'aipw',
      estimator = pred_names[[j]],
      ate = aipw_out)
    
    tmle_out <- with(predictions[[j]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
      
      out_list[[paste0('tmle', j)]] <- data.frame(
        method = 'tmle',
        estimator = pred_names[[j]],
        ate =  denormalize(tmle_out, predictions[[j]]$y))
  }
  
  dml_pre_out <- dml_pre(dat$y,
                         as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                         x = dat$x)
  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))

  out_list[['dml_ols_logit']] <- data.frame(method = 'dml',
                                            estimator = 'ols_logit',
                                            ate = dml_post_out)
  
  dml_pre_out <- dml_pre(dat$y,
                     as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                     x = dat$x)
  grf_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))

  out_list[['dml_grf']] <- data.frame(method = 'dml',
                                            estimator = 'grf',
                                            ate = dml_post_out)
  
  dml_pre_out <- dml_pre(dat$y,
                     as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                     x = dat$x)
  superlearner_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, superlearner_dml_out))

  out_list[['dml_superlearner']] <- data.frame(method = 'dml',
                                            estimator = 'superlearner',
                                            ate = dml_post_out)
  
  bind_rows(out_list) %>%
    mutate(sample = sample) %>%
    return()
}
  


lalonde_func_out <- bind_rows(
  lalonde_func(filter(lalonde_exp, treat == 0), sample = 'experimental'),
  lalonde_func(lalonde_psid1_controls, 'PSID-1'),
  lalonde_func(lalonde_psid3_controls, 'PSID-3'),
  lalonde_func(lalonde_cps1_controls, 'CPS-1'),
  lalonde_func(lalonde_cps1_controls, 'CPS-3')) 
  
  lalonde_func_out %>%
    pivot_wider(names_from = 'sample', values_from = ate) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'aipw', 'aipw (trim)', 'tmle', 'dml'))) %>%
  arrange(method, estimator) %>%
  #write_csv(here('files/homemade', 'lalonde.csv'))


bind_rows(
  lalonde_func(filter(lalonde_exp_74, treat == 0), sample = 'experimental', T),
  lalonde_func(lalonde_psid1_controls, 'PSID-1', T),
  lalonde_func(lalonde_psid3_controls, 'PSID-3', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-1', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-3', T)) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files/homemade', 'lalonde_74.csv'))

```

```{r lalonde-calc, eval = F}
unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()


lalonde_func <- function(dataset, sample, include_74 = F){
  set.seed(872)
  
  if(include_74 == F){
    lalonde_variables <- names(lalonde_exp)[3:9]
    dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
    
  } else {
    lalonde_variables <- names(lalonde_exp_74)[3:10]
    dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
  }
  
  dataset <- list(list(y = dataset$re78,
                z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                x = dataset[,lalonde_variables]))
  
  lapply(list(
    lm_sim(dataset),
    psm_sim(dataset),
    aipw_sim(dataset),
    tmle_sim(dataset),
    dml_sim(dataset)),
  bind_cols) %>%
    bind_rows() %>%
    mutate(method = c('ols', 'psm', 'aipw', 'tmle', 'dml'),
           sample = sample,
           ate = round(d),
           se = round(se),
           estimate = paste0(round(d), ' (', round(se), ')')) %>%
    select(method, sample, estimate) 
}

bind_rows(
  lalonde_func(filter(lalonde_exp, treat == 0), sample = 'experimental'),
  lalonde_func(lalonde_psid1_controls, 'PSID-1'),
  lalonde_func(lalonde_psid3_controls, 'PSID-3'),
  lalonde_func(lalonde_cps1_controls, 'CPS-1'),
  lalonde_func(lalonde_cps1_controls, 'CPS-3')) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde.csv'))


bind_rows(
  lalonde_func(filter(lalonde_exp_74, treat == 0), sample = 'experimental', T),
  lalonde_func(lalonde_psid1_controls, 'PSID-1', T),
  lalonde_func(lalonde_psid3_controls, 'PSID-3', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-1', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-3', T)) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde_74.csv'))
```


```{r lalonde, eval = F}
bind_rows(
  read_csv(here('files', 'lalonde.csv')) %>%
    mutate(sample = 'Original LaLonde'),
  read_csv(here('files', 'lalonde_74.csv')) %>%
    mutate(sample = 'With 1974 earnings')) %>%
  select(sample, method, everything()) %>%
  kableExtra::kable(booktabs = T, 
                    # digits = 3,
                    # linesep = '',
                    caption = 'ATE estimates for Lalonde NSW data as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups. Standard errors shown in parentheses. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, not having a high school degree, and having no earnings in 1975. The "With 1974 earnings" estimates additionally include earnings in 1974 as a covariate, along with an indicator for having no earnigns in 1974.')

# bind_rows(
#   read_csv(here('files', 'lalonde.csv')) %>%
#     mutate(sample = 'LaLonde'),
#   read_csv(here('files', 'lalonde_74.csv')) %>%
#     mutate(sample = 'DW (with 1974 earnings)')) %>%
#   select(sample, everything()) %>%
#   kableExtra::kable(booktabs = T, 
#                     # digits = 3,
#                     linesep = '',
#                     caption = 'ATE estimates for Lalonde NSW data from Dehejia and Wahba (1999), with PSID ')


# lalonde_exp_df <- filter(lalonde, exper == 1)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_exp_df, ATT = TRUE)
# summary(fit)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_exp_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
# 
# 
# lalonde_psid_df <- filter(lalonde, (exper == 1 & treat == 1) | exper == 0)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_psid_df, ATT = TRUE)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_psid_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
```


```{r lalonde-combined, eval = F, fig.cap = 'ATE estimates and 95% confidence intervals for Lalonde NSW data (original sample) as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups.'}
lalonde_df <- bind_rows(
  read_csv(here('files', 'lalonde.csv')) %>%
    mutate(sample = 'Original LaLonde'),
  read_csv(here('files', 'lalonde_74.csv')) %>%
    mutate(sample = 'With 1974 earnings')) %>%
  select(sample, method, everything()) %>%
  pivot_longer(-c(sample, method)) %>%
  separate_wider_delim(value, delim = '(', names = c('ate', 'se')) %>%
  separate_wider_delim(se, delim = ')', names = c('se', 'blank')) %>%
  mutate(ate = as.numeric(ate), 
         se = as.numeric(se),
         method = factor(method, levels = unique({.$method})),
         name = factor(name, levels = unique({.$name})),
         experimental = name == 'experimental') %>%
  select(-blank) 

lalonde_df %>%
  filter(sample == 'Original LaLonde') %>%
  ggplot(aes(x = name, y = ate, color = experimental)) +
  geom_pointrange(aes(ymin = ate - 1.96*se, ymax = ate + 1.96*se)) +
  geom_hline(yintercept = 0) +
  facet_wrap(~method, scales = 'free_x') +
  theme(axis.text.x=element_text(angle=-15), 
        legend.position = 'none') +
  scale_color_manual(values = ucla_palette) +
  labs(title = 'Original LaLonde Sample', x = '')   
```

```{r lalonde-combined-1974, eval = F, fig.cap = 'ATE estimates and 95% confidence intervals for Lalonde NSW data (with 1974 earnings) as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups.'}
lalonde_df %>%
  filter(sample == 'With 1974 earnings') %>%
  ggplot(aes(x = name, y = ate, color = experimental)) +
  geom_pointrange(aes(ymin = ate - 1.96*se, ymax = ate + 1.96*se)) +
  geom_hline(yintercept = 0) +
  facet_wrap(~method, scales = 'free_x') +
  theme(axis.text.x=element_text(angle=-15), 
        legend.position = 'none') +
  scale_color_manual(values = ucla_palette) +
  labs(title = 'With 1974 earnings', x = '')
```


```{r lalonde-traditional}
lalonde_results <- sim_results %>%
  mutate(set = ifelse(set == 'lalondeCPS-3 74', 'lalonde CPS-3 74', set),
         set = ifelse(set == 'lalondeCPS-3 original', 'lalonde CPS-3 original', set)) %>%
  filter(str_detect(set, 'lalonde')) %>%
  separate_wider_delim(set, delim = ' ', names = c('set', 'sample', 'include_74')) %>%
  mutate(sample = factor(sample, levels = unique({.$sample})),
         estimator = factor(estimator, levels = c('ols_logit', 'ols', 'logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'lin', 'ipw', 'g-comp', 'aipw', 'tmle', 'dml')))
  

lalonde_results_df <- lalonde_results %>%
  group_by(set, sample, include_74, method_estimator) %>%
  summarize(estimate = mean(ate),
            se = sd(ate)) %>%
  ungroup() %>%
  mutate(value = paste0(round(estimate), ' (', round(se), ')')) %>%
  select(include_74, sample, method_estimator, value) %>%
  pivot_wider(values_from = value, names_from = method_estimator) %>%
  arrange(desc(include_74))

lalonde_results_df %>%
  select(include_74:`g-comp, superlearner`) %>%
  kableExtra::kable(booktabs = T, 
                    # digits = 3,
                    # linesep = '',
                    caption = 'Traditional methods: ATE estimates for Lalonde NSW data as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups. Standard errors shown in parentheses. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, not having a high school degree, and having no earnings in 1975. The "With 1974 earnings" estimates additionally include earnings in 1974 as a covariate, along with an indicator for having no earnigns in 1974.')
```

```{r lalonde-dr}
lalonde_results_df %>%
  select(include_74, sample, `g-comp, superlearner`:`dml, superlearner`) %>%
  kableExtra::kable(booktabs = T, 
                    # digits = 3,
                    # linesep = '',
                    caption = 'Double robust methods: ATE estimates for Lalonde NSW data as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups. Standard errors shown in parentheses. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, not having a high school degree, and having no earnings in 1975. The "With 1974 earnings" estimates additionally include earnings in 1974 as a covariate, along with an indicator for having no earnigns in 1974.')
```


# Conclusion

This paper aimed to provide an introduction to and evaluation of double robust methods for covariate adjustment in causal inference. By comparing AIPW, TMLE, and DML to the more traditional statistical methods of OLS and PSM, it allows evaluation of whether these methods are worth the effort and (computational) time for social scientists to adopt them.  

Results are mixed. On the one hand, in the full range of simulated data, AIPW with GRF estimators achieves the lowest bias and root mean squared error. This is the simplest (and most computationally efficient) of the double robust methods, although GRF is computationally expensive. TMLE with GRF also does fairly well. But OLS and PSM have strong showings, in the top three smallest bias or root mean squared error. In the datasets with linear data-generating processes, OLS achieves the lowest bias and root mean squared error.  

In the experimental LaLonde data, OLS does not perform as strongly. Although it is one of the few methods to provide experimental estimates with the correct sign in the original LaLonde set of covariates, its estimates are highly unstable across samples. When 1974 earnings are used as a covariate, PSM, AIPW, and TMLE all perform fairly well, though TMLE comes out on top.  

Overall, these double robust methods do not show a clear advantage over simpler, more computationally efficient methods such as OLS and PSM. While they are useful for social scientists to understand, in most applications, OLS or PSM provide similar results. However, in certain circumstances these double robust methods may be a better choice. Especially when paired with a highly flexible estimator like GRF, these methods can be useful when the number of covariates is high or even exceeds the number of observations. If the researcher believes the data-generating process is highly nonlinear, they may also be a good a choice, at least as a sensitivity check.   

This paper has a number of limitations. First, it does not consider situations where ignorability does not hold. @hunermund_2023_double evaluate a case of this, where endogenous variables are included in the covariate space, showing this can result in biased estimates from DML.



\newpage


# References

<div id="refs"></div>

\newpage

# (APPENDIX) Appendix {-}

# Appendix


```{r dorie-results-table}
sim_results %>%
  filter(set == 'main') %>%
  group_by(estimator, method) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            # var = round(var(ate, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = sum(comp_time)/n(),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  select(method, everything()) %>%
  arrange(method, estimator) %>%
  # mutate(bias = as.character(bias),
  #        rmse = as.character(rmse)) %>%
  # mutate(across(bias:comp_time, function(x){as.character(round(x, 3))})) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each. Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.') %>%
  kable_styling(latex_options = c("hold_position"))
```

\newpage

```{r dorie-results-linear}
sim_results %>%
  filter(set == 'linear') %>%
  group_by(estimator, method) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = sum(comp_time)/n(),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  select(method, everything()) %>%
  arrange(method, estimator) %>%
  # mutate(across(bias:comp_time, function(x){as.character(round(x, 3))})) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear"). Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.') %>%
  kable_styling(latex_options = c("hold_position"))
```

\newpage

```{r dorie-results-size}
sim_results %>%
  filter(set %in% c('small', 'large')) %>% 
  group_by(estimator, method, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = sum(comp_time)/n(),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  select(method, everything()) %>%
  arrange(method, estimator, size) %>%  
  # mutate(bias = as.character(bias),
  #        rmse = as.character(rmse)) %>%
  # mutate(across(bias:comp_time, function(x){as.character(round(x, 3))})) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    longtable = T,
                    caption = 'Results of Monte Carlo simulations using dataset 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each. Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.') %>%
  kable_styling(latex_options = c("hold_position")) #'repeat_header')) 
```