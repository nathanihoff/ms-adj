---
output:
  # bookdown::word_document2:
    # reference_docx: "word-template.docx"
  bookdown::pdf_document2:
    toc: no
    keep_tex: true
    number_sections: yes
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
    #latex_engine: xelatex
always_allow_html: true
header-includes:
  #- \usepackage{setspace}\doublespace
  # - \usepackage[nolists, fighead, tabhead]{endfloat}
  # - \usepackage{endnotes}
  # - \let\footnote=\endnote
  # - \setlength{\headheight}{14.5pt}
  # - \setlength{\headheight}{13.6pt}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \lhead{N.I. Hoffmann} 
  - \rhead{`r format(Sys.time(), '%B %e, %Y')`}
# - \newcommand{\beginsupplement}{\setcounter{table}{0}  
# \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} 
# \renewcommand{\thefigure}{A\arabic{figure}}}

editor_options: 
  chunk_output_type: console


citeproc: no
#fontfamily: mathpazo
# fontsize: 12pt
# geometry: margin=.6in
indent: yes
link-citations: yes
linkcolor: blue
lang: 'en-US'

bibliography: "/Users/nathan/Documents/My Library.bib" 
# bibliography: "My Library.bib"  
csl: apa.csl
# csl: american-sociological-association.csl

title: "Double Robust, Flexible Adjustment Methods for Causal Inference: An Overview and an Evaluation"

author:  Nathan I. Hoffmann, Departments of Sociology and Statistics, UCLA
date: "`r format(Sys.time(), '%B %e, %Y')`"

thanks: "I deeply thank  my committee -- Chad Hazlett, Onyebuchi A. Arah, Jennie E. Brand -- for their extensive suggestions and advice on previous drafts. I would also like to thank the members of UCLA's Practical Causal Inference Lab, UCLA's Social Inequality Data Science Lab, participants of the 2023 American Sociological Association's Annual Meeting, and participants of the 2024 All-UC Demography Conference for their feedback on previous versions of this paper."

abstract: "Double robust methods for flexible covariate adjustment in causal inference have proliferated in recent years. Despite their apparent advantages, these methods remain underutilized by social scientists. It is also unclear whether these methods actually outperform more traditional methods in finite samples. This paper has two aims: It is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, and it compares these methods to more traditional statistical methods and flexible \"single robust\" methods. It does this by using both simulated data where the treatment effect estimate is known, and then using comparisons of experimental and observational data from the National Supported Work Demonstration. Methods covered include Augmented Inverse Probability Weighting (AIPW), Targeted Maximum Likelihood Estimation (TMLE), and Double/Debiased Machine Learning (DML). Results suggest that some of these methods do outperform traditional methods in a wide range of simulations, but only slightly. In particular, the top performers are TMLE and AIPW in conjunction with flexible machine learning algorithms. But G-computation with the same flexible machine learning algorithms obtains almost identical results, and simple regression methods are nearly comparable in bias and are much more computationally efficient."

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T, dev = c('pdf', 'png'), fig.retina = 3, ft.latex.float = 'float')
options("yaml.eval.expr" = TRUE)

library(SuperLearner)
library(broom)
library(knitr)
library(kableExtra)
library(here)
library(patchwork)
library(haven)
library(tidyverse)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


options("yaml.eval.expr" = TRUE, scipen = 3, digits = 2)

uclablue = '#2774AE'
gray = '#808080'
black = '#000000'
ucla_palette = c(black, uclablue, gray)

# theme_set(theme_cowplot(font_family = 'Palatino') + 
theme_set(theme_classic(base_family = 'Palatino') + 
      theme(legend.title=element_blank(), 
         panel.grid.major.y = element_line('grey80'),
         legend.background = element_rect(fill = alpha("white", 0.5))
         ))
ggplot <- function(...) ggplot2::ggplot(...) + 
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")

kable <- function(...) knitr::kable(..., format.args = list(big.mark = ","))
```




```{r load}


unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()
```

```{r functions}
## Pred functions ####
ols_logit_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  mu_mod <- lm(y ~  d + ., data.frame(y, d, x))
  mu1_pred <- predict(mu_mod, newdata = data.frame(y, d = 1, x))
  mu0_pred <- predict(mu_mod, newdata = data.frame(y, d = 0, x))
  
  pi_mod <- glm(d ~ ., data.frame(y, x), family = binomial(link = 'logit'))
  pi_pred <- predict(pi_mod, type = 'response')
  
  # pi_pred <-case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)

  
  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}


grf_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  forest_mu <- grf::regression_forest(X = data.frame(d, x), Y = y, 
                                 tune.parameters = "all")
  mu0_pred <- predict(forest_mu, newdata = data.frame(d = 0, x))$predictions
  mu1_pred <- predict(forest_mu, newdata = data.frame(d = 1, x))$predictions
  
  forest_pi <- grf::regression_forest(X = x, Y = d, tune.parameters = "all")
  pi_pred <- predict(forest_pi, newdata = x)$predictions

  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}

superlearner_pred <- function(y, d, x, folds = 5, seed = 158){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  set.seed(seed)
  mu_fit <- SuperLearner(
    Y = y,
    X = data.frame(d, x),
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= gaussian()
  )
  
  pi_fit <- SuperLearner(
    Y = d,
    X = x,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family= binomial()
  )
  

  return(list(
    mu0_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 0, x), type = 'response')$library.predict %*% mu_fit$coef),
    mu1_pred = as.numeric(predict(mu_fit, newdata = data.frame(d = 1, x), type = 'response')$library.predict %*% mu_fit$coef),
    pi_pred = as.numeric(predict(pi_fit, type = 'response')$pred),
    d = d,
    y = y))
}

## Methods ####

lm_sim <- function(dat){
  
  n <- length(dat)
  lm_list <- list()
  for(i in 1:n){
    start_time <- Sys.time()
    
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    tryCatch({
      lm_out <- tidy(lm(y ~ d + ., sim_dat))
      ate <- lm_out[[2,2]]
      
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
    
    lm_list[[i]] <- data.frame(dataset = i,
                               ate = ate,
                               #se = lm_out[[2,3]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                               dorie_dataset = dat[[i]]$dataset,
                               set = dat[[i]]$set,
                               size = dat[[i]]$size,
                               comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
    
  }
  
  # fail_count <- sum(sapply(lm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(lm_list) == n, fail_count, fail_count + (n - length(lm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(lm_list))
  
  # return(list(
  #   est_df = bind_rows(lm_list)
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  #   ))
}


psm_sim <- function(dat){
  
  n <- length(dat)
  psm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    
    start_time <- Sys.time()
    
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    ate <- NA
    
    tryCatch({
      form <- as.formula(paste0('d ~ ', paste(names(dat[[i]]$x), collapse = '+')))
      match_out <- MatchIt::matchit(form,
                             data = sim_dat,
                             method = 'nearest',
                             distance = 'glm') 
      
      # match_data <- MatchIt::match.data(match_out) 
      # apply(match_data, 1, unique)
      form2 <- as.formula(paste0('y ~ d + ', paste(names(dat[[i]]$x), collapse = '+')))
      
    
      psm_out <- lm(form2, 
                    MatchIt::match.data(match_out), 
                    weights = weights) %>%
        tidy()
      
      ate <- psm_out[[2,2]]
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
    
    psm_list[[i]] <- data.frame(dataset = i,
                                ate = ate,
                                # se = psm_out[[2,3]],
                                truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0),
                                dorie_dataset = dat[[i]]$dataset,
                                set = dat[[i]]$set,
                                size = dat[[i]]$size,
                                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
  }
  
  # fail_count <- sum(sapply(psm_list, function(x) is.null(x)))
  # # in case last few are errors
  # fail_count <- ifelse(length(psm_list) == n, fail_count, fail_count + (n - length(psm_list)))
  
  # end_time <- Sys.time()
  
  return(bind_rows(psm_list))
  
  # return(list(
  #   est_df = bind_rows(psm_list),
  #   fail_count = fail_count,
  #   comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  # )
}


aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}

aipw_calc_trunc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, .025) ~ quantile(pi_pred, .025),
  #   pi_pred > quantile(pi_pred, .975) ~ quantile(pi_pred, .975),
  #   T ~ pi_pred)
  
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}


tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(y)
  # H <- (d == 1)/pi_pred - (d==0)/(1-pi_pred)
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  epsilon <- glm(y ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred 
                    + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  H_0 = (1-d)/(1-pi_pred)
  H_1 = d/pi_pred
  
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H_0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H_1))
  
  ATE <- mean((target_1 - target_0), na.rm = T)
  return(ATE)
}

dml_pre <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  n <- length(y)
  n_2 <- n/2
  n_2_1 = ifelse(round(n_2) == n_2, n_2, round(n_2))
  n_2_2 = ifelse(round(n_2) == n_2, n_2, round(n_2)+1)
  
  # split the sample
  random_vec <- sample(1:n, n)
  I <- random_vec[1:n_2_1]
  I_c <- random_vec[(n_2_1+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,], 
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

dml_post <- function(y_I, d_I, x_I = NULL, y_I_c, d_I_c, x_I_c = NULL,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

## Predictor functions
ols_logit_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- lm(y ~ ., data.frame(y = y_I_c, x_I_c))
  mu_pred1 <- predict(mu_mod1, newdata = data.frame(y = y_I, x_I))

  pi_mod1 <- glm(d ~ ., data.frame(d = d_I_c, x_I_c), 
                family = binomial(link = 'logit'))
  pi_pred1 <- predict(pi_mod1, 
                     newdata = data.frame(d = d_I, x_I), 
                     type = 'response')
  
  mu_mod2 <- lm(y ~ ., data.frame(y = y_I, x_I))
  mu_pred2 <- predict(mu_mod2, newdata = data.frame(y = y_I_c, x_I_c))

  pi_mod2 <- glm(d ~ ., data.frame(d = d_I, x_I), 
                family = binomial(link = 'logit'))
  pi_pred2 <- predict(pi_mod2, 
                     newdata = data.frame(d = d_I_c, x_I_c), 
                     type = 'response')
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, 
                                       tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, 
                                       tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

superlearner_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                             folds = 5){
  
  
  mu_mod1 <- SuperLearner(
    Y = y_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod1 <- SuperLearner(
    Y = d_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  
  mu_pred1 <- predict(mu_mod1, newdata = x_I, type = 'response')$library.predict %*% mu_mod1$coef
  pi_pred1 <- predict(pi_mod1, newdata = x_I, type = 'response')$pred
  
  mu_mod2 <- SuperLearner(
    Y = y_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=gaussian()
  )
  
  pi_mod2 <- SuperLearner(
    Y = d_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family=binomial()
  )
  
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c, type = 'response')$library.predict %*% mu_mod2$coef
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c, type = 'response')$pred
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

## Functions using double robust packages ####
aipw_sim <- function(dat, seed = 185){

  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  aipw_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = as.numeric(ifelse(dat[[i]]$z == 'trt', 1, 0)), 
                          dat[[i]]$x)
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, 
                                   W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #                              Y = sim_dat$y, W = sim_dat$d)
      
      aipw_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'AIPW')
      
      aipw_list[[i]] <- data.frame(d = aipw_out[[1]],
                                   se = aipw_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(aipw_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(aipw_list) == n, fail_count, fail_count + (n - length(aipw_list)))
  
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(aipw_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

tmle_sim <- function(dat, seed = 185){
  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  tmle_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #   Y = sim_dat$y, W = sim_dat$d)
      
      tmle_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'TMLE')
      
      tmle_list[[i]] <- data.frame(d = tmle_out[[1]],
                                   se = tmle_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(tmle_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(tmle_list) == n, fail_count, fail_count + (n - length(tmle_list)))
  
  end_time <- Sys.time()
  
 return(list(
      est_df = bind_rows(tmle_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

dml_sim <- function(dat, seed = 185){
  library(mlr3)
  library(mlr3learners)
  
  start_time <- Sys.time()
  
  set.seed(seed)
  
  n <- length(dat)
  
  dml_list <- list()
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    lgr::get_logger("mlr3")$set_threshold("warn")
    
    learner = lrn("regr.ranger", num.trees=500, 
                  max.depth=5, min.node.size=2)
    ml_l = learner$clone()
    ml_m = learner$clone()

    tryCatch({
      dml_out <- DoubleML::DoubleMLPLR$new(
        DoubleML::DoubleMLData$new(sim_dat,
                                 y_col = 'y',
                                 d_cols = 'd',
                                 x_cols = names(dat[[i]]$x)), 
        ml_l=ml_l, ml_m=ml_m)
      
      dml_out$fit()
      dml_list[[i]] <- data.frame(d = dml_out$all_coef[[1,1]],
                                  se = dml_out$all_se[[1,1]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(dml_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(dml_list) == n, fail_count, fail_count + (n - length(dml_list)))
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(dml_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
      )
}



## Other functions ####
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

perform <- function(est_df, label){
  est_df %>%
    # mutate(d = d/truth,
    #        truth = 1) %>%
    summarize(bias = mean(d - truth),
              percent_bias = bias/sd(d),
              rmse = sqrt(mean((ate - truth)^2)),
              mae = median(abs(d - truth))
              # fail_count = first(fail_count),
              # comp_time = first(comp_time)
              ) %>%
    mutate(label = label,
           n = nrow(est_df) + fail_count) %>%
    select(label, everything()) %>%
    return()
}

model_matrix <- function(...){
  options(na.action='na.pass')
  matrix_out <- model.matrix(...)
  options(na.action = 'na.omit')
  return(matrix_out)
}

p_val <- function(estimate, std.error){
  z <- estimate / std.error
  2*(1 - pnorm(abs(as.numeric(z))))
}

compare_tbl <- function(model_list, treatment, tidy = T){
  out_list <- list()
  k <- 1
  for(model_set in model_list){
    names(model_set$grf) <- c('estimate', 'std.error')
    
    if(tidy == T){
    original_model <- tidy(model_set$original) %>%
      filter(term == treatment)
    } else{
      original_model <- model_set$original
    }
    
    out_list[[k]] <- original_model %>%
      bind_rows(bind_rows(model_set$grf)) %>%
      bind_rows(model_set$dml) %>%
      bind_rows(model_set$dml_cre) %>%
      bind_rows(model_set$dml_wg_cre) %>%
      mutate(term = c('Original', 'AIPW (GRF)', 'DML (SuperLearner)', 
                      'DML (SuperLearner), CRE FE', 'DML (SuperLearner), hybrid FE'), 
             p.value = p_val(estimate, std.error)) %>%
      remove_rownames() %>%
      tibble()
    
    k <- k+1
  }
  return(out_list)
}

# compare_tbl_notidy <- function(model_list){
#   out_list <- list()
#   k <- 1
#   for(model_set in model_list){
#     names(model_set[[2]]) <- c('estimate', 'std.error')
#     
#     
#     out_list[[k]] <- model_set[[1]] %>%
#       bind_rows(bind_rows(model_set[[2]])) %>%
#       bind_rows(model_set[[3]]) %>%
#       mutate(term = c('Original', 'AIPW (GRF)', 'DML (SuperLearner)'), p.value = p_val(estimate, std.error)) %>%
#       tibble()
#     
#     k <- k+1
#   }
#   return(out_list)
# }

dml_grf <- function(outcome, treatment, covariates, clustervar = NULL, dataset, 
                    fe = T, target = 'treated', paper, model, drop_na_grf = F, seed = 123){
  set.seed(seed)
  
  dataset <- filter(dataset, !is.na(!!sym(outcome))) %>%
    select(all_of(c(treatment, outcome, covariates, clustervar)))
  
  if(drop_na_grf == T){
    dataset <- dataset %>%
      drop_na()
  }
  
  y <- as.numeric(dataset[[outcome]])
  X <- model_matrix(~., select(dataset, all_of(covariates)))[,]
  d <- as.numeric(dataset[[treatment]])
  
  grf_model <- grf::causal_forest(X = X, Y = y, W = d, seed = 123,
                        clusters = dataset[[clustervar]])
  
  grf_out <- grf::average_treatment_effect(grf_model, target.sample = target)
  
  
  dataset_drop_na <- as.data.frame(model_matrix(~., dataset)[,]) %>%
    # remove annoying characters
    rename_with(., ~ gsub("'", "", iconv(.x, from = "UTF-8", to='ASCII//TRANSLIT'))) %>%
    rename_with(., ~ gsub("\\[|\\]|\\/|\\*|\\)|\\(", "", .x)) %>%
    rename_with(., ~ gsub(" ", "", .x)) %>%
    # drop missing
    drop_na() %>%
    # remove uninformative columns
    select(where(~n_distinct(.) > 1)) 
  dataset_drop_na <- dataset_drop_na %>%
    left_join(dataset_drop_na %>%
                group_by(!!sym(clustervar)) %>%
                summarize(across(everything(), mean, .names = 'mean_{.col}')) %>%
                ungroup()) %>%
    mutate(intercept = 1)
  # covariates_design <- dataset_drop_na %>% 
  #   select(select(-c(treatment, outcome, clustervar))) %>%
  #   names() %>%
  #   append('intercept', after = 0)
  covariates_design <- names(dataset_drop_na)[!(names(dataset_drop_na) %in% c(treatment, outcome, clustervar)) &
                                                !str_detect(names(dataset_drop_na),  'mean_')]
  
  
  
  # dataset_drop_na$cov_mean <- dataset_drop_na %>%
  #   select(all_of(covariates_design)) %>%
  #   mutate_all(scale) %>%
  #   apply(1, mean)


  # dml_dataframe <- as.data.frame(cbind(y, d, 
  #                                      d_bar = dataset_drop_na[[paste0('mean_', treatment)]],
  #                                      as_tibble(X)[,-1], 
  #                                      as_tibble(mean_X)[,-1], 
  #                                      cluster = dataset_drop_na[[clustervar]]))
  
  
  graph_ensemble_regr = gunion(list(
      po("learner", lrn("regr.cv_glmnet", s = "lambda.min")),
      po("learner", lrn('regr.xgboost', max_depth = 4)),
      po("learner", lrn("regr.glm"))
    )) %>>%
      po("regravg", 3)
  
  ensemble_pipe_regr = as_learner(graph_ensemble_regr)
  
  # DML
  set.seed(seed)
  dml_data <- double_ml_data_from_data_frame(dataset_drop_na,
                                             x_cols = covariates_design,
                                             y_col = outcome,
                                             d_cols = treatment,
                                             cluster_cols = clustervar)
  
  # y <- as.numeric(dataset_drop_na[[outcome]])
  # # X <- select(dataset_drop_na, covariates_design)
  # X <- model_matrix(~., select(dataset_drop_na, all_of(covariates)))[,]
  # # mean_X <- model_matrix(~., select(dataset_drop_na, all_of(paste0('mean_', covariates))))[,]
  # d <- as.numeric(dataset_drop_na[[treatment]])
  # 
  # dml_data <- double_ml_data_from_matrix(X = X, y = y, d = d, 
  #                                        cluster_vars = dataset_drop_na[[clustervar]])
  
  # dml_data <- double_ml_data_from_matrix(X = select(dataset_drop_na, all_of(covariates_design)),
  #                                            y = dataset_drop_na[,outcome],
  #                                            d = dataset_drop_na[,treatment],
  #                                            cluster_vars = dataset_drop_na[,clustervar])
  obj_dml_plr_sim_pipe_ensemble = DoubleMLPLR$new(dml_data,
                                                  ml_l = ensemble_pipe_regr,
                                                  ml_m = ensemble_pipe_regr)
  obj_dml_plr_sim_pipe_ensemble$fit() 
  dml_out <- data.frame(estimate = obj_dml_plr_sim_pipe_ensemble$coef,
                         std.error = obj_dml_plr_sim_pipe_ensemble$se)
  
  if(fe == T){
    # DML CRE
    set.seed(seed)  
    dml_data_cre <- XTDML::dml_cre_data_from_data_frame(dataset_drop_na,
                                                 x_cols = covariates_design,
                                                 y_col = outcome,
                                                 d_cols = treatment,
                                                 xbar_cols = paste0('mean_', covariates_design[-length(covariates_design)]),
                                                 dbar_cols = paste0('mean_', treatment),
                                                 cluster_cols = clustervar)  
    obj_dml_cre = XTDML::dml_cre_plr$new(dml_data_cre,
                                         ml_l = ensemble_pipe_regr,
                                         ml_m = ensemble_pipe_regr)
    
    obj_dml_cre$fit()
    dml_cre_out <- data.frame(estimate = obj_dml_cre$coef_theta,
                           std.error = obj_dml_cre$se_theta)
  
  # DML WG-CRE
  require(parameters)
  set.seed(seed)  
  dml_data_wg_cre <- XTDML::dml_hybrid_data_from_data_frame(dataset_drop_na,
                                               x_cols = covariates_design,
                                               y_col = outcome,
                                               d_cols = treatment,
                                               xbar_cols = paste0('mean_', covariates_design[-length(covariates_design)]),
                                               dbar_cols = paste0('mean_', treatment),
                                               cluster_cols = clustervar)  
  obj_dml_wg_cre = XTDML::dml_hybrid_plr$new(dml_data_wg_cre,
                                       ml_l = ensemble_pipe_regr,
                                       ml_m = ensemble_pipe_regr)
  
  obj_dml_wg_cre$fit()
  dml_wg_cre_out <- data.frame(estimate = obj_dml_wg_cre$coef_theta,
                         std.error = obj_dml_wg_cre$se_theta)
  } else {
    dml_cre_out <- data.frame(estimate = NA,
                           std.error = NA)
    dml_wg_cre_out <- data.frame(estimate = NA,
                         std.error = NA)
  }
  
  return(list(grf = grf_out, 
              dml = dml_out,
              dml_cre = dml_cre_out,
              dml_wg_cre = dml_wg_cre_out))
}

```



```{r load-sims, eval = F}
lalonde_bootstrap <- function(dataset, sample, include_74 = F, iter = 100){
  
  lalonde_variables <- c('age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75', 're75_0')
  if(include_74 == F){
        dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
        
      } else {
        lalonde_variables <- c(lalonde_variables, 're74', 're74_0')
        dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
      }
  
  bootstrap_list <- list()
  
  for(i in 1:iter){
    dataset_bootstrap <- sample_n(dataset, nrow(dataset), replace = T)
    
    bootstrap_list[[i]] <- list(y = dataset_bootstrap$re78,
                  z = ifelse(dataset_bootstrap$treat == 1, 'trt', 'ctl'),
                  x = dataset_bootstrap[,lalonde_variables],
                  y.1 = NA, y.0 = NA,
                  dataset = i,
                  set = sample,
                  size = nrow(dataset_bootstrap))
  }
  
  return(bootstrap_list)
}

lalonde_setup <- function(dataset, sample, include_74 = F){
  
  lalonde_variables <- c('age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75', 're75_0')
  if(include_74 == F){
        dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
        
      } else {
        lalonde_variables <- c(lalonde_variables, 're74', 're74_0')
        dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
      }
  

    
    list(y = dataset$re78,
                  z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                  x = dataset[,lalonde_variables],
                  y.1 = NA, y.0 = NA,
                  dataset = i,
                  set = sample,
                  size = nrow(dataset)) %>%
      return()
}

set.seed(1859)
sims_lalonde <- c(
  lalonde_bootstrap(filter(lalonde_exp, treat == 0), sample = 'lalonde experimental original'),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 original'),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 original'),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 original'),
  lalonde_bootstrap(filter(lalonde_exp_74, treat == 0), sample = 'lalonde experimental 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid1_controls, 'lalonde PSID-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_psid3_controls, 'lalonde PSID-3 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-1 74', include_74 = T),
  lalonde_bootstrap(lalonde_cps1_controls, 'lalonde CPS-3 74', include_74 = T))


# sims <- readRDS(here('files', 'sims.RDS'))

sims_main <- readRDS(here('files_main', 'sims.RDS'))
sims_large <- readRDS(here('files_large', 'sims_large.RDS'))
sims_small <- readRDS(here('files_small', 'sims_small.RDS'))

sims <- sims_main %>% append(sims_small) %>% append(sims_large) %>% append(sims_lalonde)


## Full lalonde
# sims <- list(
#   lalonde_setup(filter(lalonde_exp, treat == 0), sample = 'lalonde experimental original'),
#   lalonde_setup(lalonde_psid1_controls, 'lalonde PSID-1 original'),
#   lalonde_setup(lalonde_psid3_controls, 'lalonde PSID-3 original'),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-1 original'),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-3 original'),
#   lalonde_setup(filter(lalonde_exp_74, treat == 0), sample = 'lalonde experimental 74', include_74 = T),
#   lalonde_setup(lalonde_psid1_controls, 'lalonde PSID-1 74', include_74 = T),
#   lalonde_setup(lalonde_psid3_controls, 'lalonde PSID-3 74', include_74 = T),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-1 74', include_74 = T),
#   lalonde_setup(lalonde_cps1_controls, 'lalonde CPS-3 74', include_74 = T))

# sims <- sims %>% append(sims_large)

# sims <- readRDS(here('files', 'sims_small.RDS'))
 
# for(i in 1:length(sims_small)){
#   # sims_large[[i]]$size <- nrow(sims_large[[i]]$x)
#   sims_small[[i]]$dataset <- 7
# }
# saveRDS(sims_large, here('files_large', 'sims_large.RDS'))
# 
# 
# sims_combined <- sims %>% append(sims_small) %>% append(sims_large)
# 
# index_df <- data.frame(dorie_dataset = sapply(sims_combined, function(x) x$dataset),
#                        set = sapply(sims_combined, function(x) x$set),
#                        size = sapply(sims_combined, function(x) x$size)) %>%
#   mutate(dataset = c(1:length(sims), 1:length(sims_small), 1:length(sims_large)))
# 
# lm_df <- read_csv(here('files_main', 'lm_df.csv')) %>% 
#   mutate(set = c(rep('main', 200), rep('linear', 200))) %>%
#   bind_rows(read_csv(here('files_small', 'lm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'lm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# 
# write_csv(lm_df, here('files', 'lm_df.csv'))
# 
# psm_df <- read_csv(here('files_main', 'psm_df.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small', 'psm_df.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large', 'psm_df.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(lm_df, here('files', 'psm_df.csv'))
# 
# 
# aipw_df <- read_csv(here('files_main/homemade', 'aipw.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_df, here('files/homemade', 'aipw.csv'))
# 
# 
# aipw_trunc_df <- read_csv(here('files_main/homemade', 'aipw_trunc.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'aipw_trunc.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(aipw_trunc_df, here('files/homemade', 'aipw_trunc.csv'))
# 
# tmle_df <- read_csv(here('files_main/homemade', 'tmle.csv')) %>% 
#   mutate(set = ifelse(dataset <= 200, 'main', 'linear')) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'tmle.csv')) %>% mutate(set = 'small')) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'tmle.csv')) %>% mutate(set = 'large')) %>%
#   left_join(index_df)
# 
# write_csv(tmle_df, here('files/homemade', 'tmle.csv'))
# 
# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files/homemade', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files/homemade', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files/homemade', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files/homemade', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files/homemade', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files/homemade', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))



## add lalonde ####
# read_csv(here('files_combined', 'lm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lm_df.csv'))) %>%
#   write_csv(here('files', 'lm_df.csv'))
# 
# 
# read_csv(here('files_combined', 'psm_df.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'psm_df.csv'))) %>%
#   write_csv(here('files', 'psm_df.csv'))
# 
# read_csv(here('files_combined', 'lin.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'lin.csv'))) %>%
#   write_csv(here('files', 'lin.csv'))
# 
# read_csv(here('files_combined', 'gcomp.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde', 'gcomp.csv'))) %>%
#   write_csv(here('files', 'gcomp.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw.csv'))) %>%
#   write_csv(here('files/homemade', 'aipw.csv'))
# 
# read_csv(here('files_combined/homemade', 'aipw_trunc.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'aipw_trunc.csv'))) %>%
#   write_csv(here('files/homemade', 'aipw_trunc.csv'))
# 
# read_csv(here('files_combined/homemade', 'tmle.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'tmle.csv'))) %>%
#   write_csv(here('files/homemade', 'tmle.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_ols_logit.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_ols_logit.csv'))) %>%
#   write_csv(here('files/homemade', 'dml_ols_logit.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_grf.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_grf.csv'))) %>%
#   write_csv(here('files/homemade', 'dml_grf.csv'))
# 
# read_csv(here('files_combined/homemade', 'dml_superlearner.csv')) %>%
#   bind_rows(read_csv(here('files_lalonde/homemade', 'dml_superlearner.csv'))) %>%
#   write_csv(here('files/homemade', 'dml_superlearner.csv'))
# 
# 
# readRDS(here('files_combined/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'ols_logit_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'ols_logit_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'grf_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'grf_pred.RDS'))
# 
# readRDS(here('files_combined/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_lalonde/homemade', 'superlearner_pred.RDS'))) %>%
#   saveRDS(here('files/homemade', 'superlearner_pred.RDS'))




# dml_ols_logit_df <- read_csv(here('files_main/homemade', 'dml_ols_logit.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   left_join(index_df) 
# 
# write_csv(dml_ols_logit_df, here('files/homemade', 'dml_ols_logit.csv'))
# 
# dml_grf_df <- read_csv(here('files_main/homemade', 'dml_grf.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_grf.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_grf_df, here('files/homemade', 'dml_grf.csv'))
# 
# 
# dml_superlearner_df <- read_csv(here('files_main/homemade', 'dml_superlearner.csv')) %>%
#   mutate(dataset = 1:nrow(.)) %>%
#   bind_rows(read_csv(here('files_small/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'small',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   bind_rows(read_csv(here('files_large/homemade', 'dml_superlearner.csv')) %>% 
#               mutate(set = 'large',
#                      dorie_dataset = 7,
#                      dataset = row_number())) %>%
#   select(-dorie_dataset) %>%
#   left_join(index_df)
# 
# write_csv(dml_superlearner_df, here('files/homemade', 'dml_superlearner.csv'))


# ols_logit_pred_combined <- readRDS(here('files_main/homemade', 'ols_logit_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'ols_logit_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'ols_logit_pred.RDS')))
# 
# grf_pred_combined <- readRDS(here('files_main/homemade', 'grf_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'grf_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'grf_pred.RDS')))
# 
# superlearner_pred_combined <- readRDS(here('files_main/homemade', 'superlearner_pred.RDS')) %>%
#   append(readRDS(here('files_small/homemade', 'superlearner_pred.RDS'))) %>%
#   append(readRDS(here('files_large/homemade', 'superlearner_pred.RDS')))
# 
# saveRDS(ols_logit_pred_combined, here('files/homemade', 'ols_logit_pred.RDS'))
# saveRDS(grf_pred_combined, here('files/homemade', 'grf_pred.RDS'))
# saveRDS(superlearner_pred_combined, here('files/homemade', 'superlearner_pred.RDS'))



# dml_ols_logit <- read_csv(here('files_large/homemade', 'dml_ols_logit.csv')) 
# 
# dml_ols_logit %>%
#   filter(set == 'main') %>%
#   write_csv(here('files_main/homemade', 'dml_ols_logit.csv'))
# 
# dml_ols_logit %>%
#   filter(set == 'large') %>%
#   write_csv(here('files_large/homemade', 'dml_ols_logit.csv'))

```




# Introduction

In causal inference, functional form misspecification of underlying models can bias estimates of treatment effects [@hernan_2020_causal; @morgan_2015_counterfactuals]. There have been two important developments that attempt to overcome this. First, methodologists have developed machine learning methods that allow greater flexibility in estimation, adjusting for covariates in data-driven, complex ways [@balzer_2021_invited; @brand_2023_recent]. The second development is double robust methods [@bang_2005_doubly; @kang_2007_demystifying], which estimate two models: one for treatment exposure and another for the outcome. These models are robust to misspecification of either one of these "nuisance" models.  

A number methods unifying these two developments have proliferated. These double robust methods for flexible covariate adjustment use machine learning methods to adapatively model the data generating processes at play. These models purportedly overcome the shortcomings of both traditional statistical methods and machine learning methods. Common statistical methods -- such as OLS regression and matching on propensity scores estimated from logistic regression -- have rigid functional form assumptions and fail to calculate stable estimates when the number of covariates is large relative to the number of observations. Machine learning methods, on the other hand, are often difficult to interpret. They can also suffer from overfitting, where the flexibility of the model becomes a weakness and predictions out-of-sample are poor, yet efforts to correct for overfitting can introduce regularization bias [@hastie_2009_elements]. Double robust methods with machine learning dispose of the constricting functional form assumptions of common statistical methods, and they correct for the regularization bias of flexible machine learning methods. They can also accommodate large numbers of covariates and produce easily interpretable treatment effect estimates. Despite their apparent advantages, these methods remain rarely utilized by social scientists. Part of the barrier has been lack of familiarity with these methods. It has also been unclear how these methods compare, or whether such methods actually perform better than traditional methods in finite samples.  

This paper makes advances on these fronts. First, it is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, explaining the methods to a social scientist audience. Methods covered include Augmented Inverse Probability Weighting (AIPW), Targeted Maximum Likelihood Estimation (TMLE), and Double or Debiased Machine Learning (DML). This paper reviews the theory behind these methods as well as simple R implementations. 

Second, this paper evaluates these methods. They are tested on simulations from @dorie_2019_automated, which cover a range of data-generating processes where ignorability holds -- i.e., there are no unmeasured confounders. In the Online Appendix, I also estimate experimental and observational effects from the National Support for Work Demonstration (NSW) originally analyzed by @lalonde_1986_evaluating. In these evaluations, double robust methods are compared to "single robust" methods (i.e., ones with one nuisance model). These include traditional or simpler statistical methods commonly used by social scientists: ordinary least squares (OLS) regression, matching on propensity scores estimated from logistic regression (PSM), and inverse probability weighting (IPW). They are also compared to two more flexible methods that may overcome the misspecification issues that double robust methods aim to overcome: G-computation and the Lin estimator [@lin_2013_agnostic]. Lastly, I use two of the double robust methods to replicate analyses from three recent *American Sociological Review* (ASR) papers that use OLS or logistic regression to estimate causal effects.  

Results from simulations where ignorability holds show that some double robust methods outperform traditional statistical methods, but only slightly. AIPW and TMLE perform the best, at least when used in conjunction with flexible machine learning algorithms, while DML does slightly worse than traditional methods. G-computation with flexible machine learning performs as well as the lowest-error double robust methods. Despite their lower average error, the computation times of these flexible methods are high. With its still relatively low error and much faster computation time, OLS remains a sensible choice as an estimator, and the Lin estimator -- which is as quick as standard OLS regression -- performs only slightly worse than the best double robust methods.  

Results from the NSW study highlight the importance of ignorability. When observational samples are not comparable to the experimental sample, or when important covariates are not included, all methods -- double robust or traditional -- fail to recover experimental estimates. When appropriate covariates are included, most methods perform well.  

In the *ASR* replications, double robust methods provide similar coefficients for most, but not all, of the analyses. In some cases, significance is lost, and in a minority of cases the coefficient sign changes. These replications highlight the importance of testing the robustness of quantitative results by comparing multiple methods and estimators.  
<!-- Exceptions include IPW, DML, and the Lin estimator (surprising given the simulation results), whose estimates tend to be less stable across samples.   -->


# Motivation

## Literature Review

Although previous work has compared different methods for covariate adjustment, there has not yet been a thorough comparison and evaluation of the recent and popular double robust methods of AIPW, TMLE, and DML for a social scientist audience. 

Although some introductions to double robust methods exist, they do not discuss them in the context of covariate adjustment for causal inference, or their treatment is overly technical for a social scientist audience. For example, @kang_2007_demystifying provide an an excellent overview and evaluation of double robust methods, but in the context of missing data, and the authors consider AIPW but not TMLE or DML. @bang_2005_doubly introduce double robust models for both causal inference and missing data, but their treatment is rather technical, and they only discuss AIPW. @lundberg_2022_researcher provide brief schematic overviews of double robust methods for a social scientist audience, but they do not evaluate these methods.

Existing evaluations of double robust methods have focused on only one double robust method and compared it to few traditional statistical methods. @dorie_2019_automated compare estimations from a number of different flexible methods, but these do not include AIPW or DML. @chatton_2020_gcomputation compare four methods -- G-computation, IPW, full matching, and TMLE -- but authors only consider one double robust method, and their focus is on omitted variable bias rather than determining which method is the most useful. @cousineau_2022_estimating evaluates the performance of optimization-based methods for causal inference, but these do not include the double robust methods covered in the current paper. @knaus_2022_double reviews DML-based methods in an econometrics setting but does not compare them to traditional statistical methods for covariate adjustment.  

An evaluation of multiple double robust methods that compares them to traditional statistical methods as well as flexible "single robust" methods is needed to understand just how practically useful these methods are for a social science audience. This paper does this as well as provides a gentle introduction to these methods.  


## Historical Overview 

According to @bang_2005_doubly, double robust methods have their origins in missing data models. @robins_1994_estimation and @rotnitzky_1998_semiparametric developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models. Drawing on the fact that causal inference is fundamentally a missing data problem, @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference.  

But @kang_2007_demystifying argue that double robust methods are older. They cite work by @cassel_1976_results, who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated. Arguably, double robust methods go back even further than this. The form of double robust methods is similar to residual-on-residual regression, which dates back to the Frisch-Waugh-Lowell (FWL) theorem [@frisch_1933_partial; @lovell_1963_seasonal]:
$$\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)}$$
where $\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$. This formulation writes the regression coefficient as composed of an outcome model ($\tilde Y_i$) and exposure model ($\tilde D_i$), the two models used in double robust estimators. Of the methods considered in this paper, double machine learning (DML) makes this connection most explicit by using residual-on-residual regression as part of its estimation strategy.  

There are also links between double robust methods and matching with regression adjustment. This work goes back at least as far as @rubin_1973_use, who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.  

Today, double robust methods abound [e.g. @arkhangelsky_2021_doublerobust; @ratkovic_2023_relaxing; @sloczynski_2018_general; @dukes_2022_doubly; @kennedy_2023_semiparametric]. Although double robust methods exist for instrumental variables [@okui_2012_doubly; @wang_2018_bounded], difference-in-differences [@santanna_2020_doubly], longitudinal data [@yu_2006_double; @tran_2019_double], and other causal applications, this paper focuses on three of the most popular and foundational methods for covariate adjustment in a cross-sectional setting.

## Aims of Double Robust Methods
Double robust methods for covariate adjustment aim to overcome what many consider to be the shortcomings of both traditional statistical methods and flexible machine learning methods [@diaz_2020_machine]. Statistical methods that are popular with social scientists -- such as OLS regression and matching on propensity scores from logistic regression -- have two main weaknesses that double robust methods address. First, they assume simple (linear or transformed linear) functional forms. In the presence of highly nonlinear data generating processes, they may provide biased estimates. Second, these methods cannot handle large numbers of covariates relative to sample size, i.e. sparsity. While some machine learning methods can produce estimates even when the number of covariates exceeds the number of observations (such as lasso), OLS fails in this case due to the $X^\top X$ matrix not being of full rank and hence not invertible. In cases with many covariates, but not more than the number of observations, estimation is unstable with many traditional statistical methods.  

Flexible machine learning methods also have their drawbacks. First, naive application of these methods can result in overfitting, with predictive accuracy maximized in sample but treatment effect estimation being biased. When regularization is used to correct for overfitting, "regularization bias" can result. Furthermore, results of these machine learning methods can be difficult to interpret without further processing. Machine learning methods have often been developed with a focus on prediction rather than on producing treatment effect point estimates.  

Double robust methods attempt to overcome the downsides of both traditional and machine learning methods by incorporating flexible models into a framework that avoids overfitting and regularization bias and provides easily interpretable estimates. These methods are also motivated by the idea that many older methods ignore information present in the data. Methods tend to model either only the outcome -- as in OLS regression and G-computation -- or only the treatment assignment -- as in propensity score matching or inverse probability weighting. Double robust methods, on the other hand, model both of these.  


# Conceptual Overview

Double robust methods estimate two models: an *outcome model*:
\begin{equation}
\mu_d(X_i) = E(Y_i \mid D_i = d, X_i)
(\#eq:outcome)
\end{equation}
and an *exposure model* (or treatment or propensity score model):
\begin{equation}
\pi(X_i) = E(D_i \mid X_i),
(\#eq:exposure)
\end{equation}
where $\mu_d(\cdot)$ is a model of the outcome, $D_i = d_i \in \{0, 1\}$ is the treatment assignment (where 0 is control and 1 is treated), $X_i$ is a vector of covariates for unit $i = 1, \ldots, N$, $Y_i$ is the outcome, and $\pi(\cdot)$ is a model of the exposure. The covariates included in $X_i$ can be different for the two models.  

The focus of this paper is on the average treatment effect (ATE), which under the potential outcomes framework [@rubin_1974_estimating] is defined as
$$\tau = E[Y_i(1) - Y_i(0)],$$
where $Y_i(1)$ and $Y_i(0)$ are the potential outcomes of $Y_i$ under treatment and control, respectively. An estimator is called "double robust" if it achieves consistent estimation of the ATE (or whatever estimand the researcher is interested in) as long as *at least one* of Equations \@ref(eq:outcome) or \@ref(eq:exposure) is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  

It is important to consider what is meant by a "correct" model specification [@keil_2018_resolving]. These estimators are robust from a statistical standpoint, but not necessarily a causal identification one. The researcher must know which variables are possible confounders and to include them in the appropriate models, while not including colliders or mediators [@hunermund_2023_double]. The simulations discussed in this paper assume conditional ignorability; rather than testing what happens when models are missing important covariates, it focuses on accurate specification of the functional form of the treatment and outcome models.    


## Assumptions

Most double robust methods require almost all of the standard assumptions necessary for most methods that depend on selection on observables. Although some double robust methods relax one or two of these, the methods discussed in this paper rely on six standard assumptions when estimating the ATE.    

1. Consistency: $Y_i(d) = Y_i \mid D_i = d$, i.e. under treatment (control), we observe the potential outcome under treatment (control).   

2. One version of treatment: All treated units receive the same version of treatment.  

3. No interference: $Y_i(D_i, D_j) = Y_i(D_i)$, i.e. the potential outcome for one unit depends only on its own treatment, not the value of other units' treatment.  

4. Positivity/overlap: $0 < \Pr(D = 1 \mid X=x) < 1$ for all values of $X$, i.e. there is non-zero probability of receiving treatment or control for every combination of covariates in the data. This means we can find at least one control unit to compare every treated unit to (and vice versa).  

5. Independent and identically distributed (IID) observations: In order to make population-level inference, the sample needs to be representative of the population.  

6. Conditional ignorability: $\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid X_i$, i.e. there are no unmeasured confounders.

The first three assumptions are embedded in the potential outcomes notation. Assumptions 2 and 3, together, are also called the Stable Unit Treatment Value assumption [SUTVA, @rubin_1980_randomization]. Special attention should be paid to Assumption 6: double robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the double robust methods covered in this tutorial make no functional form assumptions.  


# Overview of Techniques  
Each of the methods reviewed in this paper can be used with a variety of estimation techniques, including both traditional statistical methods and flexible machine learning methods. Each involves a model for the outcome and another for the treatment exposure, but choice of estimator for these two models is left to the discretion of the user. Double robust methods are distinct in the ways these estimated models relate and are combined into a final estimate of the desired estimand.  

This section provides some intuition for the mathematical theory behind each method and contains R code to simply implement these methods. This is the code used to evaluate these methods later in this paper, and it represents basic implementations of these methods. However, for researchers wishing to put these methods into practice, using a dedicated R packages for each method is probably a better idea. These packages have many flexible options, such as accounting for complex survey design, targeting estimands besides the ATE, and integrating with a variety of estimation techniques. In the Online Appendix, Table \@ref(tab:packages) suggests packages for each method.  

## Augmented Inverse Probability Weighting (AIPW)

The oldest of these modern methods, AIPW arose in the context of missing data imputation [@robins_1994_estimation]. @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference. Introductions to AIPW exist in the contexts of political science [@glynn_2010_introduction] and econometrics [@funk_2011_doubly]. The `AIPW` R package provides a simple implementation of the method [@zhong_2021_aipw].   

AIPW combines estimates from a model for the treatment exposure, $\pi(X)$, and a model for the outcome, $\mu(X)$. The name comes from the close similarity to inverse probability weights (IPW), but whereas IPW only weights for probability of treatment, AIPW "augments" these weights with an estimate of the response surface.    

Formally, the model can be written as the difference between an estimated outcome for treated units and an estimated outcome for untreated units (see the demonstration below):

$$\begin{aligned}
\hat \tau_{AIPW} = &\frac{1}{n} \sum_{i=1}^n \left( \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)} + \hat \mu_1(\mathbf X_i) \right) 
- \frac{1}{n} \sum_{i=1}^n \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} + \hat \mu_0(\mathbf X_i) \right)
\end{aligned}$$

In practice, AIPW weights may be very small or very large, a problem that inverse probability weights also suffer from. This can make AIPW prone to high variance. To remedy this, the predicted probabilities of treatment are often truncated, setting extremely small or large weights to some less extreme value [as in the `AIPW` R package, @zhong_2021_aipw].  

Below is R code to implement AIPW with truncation of extreme weights. As with all of the double robust methods reviewed here, we begin with predicted values (such as from a machine learning algorithm) for the outcome for treated units `mu1_pred` and untreated units `mu0_pred` as well as predicted values for treatment assignment probability `pi_pred`. We also have `d`, the vector of actual treatment assignments, and `y`, the observed outcome values.

```{r aipw-demo, eval = F, echo = T}
require(tidyverse)

aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  # Truncate extreme values of the weights
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  # Calculate the predicted outcome value for treated units
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  # Calculate the predicted outcome value for untreated units
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  # Calculate the ATE
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}
```

@glynn_2010_introduction provide an alternate but equivalent formula, where the basic inverse probability weight (IPW) estimator (which incorporates only the exposure model $\hat \pi$) is corrected using a weighted average of two outcome regression estimates: 
$$\begin{aligned}
\hat \tau_{AIPW} = & \frac{1}{n} \sum_{i=1}^n \left\{ \left[ \frac{D_i Y_i}{\hat \pi(\mathbf X_i)} - \frac{(1-D_i) Y_i}{ 1- \hat \pi(\mathbf X_i)} \right] - \frac{D_i - \hat \pi (\mathbf X_i)}{\hat \pi (\mathbf X_i)(1 - \hat pi (\mathbf X_i))} [(1- \hat \pi (\mathbf X_i)) \hat \mu_1(\mathbf X_i) + \hat \pi (\mathbf X_i) \hat \mu_0(\mathbf X_i)] \right\}.
\end{aligned}$$ 

## Targeted Maximum Likelihood Estimation (TMLE)

Extending and improving previous double robust methods, @vanderlaan_2006_targeted first proposed TMLE using a parametric framework and the efficient influence curve [@hines_2022_demystifying] to obtain estimates and standard errors. Mark van der Laan has gone on to collaborate on both a gentle introduction [@gruber_2009_targeted], two textbooks [@vanderlaan_2011_targeted; @vanderlaan_2018_targeted], and an R package [@gruber_2012_tmle] for implementing the method. @schuler_2017_targeted and @luque-fernandez_2018_targeted provide introductions for epidemiologists.

TMLE begins by estimating the relevant part of the data-generating distribution $P(Y)$, i.e. the conditional density $Q = P(Y \mid X)$. It next estimates the exposure model. Although any estimation method can be used for these steps, the originators of the method suggest using a "SuperLearner," i.e. ensemble learning with cross-validation [@vanderlaan_2007_super]. Next, the exposure model is used to calculate a "clever covariate," which is similar to an IPW. The coefficient for this clever covariate is estimated using maximum likelihood -- whence the "MLE" in "TMLE." Finally, the estimate of $Q$ is updated in a function involving the clever covariate. This process can be iterated, but usually one iteration is enough. The estimate of the distribution $Q$ can be used to calculate the estimand of interest.  

<!-- Then create the variable $H_{di}$ for targeting step: -->
<!-- $$H_{di} = \frac{I(D_i = 1)}{\hat \pi (\mathbf X_i)} - \frac{I(D_i=0)}{1 - \hat \pi (\mathbf X_i)}.$$ -->

Formally, first generate estimates of $\mu_{d}(\mathbf X_i) = E(Y \mid D=d, \mathbf X_i)$ and $\pi(\mathbf X_i) = P(D=1 \mid \mathbf X_i)$. Next, calculate the clever covariates for each individual in the data. These quantities are similar to inverse probability weights, with $H_{0i}$ for untreated and $H_{1i}$ for treated units:
$$\begin{aligned}
H_{0i}(D=0, \mathbf X = \mathbf x_i) &= \frac{1-d_i}{1- \hat \pi (\mathbf x_i)}, &
H_{1i}(D=1, \mathbf X = \mathbf x_i) = \frac{d_i}{\hat \pi (\mathbf x_i)}.
\end{aligned}$$

In the next step, we estimate fluctuation parameters $\epsilon = (\epsilon_0, \epsilon_1)$ through maximum likelihood of the following logistic regression with fixed intercept $\text{logit}(\mu_{di})$: 
<!-- $$E(Y=1 \mid D, \mathbf X) = \text{logit}(\mu_{di}) + \epsilon H_{di}$$ -->
$$\text{logit}[E(Y=1 \mid D, \mathbf X)] = \text{logit}(\hat \mu_{di}) + \epsilon_0 H_{0i} + \epsilon_1 H_{1i}$$

Here we are assuming that $Y$ is a dichotomous variable taking the values of 0 or 1; the method is extended to continuous outcomes simply by normalizing the value of $Y$ to fall between 0 and 1. Then we generate updated ("targeted") estimates of potential outcomes:
$$\begin{aligned}
\hat \mu_0^*(\mathbf x_i) &= \text{expit}[\text{logit}(\hat \mu_0(\mathbf x_i)) + \hat \epsilon H_{0i}]\\
\hat \mu_1^*(\mathbf x_i) &= \text{expit}[\text{logit}(\hat \mu_1(\mathbf x_i)) + \hat \epsilon H_{1i}]
\end{aligned}$$
where $\text{expit}(\cdot)$ is the inverse logit function.   

Finally, we estimate the parameter of interest -- in this case, the ATE:
$$\hat \tau_{TMLE} = \frac{1}{n} \sum_{i=1}^n [\hat \mu_1^*(\mathbf x_i) - \hat \mu_0^*(\mathbf x_i)]$$

Here is R code to implement TMLE, again with predicted outcome values `mu1_pred` and `mu0_pred` and predicted probability of treatment `pi_pred`. Since the outcome is bounded and continuous, it is transformed to fall between 0 and 1 via $\tilde Y_{i} = [Y_i - \min(Y)]/[(\max(Y)-\min(Y)]$.
```{r tmle-demo, eval = F, echo = T}
# Functions for normalization and de-normalization
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}

tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  # Normalize the outcome variable
  mu1_pred <- normalize(mu1_pred, y)
  mu0_pred <- normalize(mu0_pred, y)
  y_tilde <- normalize(y, y))
  
  n <- length(y)

  # Calculate clever covariates
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  # Estimate fluctuation parameter through maximum likelihood estimation
  epsilon <- glm(y_tilde ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  # Targeted estimates of the potential outcomes
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H1))
  
  # Estimate ATE
  ATE <- mean(target_1 - target_0)
  return(denormalize(ATE, y))
}
```


## Double/Debiased Machine Learning (DML)

The most recently developed of the methods reviewed here, DML was proposed in an econometrics context [@chernozhukov_2018_double] and has since seen a flurry of development [@dukes_2022_doubly; @kennedy_2023_semiparametric; @semenova_2021_debiased; @chernozhukov_2022_long; @farbmacher_2022_causal; @jung_2021_estimating]. The R package `DoubleML` [@bach_2021_doubleml] provides straightforward implementation of the method.  

DML is motivated by the need to handle problems with high-dimensional nuisance parameters, i.e. a large number of measured confounders. Flexible machine learning is appropriate for this task, but such methods suffer from regularization bias, where efforts to control the overfitting of models can bias estimates. DML removes this bias in a two-step procedure. First, it solves the auxiliary problem of estimating the treatment exposure model $E(D \mid X) = \pi(X)$. It then uses this model to remove bias: Neyman orthogonalization allows the creation of an orthogonalized regressor, essentially partialing out the effect of covariates $X$ from treatment $D$. The debiased $D$ is then used to estimate the conditional mean of the outcome $E(Y \mid X) = \mu(X)$, which can be used to calculate the estimand of interest.  

More formally, suppose we want to estimate $\tau$ in the following framework:^[Note that this basic DML setup -- presented in the introduction of @chernozhukov_2018_double -- assumes a partially linear model and targets the ATE. If we are interested in the CATE and heterogeneous effects, Chernozhukov et al. [-@chernozhukov_2018_double, p. C35] present an alternative score function that closely resembles AIPW. In this case, DML and AIPW are identical, except DML also includes sample splitting, accounting for the regularization bias that flexible machine learning estimators may induce. See also @jacob_2021_cate and @nie_2021_quasioracle.]
$$y_i = \tau d_i + g_0(\mathbf x_i) + u_i,$$
$$d_i = m_0(\mathbf x_i) + v_i.$$

The idea is to estimate $g_0$ and $m_0$ separately, then use an orthogonalized or debiased score function -- here, residual-on-residual regression -- to obtain an estimate of $\tau$, which we can designate $\hat \tau$. However, this leaves a term in the asymptotic distribution of $\hat \tau$ that biases the estimate. To avoid this, DML uses sample splitting [@angrist_1995_splitsample].  

We randomly split the sample of $n$ observations into two sets, $I$ and $I^c$, each of size $n/2$.^[In practice, we can split the sample into any number of folds, and more than two sets might be better.] Using any prediction algorithm, we then estimate the response and treatment models using only set $I^c$:  

1) Estimate treatment model $\hat m_0$ in the equation $d_i = \hat m_0(\mathbf x_i) + \hat v_i, \forall i \in I^c$.  
2) Estimate the outcome model $\hat g_0$ in the equation $y_i =  \hat g_0(\mathbf x_i) + \hat u_i, \forall i \in I^c$.

<!-- Note that, in this setup, the estimation of the outcome does not consider treatment assignment $d_i$. This is a key difference between DML and AIPW or TMLE.   -->

Next, we use the estimated models to perform residual-on-residual regression *on the left out set* $I$ to obtain an estimate of $\tau$:
$$\hat \tau(I^c, I) = \left(\sum_{i \in I} \hat v_i d_i \right)^{-1} 
\sum_{i \in I} \hat v_i (y_i - \hat g_0 (\mathbf x_i)),$$
where $\hat v_i = d_i - \hat m_0(\mathbf x_i)$. Using half the sample results in efficiency loss. To rectify this, we repeat the above procedure, switching the split sets. We then have $\hat \tau(I^c, I)$ and $\hat \tau(I, I^c)$. The cross-fitting DML estimator is:
$$\hat \tau_{DML} = \frac{\hat \tau (I^c, I) + \hat \tau (I, I^c)}{2}.$$  

R code to implement DML is shown below. Since DML involves sample splitting, this code is a little different from the above examples. We start with observed outcome values `y`, treatment assignment `d`, and covariate matrix `x`. First, a pre-processing function `dml_pre()` randomly splits the sample, outputting $I$ and $I^c$ sets of each of these variables. The second step predicts outcome values and treatment probabilities for each half of the sample, using models fit to the other half. In the code chunk here, generalized random forests from the `grf` package are used to predict these, but any prediction algorithm can be used. Finally, a post-prediction function `dml_post()` performs the residual-on-residual regression for each half of the sample and finds the average of the two estimates to produce an ATE estimate. 
```{r dml-demo, eval = F, echo = T}
# Pre-processing: sample splitting
dml_pre <- function(y, d, x, seed = 1758){
  set.seed(seed)
  
  n <- length(y)
  n_2 <- round(n/2)
  
  # Split the sample
  random_vec <- sample(1:n, n, replace = F)
  I <- random_vec[1:n_2]
  I_c <- random_vec[(n_2+1):n]
  
  return(list(
    y_I = y[I],
    d_I = d[I],
    x_I = x[I,],
    y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

# Predictor function: in this case, generalized random forests
grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  # Train on the I_c sample, predict on the I sample
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, tune.parameters = "all")
  mu_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  # Train on the I sample, predict on the I_c sample
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, tune.parameters = "all")
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod2, newdata = x_I_c)$predictions

  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

# Implement DML: takes outputs from pre_dml() and grf_dml()
dml_post <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  # Residual-on-residual regression for each sample separately
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  # Average estimates from each sample
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

dml_pre_out <- dml_pre(y = y, d = d, x = x)
grf_dml_out <- do.call(grf_dml, dml_pre_out)
dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
```





## A simple demonstration using AIPW

To demonstrate double robustness, this section presents one of the simpler double robust estimators, AIPW. As shown above, we can write this estimator as follows:

$$\begin{aligned}
\hat \tau = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) 
- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}$$

For each individual in the sample, this estimator calculates two quantities:

- The treated potential outcome

\begin{equation}
\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)
(\#eq:treated-aipw)
\end{equation}

- The control potential outcome

\begin{equation}
\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i)
(\#eq:control-aipw)
\end{equation}

Let's focus on the treated model, Equation \@ref(eq:treated-aipw). First, assume that the outcome model $\mu_1(X_i)$ is *correctly* specified and the exposure model $\pi(X_i)$ is *incorrectly* specified. Let's also assume (for now) that we're dealing with a treated unit, i.e. $D_i = 1$. Then
$$E[\hat \mu_1 (X_i)] = E[Y_1 \mid X_i].$$
This means that
$$E[Y_i - \hat \mu_1(X_i)] = 0,$$
and hence
$$E[\hat Y_{1i}] = 0 + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$
So the model relies *only* on the outcome model. The incorrectly specified exposure model completely disappears from the equation. If we're dealing with a control unit ($D_i=0$), we get the same result:
$$\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$  

Now, what if the *exposure* model $\pi(X_i)$ is correctly specified and the outcome model $\mu_1(X)$ is incorrect? First, we rewrite the estimator for the treated outcome:
\begin{align}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \nonumber \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \nonumber \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). 
(\#eq:exposure-correct)
\end{align}
Since the exposure model is correctly specified, we have $D_i = \hat \pi(X_i)$ on average, so
$$E[D_i - \hat \pi(X_i)] = 0.$$
This means that the second term in Equation \@ref(eq:exposure-correct) is 0, so
$$E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].$$
This shows that when the exposure model is correct, then the estimator depends *only* on the exposure model. We can make similar arguments for the control model for $\hat Y_{0i}$ in Equation \@ref(eq:control-aipw).  

This demonstration shows that this estimator achieves double robustness: The estimator is robust to misspecification of either the exposure or the outcome model (but not both). The other two double robust methods considered in this paper can be shown to have the same property, but proving so is more complicated.

# Evaluation Strategy  

These double robust methods have many similarities. How do the results they give compare? This section tests the performance of each in practice using two strategies. First, results are compared using simulated data from a causal inference competition [@dorie_2019_automated]. The true treatment effect is known and all potential confounders are observed, so these simulations allow assessment of bias and related quantities. Second, these methods are applied to data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW). The NSW randomly provided training to disadvantaged workers, allowing an experimental estimate of the effect of the intervention, and data assembled by @dehejia_1999_causal compares these experimental estimates to observational ones.  

```{r methods-overview}

read_csv(here('files/methods.csv')) %>%
  huxtable::hux() %>%
  huxtable::theme_article() %>%
  huxtable::set_width(1) %>%
  # huxtable::set_all_padding(0) %>%
  # huxtable::set_latex_float('h!') %>%
  huxtable::set_caption('Double and single robust methods used for evaluation. The super learner estimator is an ensemble learning method relying on GLM, glmnet, and XGBoost.')
  
  # pander::pander(split.cell = 15, split.table = Inf, 
  #                justify = 'left',
  #                caption = 'Double and single robust methods used for evaluation',
  #                label = 'methods-overview') 
  # kable(booktabs = T, linesep = '', caption = 'Double and single robust methods used for evaluation') %>%
  # kable_styling(latex_options = c("hold_position"))


```


The three double robust methods are compared to two sets of traditional or "single robust" methods used as benchmarks (see Table \@ref(tab:methods-overview) for an overview of all methods used). By "single robust," I mean methods that are not robust to any misspecification. First are one-model methods. The most classic method considered -- linear regression -- models only the response surface. It is estimated using ordinary least squares regression ("OLS"), entering each variable separately without any interactions or higher-order terms. Two other one-model methods model only the treatment assignment mechanism. In propensity score matching (PSM), propensity scores are estimated from logistic regression with each variable entered separately and without any higher order terms, then matched using the `MatchIt` package. Finally, stabilized inverse probability weights [IPW, @austin_2015_moving, p. 3663] are used for weighted OLS regression. These IPWs are estimated using propensity scores estimated by each of the three under-the-hood estimation techniques described below; extreme propensity scores are truncated so that they range from 0.01 to 0.99.  

The second set of single robust methods are two-model methods, which estimate separate models for treated and untreated units. In theory these could solve the misspecification problem that double robust methods are meant to solve, but they could still suffer from overfitting. G-computation [@robins_1986_new; @snowden_2011_implementation] uses some estimation technique to predict outcomes under treatment and control for each unit in the dataset. The ATE estimate is the difference in the average prediction under treatment and the average prediction under control. The second two-model method is the Lin estimator [@lin_2013_agnostic]. This method aims to solve issues with the bias induced by OLS regression in a randomization framework by interacting the treatment indicator with mean-centered covariates. @hazlett_2024_understanding show that this method is equivalent to estimating two separate OLS regression models for the treated and control units.  

Because many of these methods allow the user to choose the underlying estimation method, results compare three estimators The first estimator uses a logistic regression for the exposure model and an OLS regression for the outcome model. Second is generalized random forests [GRF, @athey_2019_generalized] using the `grf` R package, with separate models for exposure and outcome. The final estimator is the SuperLearner (as promoted by the makers of TMLE) using the `SuperLearner` package [@polley_2023_superlearner], again with separate models for exposure and outcome. GLM, glmnet [a weighted average of lasso and ridge regression, @friedman_2021_package], and XGBoost with a maximum tree depth of 4 [@chen_2016_xgboost] are the models considered for the SuperLearner. These three estimation techniques are used for each of the three double robust methods and for IPW. GRF and SuperLearner are also used for G-computation (OLS predictions with G-computation return identical results to OLS regression coefficient estimates).  

For the simulations, results compare only point estimates from these methods. For evaluation of the experimental LaLonde data, I use bootstrapping with 100 samples to obtain standard errors and confidence intervals. I use the code shown above for AIPW, TMLE, and DML, and I also compare results for two ready-made double robust packages. These include `grf` [@tibshirani_2024_grf], which implements AIPW with generalized random forests, and `DoubleML` [@bach_2024_doubleml], which I use to implement a partially linear DML model with a SuperLearner using GLM, glmnet, and XGBoost.






```{r dorie-sim, eval = F}
set.seed(185)

# sims %>%
#   lapply(function(x) append(x, list()))

sim_list <- list()
for(i in 1:20){ # up to 77
  #sim_list_nest <- list()
  for(j in 1:10){ # up to 100
    print(paste0(i,',',j))
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016,
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'main'))
  }
}

sim_linear <- list()
for(i in c(1, 3)){
  for(j in 1:100){ 
    print(paste0(i,',',j))
    sim_linear[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      i, j, extraInfo = T) %>% 
      append(list(dataset = i, set = 'linear'))
  }
}


saveRDS(append(sim_list, sim_linear), here('files', 'sims.RDS'))
```

```{r large, eval = F}
# dataset 7: polynomial     0.35    one-term exponential      0.75      high
set.seed(185)

sim_list <- list()

for(j in c(2, 5, 10, 20)){ # up to 100
  for(i in 1:20){ # up to 77
  #sim_list_nest <- list()

    print(paste0(i,',',j))
    
    dataset <- aciccomp2016::input_2016 %>% 
      slice(rep(1:n(), each = j))
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(dataset, 7, i, extraInfo = T) %>% 
      append(list(dataset = 7, set = 'large', size = j*4802))
  }
}

saveRDS(sim_list, here('files', 'sims_large.RDS'))

```

```{r sims-small, eval = F}
set.seed(185)

sim_list <- list()
for(j in c(150, 300, 600, 1200, 2400, 4802)){ 
  for(i in 1:20){ 
  #sim_list_nest <- list()

    print(paste0(i,',',j))
  
    sample_index <- sample(1:4802, j, replace = F)
    
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 7, i, extraInfo = T) %>% 
      append(list(dorie_dataset = 7, set = 'small', size = j))
    
    for(name in names(sim_list[[paste0(i,',',j)]])[1:7]){
      sim_list[[paste0(i,',',j)]][[name]] <- sim_list[[paste0(i,',',j)]][[name]][sample_index]
    }
    
    sim_list[[paste0(i,',',j)]]$x <- sim_list[[paste0(i,',',j)]]$x[sample_index,]
  }
}

saveRDS(sim_list, here('files', 'sims_small.RDS'))
```



```{r sim-test, eval = F}
data_test <- bind_rows(aciccomp2016::input_2016, aciccomp2016::input_2016)

acs <- read_dta('/Users/nathan/Data/ACS/acs_2008_2021.dta')

test <- aciccomp2016::dgp_2016(data_test, parameter_list, 1950, extraInfo = T)
set.seed(185)
test_acs <- aciccomp2016::dgp_2016(filter(acs, year == 2016) %>% drop_na() %>% sample_n(1e5), 
                                   parameter_list, 1950, extraInfo = T)
```


```{r lm-sim, eval = F}
lm_df <- lm_sim(sims) 

write_csv(lm_df, here('files', 'lm_df.csv'))


# lm_df %>%
#   group_by(set) %>%
#   summarize(ate_est = mean(ate), 
#             se = sd(ate))
```


```{r psm-sim, eval = F}
psm_df <- psm_sim(sims) 

write_csv(psm_df, here('files', 'psm_df.csv'))
```


```{r pred-calc, eval = F}

ols_logit_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  ols_logit_pred_list[[i]] <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(ols_logit_pred_list, here('files/homemade', 'ols_logit_pred.RDS'))

# tibble(ols_logit_pred_list[[i]]$y,
#              ols_logit_pred_list[[i]]$mu1_pred, 
#              ols_logit_pred_list[[i]]$mu0_pred,
#              ols_logit_pred_list[[i]]$d,
#              hist(ols_logit_pred_list[[12]]$pi_pred)
#              ) 

grf_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  grf_pred_list[[i]] <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(grf_pred_list, here('files/homemade', 'grf_pred.RDS'))

 
superlearner_pred_list <- list() 
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  superlearner_pred_list[[i]] <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')),
                dorie_dataset = sims[[i]]$dataset,
                set = sims[[i]]$set,
                size = sims[[i]]$size))
}
saveRDS(superlearner_pred_list, here('files/homemade', 'superlearner_pred.RDS'))


```


```{r ipw, eval = F}
ols_logit_pred_list2 <- readRDS(here('files/homemade', 'ols_logit_pred.RDS'))
grf_pred_list2 <- readRDS(here('files/homemade', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files/homemade', 'superlearner_pred.RDS'))


ipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    

    y <- sims[[i]]$y
    D <- sims[[i]]$z == 'trt'
    PrD <- mean(D)
    # ps <- predictions[[j]][[i]]$pi_pred
    ps <- case_when(
      predictions[[j]][[i]]$pi_pred < .01 ~ .01,
      predictions[[j]][[i]]$pi_pred > .99 ~ .99,
      T ~ predictions[[j]][[i]]$pi_pred)
      
    
    # ate_ipw <- sum((D*y)/ps) / sum(D/ps) - sum(((1-D)*y)/(1-ps)) / sum(1-D/(1-ps)) 
    
    # ate_ipw <- mean(y*(D==1*PrD)/ps - y*((D==0)*(1-PrD))/(1-ps))
    
    
    ipw <- (D*PrD)/ps + ((1-D)*(1-PrD))/(1-ps)
    lm_out <- lm(sims[[i]]$y ~ D, weights=ipw)
    ate_ipw <- coefficients(lm_out)[[2]]
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    ipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = ate_ipw, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(ipw_out_list), here('files', 'ipw.csv'))
```

```{r g-comp, eval = F}
grf_pred_list2 <- readRDS(here('files/homemade', 'grf_pred.RDS'))
superlearner_pred_list2 <- readRDS(here('files/homemade', 'superlearner_pred.RDS'))


gcomp_out_list <- list()
predictions <- list(grf_pred_list2, superlearner_pred_list2)
pred_names <- c('grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    
    gcomp_out <- with(predictions[[j]][[i]], mean(mu1_pred - mu0_pred))
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    gcomp_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = gcomp_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(gcomp_out_list), here('files', 'gcomp.csv'))
```

```{r lin, eval = F}

n <- length(sims)
lin_list <- list()
for(i in 1:n){
  start_time <- Sys.time()
  
  print(paste0(i, ' out of ', n))
  sim_dat <- data.frame(y = sims[[i]]$y, 
                        d = ifelse(sims[[i]]$z == 'trt', 1, 0), 
                        sims[[i]]$x)
  
  ate <- NA
  tryCatch({
    lm_out <- tidy(estimatr::lm_lin(y ~ d, 
                                    covariates = as.formula(paste0('~ ', paste(names(sims[[i]]$x), collapse = ' + '))), 
                                    data = sim_dat))
    ate <- lm_out[[2,2]]
    
    }, error=function(e){
      cat("ERROR :",conditionMessage(e), "\n")
      })
  
  lin_list[[i]] <- data.frame(dataset = i,
                             ate = ate,
                             #se = lm_out[[2,3]],
                             truth = mean(sims[[i]]$y.1) - mean(sims[[i]]$y.0),
                             dorie_dataset = sims[[i]]$dataset,
                             set = sims[[i]]$set,
                             size = sims[[i]]$size,
                             comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs')))
}


write_csv(bind_rows(lin_list), here('files', 'lin.csv'))
```

```{r aipw, eval = F}



aipw_calc2 <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, c(.025)) ~ as.numeric(quantile(pi_pred, c(.025))),
  #   pi_pred > quantile(pi_pred, c(.975)) ~ as.numeric(quantile(pi_pred, c(.975))),
  #   T ~ pi_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)
  
  ipw <- (d*y)/pi_pred - (1-d)*y/(1-pi_pred)
  adjust <- (d-pi_pred)/(pi_pred*(1-pi_pred)) * ((1-pi_pred)*mu1_pred + pi_pred*mu0_pred)
  
  ate <- (1/n) * sum(ipw - adjust)
  
  return(ate)
}


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    #   }
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade', 'aipw.csv'))


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    #   }
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    
    
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade', 'aipw_trunc.csv'))



```


```{r tmle, eval = F}

tmle_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    # if(j == 1){
    #   predictions[[j]][[i]]$pi_pred <- case_when(
    #     predictions[[j]][[i]]$pi_pred < .01 ~ .01,
    #     predictions[[j]][[i]]$pi_pred > .99 ~ .99,
    #     T ~ predictions[[j]][[i]]$pi_pred)
    # }
    
    start_time <- Sys.time()
    tryCatch({
      tmle_out <- with(predictions[[j]][[i]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
    }, error=function(e){
      tmle_out <- NA
      })
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    tmle_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = denormalize(tmle_out, predictions[[j]][[i]]$y), 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      fail = is.na(tmle_out),
      dorie_dataset = sims[[i]]$dataset,
      set = sims[[i]]$set,
      size = sims[[i]]$size)
  }
}

write_csv(bind_rows(tmle_out_list), here('files/homemade', 'tmle.csv'))


# logit_logit_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   logit_logit_pred_out <- logit_logit_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(logit_logit_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y)) 
#   truth <- mean(dat$y.1) - mean(dat$y.0)
# 
#   logit_logit_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(logit_logit_pred_list), label = 'ols, logit, tmle')
# 
# 
# #### 
# 
# grf_pred_list <- list()
# 
# for(i in 1:10){ #length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   grf_pred_out <- grf_pred(
#     y = dat$y, # with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   yt <- function(x, y){(x - min(y)) / (max(y) - min(y))}
#   
#   ate <- with(grf_pred_out, 
#               tmle_calc(mu1_pred = yt(mu1_pred, dat$y), 
#                         mu0_pred = yt(mu0_pred, dat$y),
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = yt(y, dat$y)))
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#   
#   grf_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(grf_pred_list), label = 'grf, tmle')


# superlearner_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   superlearner_pred_out <- superlearner_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(superlearner_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y))
#   truth = mean(dat$y.1) - mean(dat$y.0)
#   
#   superlearner_pred_list[[i]] <- data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(superlearner_pred_list), label = 'superlearner, tmle')
```




```{r dml, eval = F}

## DML OLS logit

ols_logit_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
   
  start_time <- Sys.time()
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)

  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
   
  ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(ols_logit_dml_list), 
          here('files/homemade', 'dml_ols_logit.csv'))

## DML OLS logit (truncated)
# 
# ols_logit_dml_list <- list()
# 
# set.seed(1285)
# for(i in 1:length(sims)){
#   print(i)
#   dat <- sims[[i]]
#   
#   start_time <- Sys.time()
#   dml_pre_out <- dml_pre(dat$y,
#                        as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#                        x = dat$x)
# 
#   ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
#   
#   ols_logit_dml_out$pi_pred1 <- case_when(
#         ols_logit_dml_out$pi_pred1 < .01 ~ .01,
#         ols_logit_dml_out$pi_pred1 > .99 ~ .99,
#         T ~ ols_logit_dml_out$pi_pred1)
# 
#   ols_logit_dml_out$pi_pred2 <- case_when(
#         ols_logit_dml_out$pi_pred2 < .01 ~ .01,
#         ols_logit_dml_out$pi_pred2 > .99 ~ .99,
#         T ~ ols_logit_dml_out$pi_pred2)
#     
#     
#   
#   dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
#   comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
#   
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#    
#   ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
#                               truth = truth, 
#                               comp_time = comp_time,
#                               dataset = i,
#                               dorie_dataset = dat$dataset,
#                               set = dat$set,
#                               size = dat$size)
# }
# 
# write_csv(bind_rows(ols_logit_dml_list), 
#           here('files/homemade', 'dml_ols_logit_trunc.csv'))



# perform(bind_rows(ols_logit_dml_list), label = 'grf, dml')
 

## GRF
grf_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
   
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      grf_dml_out <- do.call(grf_dml, dml_pre_out)

      dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  grf_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(grf_dml_list), 
          here('files/homemade', 'dml_grf.csv'))

# perform(bind_rows(grf_dml_list), label = 'grf, dml')

## Superlearner
superlearner_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  dml_post_out <- NA
   tryCatch({
      superlearner_dml_out <- do.call(superlearner_dml, dml_pre_out)
      dml_post_out <- do.call(dml_post, append(dml_pre_out, superlearner_dml_out ))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  superlearner_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
}

write_csv(bind_rows(superlearner_dml_list), 
          here('files/homemade', 'dml_superlearner.csv'))



#perform(bind_rows(superlearner_dml_list), label = 'superlearner, dml')

```

```{r aipw-package, eval = F}
aipw_package_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(paste0(i, ' out of ', length(sims)))
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  y <- dat$y
  d <- as.numeric(ifelse(dat$z == 'trt', 1, 0))
  x <- model_matrix(~., dat$x)[,]
  
  grf_model <- grf::causal_forest(X = x, Y = y, W = d)                        
  grf_out <- grf::average_treatment_effect(grf_model, target.sample = 'all', method = 'AIPW', seed = 123)
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  aipw_package_list[[i]] <- data.frame(ate = grf_out[['estimate']], 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
  
  if(i %% 100 == 0){
    write_csv(bind_rows(aipw_package_list), 
          here('files/homemade', 'aipw_package.csv'))
  }

}

write_csv(bind_rows(aipw_package_list), 
          here('files/homemade', 'aipw_package.csv'))


```


```{r dml-package, eval = F}
library(DoubleML)
library(mlr3)
library(mlr3pipelines)
library(mlr3learners)
library(mlr3extralearners)

dml_package_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(paste0(i, ' out of ', length(sims)))
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_data <- double_ml_data_from_matrix(X = model_matrix(~., dat$x)[,],
           y = dat$y,
           d = as.numeric(ifelse(dat$z == 'trt', 1, 0)))

  graph_ensemble_regr = gunion(list(
      po("learner", lrn("regr.cv_glmnet", s = "lambda.min")),
      po("learner", lrn('regr.xgboost', max_depth = 4)),
      po("learner", lrn("regr.glm"))
    )) %>>%
      po("regravg", 3)
  
  ensemble_pipe_regr = as_learner(graph_ensemble_regr)
  set.seed(123)
  obj_dml_plr_sim_pipe_ensemble = DoubleMLPLR$new(dml_data,
                                                  ml_l = ensemble_pipe_regr,
                                                  ml_m = ensemble_pipe_regr)
  obj_dml_plr_sim_pipe_ensemble$fit()
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  dml_package_list[[i]] <- data.frame(ate = as.vector(obj_dml_plr_sim_pipe_ensemble$coef), 
                              truth = truth, 
                              comp_time = comp_time,
                              dataset = i,
                              dorie_dataset = dat$dataset,
                              set = dat$set,
                              size = dat$size)
  
  if(i %% 100 == 0){
    write_csv(bind_rows(dml_package_list), 
          here('files/homemade', 'dml_package.csv'))
  }

}

write_csv(bind_rows(dml_package_list), 
          here('files/homemade', 'dml_package.csv'))



```



# Simluations with Dorie et al. (2019) Data

 



```{r results-load}

lm_df <- read_csv(here('files', 'lm_df.csv')) %>%
  mutate(method = 'ols')

psm_df <- read_csv(here('files', 'psm_df.csv')) %>%
  mutate(method = 'psm')

lin_df <- read_csv(here('files', 'lin.csv')) %>%
  mutate(method = 'lin')

ipw_df <- read_csv(here('files', 'ipw.csv')) %>%
  mutate(estimator = method,
         method = 'ipw')

gcomp_df <- read_csv(here('files', 'gcomp.csv')) %>%
  mutate(estimator = method,
         method = 'g-comp')

aipw_df <- read_csv(here('files/homemade', 'aipw.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

aipw_trunc_df <- read_csv(here('files/homemade', 'aipw_trunc.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

tmle_df <- read_csv(here('files/homemade', 'tmle.csv')) %>%
  # filter(fail == F) %>%
  select(-fail) %>%
  mutate(estimator = method,
         method = 'tmle')

dml_df <- bind_rows(
  mutate(read_csv(here('files/homemade', 'dml_ols_logit.csv')), 
         estimator = 'ols_logit',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_grf.csv')), 
         estimator = 'grf',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_superlearner.csv')), 
         estimator = 'superlearner',
         method = 'dml',
         dataset = row_number())
) %>%
  select(names(tmle_df))

# aipw_package_df <- read_csv(here('files/homemade', 'aipw_package.csv')) %>%
#   mutate(estimator = 'grf (package)',
#          method = 'aipw')
# 
# dml_package_df <- read_csv(here('files/homemade', 'dml_package.csv')) %>%
#   mutate(estimator = 'superlearner (package)',
#          method = 'dml')


sim_results <- bind_rows(lm_df, psm_df, lin_df, ipw_df, gcomp_df, aipw_trunc_df, tmle_df, dml_df) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'ols', 'logit', 
          'grf', 'grf (package)', 'superlearner', 'superlearner (package)')),
         method = factor(method, levels = c('ols', 'psm', 'ipw', 'g-comp', 'lin', 
                                            'aipw', 'tmle', 'dml', 'aipw (original)', 'aipw (package)'))) %>%
  arrange(method, estimator) %>%
  #mutate(estimator = ifelse(is.na(estimator), 'NA', estimator)) %>%
  mutate(bias = ate - truth,
         method_estimator = ifelse(is.na(estimator), as.character(method), paste0(method, ', ', estimator))) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) 

# write_csv(sim_results, here('files', 'lalonde_means.csv'))
```


In 2016, the Atlantic Causal Inference Conference hosted a competition for causal inference methods that adjust on observables. @dorie_2019_automated published the results of this competition, along with the data used in the competition. Below, I test double robust methods on the 20 data sets used for the "do-it-yourself" part of the competition. The data represent a hypothetical twins study investigating the impact of birth weight on IQ. The data have 4,802 observations and 52 covariates. The authors of the study specify a different data generating process for the potential outcomes in each data set. In all cases, ignorability holds (all potential confounders are observed), but the authors vary the following:  

- degree of nonlinearity
- percentage of treated
- overlap for the treatment group
- alignment (correspondence in variables used to generate the assignment mechanism and the response surface)
- treatment effect heterogeneity  

The true treatment effect also varies, but as a function of the other DGP characteristics. It has a mean of `r sim_results %>% filter(set == 'main') %>% summarize(round(mean(truth), 1))`, standard deviation of `r sim_results %>% filter(set == 'main') %>% summarize(round(sd(truth), 1))`, and range of `r sim_results %>% filter(set == 'main') %>% pull(truth) %>% min() %>% round(1)` to `r sim_results %>% filter(set == 'main') %>% pull(truth) %>% max() %>% round(1)`. 

## Main results

The 20 data sets used here cover a range of these attributes; see the supplemental material from @dorie_2019_automated for details. I use 10 simulations of each data set, resulting in 200 data sets. I then calculate bias, percent bias (the estimator's bias as a percentage of its standard error), root mean squared error (rmse), and median absolute error (mae). I also present the number of datasets for which the method fails and the median computation time for each data set, in seconds.^[Simulations were run on a 2020 MacBook Pro laptop computer with a 2 GHz Quad-Core Intel Core i5 Processor and 16 GB of memory.] In the main text I present average bias and RMSE, while the Online Appendix contains tables with full results.  

Bias results for the full range of simulations are shown in Figure \@ref(fig:dorie-results-bias). Bias is quite low for many of the methods, however IPW (logit) and AIPW (OLS/logit) have high bias and variance, while TMLE (OLS/logit) has moderately high bias and variance. With an average true treatment effect of `r sim_results %>% filter(set == 'main') %>% summarize(round(mean(truth), 1))`, bias with absolute value greater than 1 is substantial. The traditional methods, however, achieve fairly low bias in general.

Figure \@ref(fig:dorie-results-rmse) orders methods by RMSE and presents both bias and RMSE. The lowest RMSE is achieved by three of the methods using SuperLearner estimators (AIPW, TMLE, and G-computation) with values of about 0.35, followed closely by the same three methods using GRF, with values closer to 0.5. The computationally efficient Lin estimator does not do much worse, with an RMSE of 0.6, and OLS and PSM achieve acceptable RMSE of 0.7 to 0.9. Interestingly, DML with the computationally efficient OLS/logit estimators achieves lower RMSE than with GRF or SuperLearner (0.8 compared to 0.9 and 1.6, respectively). The only estimators with RMSE that exceed 2 are TMLE (OLS/logit) with 2.3, IPW (logit) with 8.1, and AIPW (OLS/logit) with 10.1. These methods all use logit models to estimate probability of treatment, and these high error rates are likely due to extreme values of these estimates.  

<!-- Overall, while the double robust methods of AIPW and TMLE achieve slightly lower RMSE than the more traditional, differences are quite small. OLS, PSM, and the Lin estimator all achieve fairly low RMSE and are computed in less than a second. Compare this to the lowest-RMSE method AIPW with SuperLearner, which takes an average of 130 seconds per simulation. -->

Overall, traditional methods perform surprisingly well in comparison with the double robust methods, and flexible single robust methods may be as effective as double robust methods. Even in the full range of datasets -- which include highly nonlinear exposure and outcome data-generating processes -- OLS, propensity score matching, and the Lin estimator obtain some of the smallest bias and RMSE. While double robust methods achieve the lowest RMSE, the choice of underlying estimator appears more important than the choice of method. AIPW and TMLE both do well with a flexible underlying estimator, while DML does worse than OLS. Of the estimators considered, the SuperLearner (which considers GLM, glmnet, and XGBoost models) appears to be best for the double robust methods, with GRF following closely. G-computation does only a hair worse than these two double robust methods without explicitly accounting for regularization bias. Notably, the method with the longest computation time -- DML with a SuperLearner -- takes nearly 2,000 times as long as OLS (an average of 129 seconds per simulation compared to 0.061 seconds).  

```{r dorie-results-bias,  fig.cap = 'Bias of Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each.'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

sim_results %>% 
  filter(set == 'main') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'Main datasets') +
  scale_y_continuous(trans=ggallin::pseudolog10_trans, breaks = c(-30, -20, -10, -1, 0, 1, 10, 20, 30)) +
  # scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2),
        legend.position = 'none') 
  
ggsave(here('draft/figures', 'dorie-results-bias.png'), width = 7,  height = 5.5, dpi = 600)
```

```{r dorie-results-rmse, fig.cap = 'Root mean squared error and bias for Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each. Values greater in absolute value than 4 are plotted at 4 and labeled with their actual value.'}

sim_results_plot <- sim_results %>%
  filter(set == 'main') %>%
  group_by(method_estimator) %>%
  summarize(bias = mean(ate - truth, na.rm = T),
            rmse = sqrt(mean((ate - truth)^2, na.rm = T))
              ) %>%
  arrange(rmse) %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  pivot_longer(c(bias, rmse)) %>%
    mutate(label = ifelse(abs(value) > 4, round(value), NA),
         value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

sim_results_plot %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point() +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', y = '', title = 'Main datasets') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))

ggsave(here('draft/figures', 'dorie-results-rmse.png'), width = 7,  height = 5.5, dpi = 600)
```

## Linear DGPs

Due to their functional form assumptions, traditional methods may perform better when the data generating processes are linear. To test this, I use 100 simulations of each of the two datasets from @dorie_2019_automated with linear data generating processes for both exposure and outcome (numbers 1 and 3). In these two datasets, the average true treatment effect is `r sim_results %>% filter(set == 'linear') %>% summarize(mean(truth))` with a standard deviation of `r sim_results %>% filter(set == 'linear') %>% summarize(sd(truth))`.  

Figure \@ref(fig:dorie-linear-bias) shows the bias from each simulation for each method (table in the Online Appendix). Similarly to the full set of simulations, bias is fairly low for most methods. IPW (logit) and AIPW (OLS/logit) again suffer from greater bias than other methods, though not to quite as extent as in the full range of simulations. Unsurprisingly, methods that assume linearity -- OLS, PSM, the Lin model -- achieve low bias and variance. IPW does less well than expected, even when treatment assignment is modeled with flexible GRF and SuperLearner.    

Figure \@ref(fig:dorie-linear-rmse) orders the methods by RMSE and presents both RMSE and bias. The methods achieving the lowest RMSE are again three of the SuperLearner methods (TMLE, AIPW, and G-computation) followed by the Lin estimator, DML (OLS/logit), and OLS. The only methods with RMSE above 1 are IPW (logit) and AIPW (OLS/logit), again likely due to unstable logit predictions.  

With these linear DGPs, there seems little reason to sacrifice computational efficiency for a very slight reduction in RMSE. The Lin estimator has an RMSE of 0.28 compared to the lowest-RMSE method, TMLE (SuperLearner), of 0.25, and computes in 0.15 seconds compared to the latter's 126 seconds. Even standard OLS has quite a low RMSE of 0.39. While DML performs better with the linear DGPs than the full range of simulations, it still obtains higher RMSE than at least certain AIPW and TMLE variants. Finally, G-computation again shows its strength, outperforming most other methods when it is estimated using a SuperLearner.




```{r dorie-linear-bias,  fig.cap = 'Bias of Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear").'}
sim_results %>% 
  filter(set == 'linear') %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = bias)) +
  geom_jitter(alpha = .2, width = .3, aes(color = method)) +
  geom_boxplot(alpha = .5, outlier.alpha = 0) +
  geom_hline(yintercept = 0) +
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', title = 'Linear DGPs') +
  scale_y_continuous(trans=ggallin::pseudolog10_trans, breaks = c(-20, -10, -5, -1, 0, 1, 5)) +
  # scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2),
        legend.position = 'none') 

ggsave(here('draft/figures', 'dorie-linear-bias.png'), width = 7,  height = 5.5, dpi = 600)
```


```{r dorie-linear-rmse,  fig.cap = 'Root mean squared error and bias for Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear"). Values greater in absolute value than 4 are plotted at 4 and labeled with their actual value.'}

sim_results_plot <- sim_results %>%
  filter(set == 'linear') %>%
  group_by(method_estimator) %>%
  summarize(bias = mean(ate - truth, na.rm = T),
            rmse = sqrt(mean((ate - truth)^2, na.rm = T))
              ) %>%
  arrange(rmse) %>%
  mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
  pivot_longer(c(bias, rmse)) %>%
  mutate(label = ifelse(abs(value) > 4, round(value), NA),
         value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

sim_results_plot %>%
  #filter(method %in% c('ols', 'psm', 'aipw')) %>%
  ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
  geom_point() +
  # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
  geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
  geom_hline(yintercept = 0) + 
  #facet_wrap(~method, scales = 'free_x') +
  labs(x = '', y = '', title = 'Linear DGPs') +
  theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
  #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
  #scale_y_continuous(trans=ggallin::pseudolog10_trans)
  ylim(c(-4, 4))

ggsave(here('draft/figures', 'dorie-linear-rmse.png'), width = 7,  height = 5.5, dpi = 600)
```

## Do results vary by DGP?
```{r vary-old, eval = F}
sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number()))

dgp_names <- c('Treatment assignment', 'Probability of treatment',
               'Overlap of treatment', 'Response surface',
               'Alignment (same terms for treatment and response)',
               'Treatment effect heterogeneity')

names(dgp_names) <- names(aciccomp2016::parameters_2016)

for(name in colnames(aciccomp2016::parameters_2016[1])){
  plot <- sim_results_dgp %>% 
    mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
    mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
    #filter(method %in% c('ols', 'psm', 'aipw')) %>%
    ggplot(aes(x = method_estimator, y = bias)) +
    geom_jitter(alpha = .2, width = .3, aes(color = method)) +
    geom_boxplot(alpha = .5, outlier.alpha = 0) +
    #facet_wrap(~method, scales = 'free_x') +
    labs(x = '', title = dgp_names[name]) +
    theme(#axis.text.x=element_text(angle=-15), 
          legend.position = 'none') +
    scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
    facet_wrap(vars(!!sym(name)), ncol = 1, scales = 'free') 
  
  print(plot)
}



```

```{r vary-fig, eval = F}
sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number()))

dgp_names <- c('Treatment assignment', 'Probability of treatment',
               'Overlap of treatment', 'Response surface',
               'Alignment (same terms for treatment and response)',
               'Treatment effect heterogeneity')

names(dgp_names) <- names(aciccomp2016::parameters_2016)

for(name in colnames(aciccomp2016::parameters_2016[1])){
  
  sim_results_plot <- sim_results_dgp %>%
    group_by(!!sym(name), method_estimator) %>%
    summarize(bias = round(mean(ate - truth, na.rm = T), 3),
              rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3)
                ) %>%
    arrange(rmse) %>%
    mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
    mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) %>%
    pivot_longer(c(bias, rmse)) %>%
    mutate(label = ifelse(abs(value) > 4, round(value), NA),
           value = ifelse(abs(value) > 4, 4*value/abs(value), value)) 

  sim_results_plot %>%
    #filter(method %in% c('ols', 'psm', 'aipw')) %>%
    ggplot(aes(x = method_estimator, y = value, color = name, shape = name)) +
    geom_point() +
    # ggrepel::geom_text_repel(aes(label = label), show.legend = F) +
    geom_text(aes(label = label), nudge_y = ifelse(sim_results_plot$label > 0, -.5, .5), show.legend = F) +
    geom_hline(yintercept = 0) + 
    #facet_wrap(~method, scales = 'free_x') +
    labs(x = '', y = '', title = dgp_names[name]) +
    theme(axis.text.x=element_text(angle=90, hjust=0.95,vjust=0.2)) +
    #scale_x_discrete(labels=function(x){sub(", ", "\n", x)}) +
    #scale_y_continuous(trans=ggallin::pseudolog10_trans)
    ylim(c(-4, 4)) +
    facet_wrap(vars(!!sym(name)), ncol = 1, scales = 'free_x') %>%
    print()
  
}



```

```{r dgp}
dgp_names <- c('Treat. assign.', 'Prob. of treat.',
               'Overlap', 'Response surface',
               'Alignment',
               'Treat. heterogeneity')

rename_vec <- names(aciccomp2016::parameters_2016)
names(rename_vec) <- dgp_names

sim_results_dgp <- sim_results %>%
  filter(set == 'main') %>%
  # mutate(paramter_num = ceiling(dataset/10)) %>%
  left_join(mutate(aciccomp2016::parameters_2016, dorie_dataset = row_number())) %>%
  rename(rename_vec) %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator}))) 


dgp_list <- list()

for(name in dgp_names){
  
  dgp_list[[name]] <- sim_results_dgp %>%
    group_by(!!sym(name), method_estimator) %>%
    summarize(rmse = sqrt(mean((ate - truth)^2, na.rm = T))
                ) %>%
    ungroup() %>%
    group_by(!!sym(name)) %>%
    slice_min(rmse, n = 6) %>%
    mutate(rank = c('Lowest RMSE', 'Second-lowest', 'Third-lowest', 
                    'Fourth-lowest', 'Fifth-lowest', 'Sixth-lowest'),
           `DGP parameter` = name) %>%
    ungroup() %>%
    mutate(rmse = round(rmse, 3),
          value = paste(method_estimator, rmse, sep = ': '),
          dgp_value = as.character(!!sym(name))) %>%
    select(`DGP parameter`, dgp_value, value, rank) %>%
    pivot_wider(names_from = rank) 
  
}

bind_rows(dgp_list) %>%
  select(-c(`Fourth-lowest`:`Sixth-lowest`)) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Data generating process: Three lowest RMSE methods by DGP for Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each.')
```

```{r dgp-2}
bind_rows(dgp_list) %>%
  select(-c(`Lowest RMSE`:`Third-lowest`)) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Data generating process: Fourth- to sixth-lowest RMSE methods by DGP for Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each.')

```


While the AIPW, TMLE, and G-computation with a SuperLearner may be the top performing methods overall, this does not mean that there are some data-generating processes (DGPs) where some other method may do better. Across the the 20 datasets, @dorie_2019_automated vary five DGP characteristics: degree of nonlinearity, the percentage treated, overlap for the treatment group, alignment (correspondence in variables used to generate the exposure and response models), and treatment effect heterogeneity. To test how the methods perform across different values of these characteristics, I begin with the 200 simulations from the 20 datasets used in the main results (Figures \@ref(fig:dorie-results-bias) and \@ref(fig:dorie-results-rmse)). I limit datasets to those generated by a particular value of a DGP characteristic, and I then calculate RMSE for each method for only these datasets. For example, alignment is 0 for datasets 8 and 16 (meaning there is 0 correlation between the terms included in the treatment and outcome models), so for this value of the DGP RMSE is calculated only for those two datasets.  

Tables \@ref(tab:dgp) and \@ref(tab:dgp-2) shows the six top-performing methods by lowest RMSE for each each value of the DGP characteristics. Across DGPs, the same methods dominate as in the full range of simulations: the SuperLearner with AIPW, TMLE, and G-computation. In fact, these three methods take the top three spots for *every* DGP variation in the simulations.  

Table \@ref(tab:dgp-2) shows the fourth- to sixth-lowest RMSE methods for each DGP variation. GRF with AIPW, G-computation, and TMLE take most of these spots, but the Lin estimator and OLS appear a few times.  

Overall, there is little variation across different types of DGPs in which method performs the best. Flexible estimators take the top spots (though notably not those used with DML), and traditional methods do fairly well across the board. 


## Do results vary by sample size?

In the above simulations, double robust methods have only slightly outperformed traditional methods. Is the issue with previous results simply that the sample size of the simulation data ($n = 4,802$) is too small for double robust methods to seriously outperform traditional methods? The double robust methods reviewed here have been shown to have lower bias asymptotically, so perhaps their superiority to traditional or single robust methods will be starker in larger samples. To test this, this section uses simulated datasets of varying sizes, from 150 to 96,040 (20 times the original sample size). These datasets are also derived from the @dorie_2019_automated `aciccomp2016` package, using parameter set 7, a fairly nonlinear DGP with high heterogeneity. For sample sizes less than 4,802, the sample is randomly drawn from a randomly generated 4,802-unit sample. For sample sizes greater than 4,802, the design matrix (but not the outcome variable) is duplicated, then fed into the `dgp_2016` function. This preserves covariate distributions but retains stochasticity in the outcome.  

Table \@ref(tab:dorie-size) presents the four methods with the lowest RMSE for each sample size (the Online Appendix presents a table with full results). Again, AIPW, TMLE, and G-computation methods that incorporate the SuperLearner or GRF dominate. There are two exceptions: In the tiny sample of 150, IPW (SuperLearner) and IPW (GRF) figure into the best four methods. PSM is the best-performing method in samples of 1200, but it fails to estimate 17 of the 20 datasets. (With 58 covariates, some of the methods fail in smaller samples.)  

Figure \@ref(fig:dorie-size-fig) presents RMSE for all sample sizes for all methods. In the smallest samples, GRF and SuperLearner are able to provide estimates, while traditional methods fail. Beginning at 1,200 observations, all methods are able to provide estimates for all datasets, with the exception of PSM, which fails to calculate even in some large samples. For most methods, RMSE decreases nearly monotonically as sample size grows. Two exceptions are IPW (logit) and AIPW (OLS/logit), whose error is highest in the maximum sample of 96,040, likely due to extreme logit estimates. In addition, the rank order of methods is nearly constant across sample sizes. DML does not achieve lower RMSE than OLS, PSM, or the Lin estimator in most sample sizes, and methods using the SuperLearner or GRF perform the best across sample sizes.   

In sum, although methods incorporating SuperLearner or GRF do the best in most sample sizes, traditional methods still perform fairly well, achieving low RMSE once the sample size is large enough for them to stably compute.  




```{r dorie-size-fig-old, fig.height = 8, eval = F, fig.cap = 'RMSE of Monte Carlo simulations using dataset 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each'}
addline_format <- function(x,...){
    gsub('\\s','\n',x)
}

# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse)) +
#   geom_point() +
#   facet_wrap(~method_estimator, ncol = 1, scales = 'free')


# # text labels
# sim_results %>%
#   filter(set %in% c('small', 'large')) %>% 
#   group_by(method, estimator, method_estimator, size) %>%
#   summarize(bias = round(mean(ate - truth, na.rm = T), 3),
#             percent_bias = bias/sd(ate, na.rm = T),
#             rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
#             mae = median(abs(ate - truth), na.rm = T),
#             comp_time = sum(comp_time)/n(),
#             fail_count = n() - sum(!is.na(ate))
#               ) %>%
#   mutate(estimator = if_else(is.na(estimator), 'NA', estimator)) %>%
#   mutate(method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          size = factor(size, levels = sort(unique({.$size})))) %>%
#   ggplot(aes(x = size, y = rmse, color = estimator, label= method)) +
#   geom_text() +
#   scale_y_continuous(trans='log10')


size_plot <- sim_results %>%
  filter(set %in% c('small', 'large')) %>%
  group_by(method, estimator, method_estimator, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = median(comp_time),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  group_by(method_estimator) %>%
  mutate(min_size = min(size*bias/bias, na.rm = T)) %>%
  ungroup() %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
         label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
         label_max = if_else(size == max(as.numeric(size)), 
                             as.character(method_estimator), NA_character_),
         size = factor(size, levels = sort(unique({.$size})))) 

text_size = 3

(size_plot %>%
  filter(method %in% c('ols', 'psm', 'ipw', 'g-comp', 'lin')) %>%
  ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
  geom_line() +
  ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  scale_y_continuous(trans='log10') +
  theme(legend.position = 'none')) /
  (size_plot %>%
    filter(method %in% c('aipw', 'tmle', 'dml')) %>%
    ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
    geom_line() +
    ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                             size = text_size, segment.color = 'transparent') +
    ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                             size = text_size, segment.color = 'transparent') +
    scale_y_continuous(trans='log10') +
    theme(legend.position = 'none'))
```


```{r dorie-size}
# 
# 
# size_plot <- sim_results %>%
#   filter(set %in% c('small', 'large')) %>%
#   group_by(method, estimator, method_estimator, size) %>%
#   summarize(rmse = sqrt(mean((ate - truth)^2, na.rm = T))) %>%
#   group_by(method_estimator) %>%
#   mutate(min_size = min(size*rmse/rmse, na.rm = T)) %>%
#   ungroup() %>%
#   mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
#          method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
#   mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
#          label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
#          label_max = if_else(size == max(as.numeric(size)), 
#                              as.character(method_estimator), NA_character_),
#          size = factor(size, levels = sort(unique({.$size})))) 

size_plot <- sim_results %>%
  filter(set %in% c('small', 'large')) %>%
  group_by(method, estimator, method_estimator, size) %>%
  summarize(bias = round(mean(ate - truth, na.rm = T), 3),
            percent_bias = bias/sd(ate, na.rm = T),
            rmse = round(sqrt(mean((ate - truth)^2, na.rm = T)), 3),
            mae = median(abs(ate - truth), na.rm = T),
            comp_time = median(comp_time),
            fail_count = n() - sum(!is.na(ate))
              ) %>%
  group_by(method_estimator) %>%
  mutate(min_size = min(size*bias/bias, na.rm = T)) %>%
  ungroup() %>%
  mutate(estimator = if_else(is.na(estimator), 'NA', estimator),
         method_estimator = str_replace(method_estimator, 'superlearner', 'superl.')) %>%
  mutate(method_estimator = factor(method_estimator, levels = unique({.$method_estimator})),
         label_min = if_else(size == min_size, as.character(method_estimator), NA_character_),
         label_max = if_else(size == max(as.numeric(size)), 
                             as.character(method_estimator), NA_character_),
         size = factor(size, levels = sort(unique({.$size})))) 

size_plot %>%
  group_by(size) %>%
  slice_min(rmse, n = 4) %>% 
  # mutate(rank = paste('Rank', row_number())) %>%
  mutate(rank = c('Lowest', 'Second-lowest', 'Third-lowest', 'Fourth-lowest')) %>%
  ungroup() %>%
  mutate(rmse = round(rmse, 3),
         value = paste(method_estimator, rmse, sep = ': ')) %>%
  select(size, value, rank) %>%
  pivot_wider(names_from = rank) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Sample size: Four lowest RMSE methods by sample size for Monte Carlo simulations using dataset 7 from Dorie et al. (2019), 20 replications each')
  

```

```{r dorie-size-fig, fig.height = 8, fig.cap = 'RMSE of Monte Carlo simulations using dataset 7 from Dorie et al. (2019) with varying sample sizes, 20 replications each'}




text_size = 2.5

# best_methods <- size_plot %>%
#   group_by(size) %>%
#   slice_min(rmse, n= 5) %>%
#   pull(method_estimator) %>%
#   unique()

best_methods <- size_plot %>%
  filter(size %in% c(300, 96040)) %>%
  group_by(size) %>%
  slice_min(rmse, n= 10) %>%
  pull(method_estimator) %>%
  unique()

size_plot %>%
  # filter(method_estimator %in% best_methods) %>%
  ggplot(aes(x = size, y = rmse, color = method, linetype = estimator, group = method_estimator)) +
  geom_line() +
  ggrepel::geom_text_repel(aes(label = label_min), nudge_x = -1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  ggrepel::geom_text_repel(aes(label = label_max), nudge_x = 1, na.rm = T, 
                           size = text_size, segment.color = 'transparent') +
  scale_y_continuous(trans='log10', breaks = c(0.1, 0.5, 1, 10, 100)) +
  theme(legend.position = 'none') +
  labs(x = 'Size (n)')
```




# Replications

```{r aksoy, eval = F}
aksoy <- read_dta(here('replication', 'aksoy.dta')) %>%
  mutate(cid_num = as.integer(factor(cid))) %>%
  as_factor()

# first three models from table 2

m1 <- lm(ISL ~ `_Y28` + `_Y32` + `_Y42` + `_Y46` + `_Y50` + `_Y54` + `_Y57` + `_Y62` + 
     `_Y66` + `_Y70` + `_Y71` + dlen + cid,
   data = filter(aksoy, electyr == T)) # %>%
  # tidy() %>%
  # filter(!str_detect(term, 'cid'), !str_detect(term, '_Y'))

m2 <- lm(ISL ~ `_Y28` + `_Y32` + `_Y42` + `_Y46` + `_Y50` + `_Y54` + `_Y57` + `_Y62` + 
     `_Y66` + `_Y70` + `_Y71` + dlen + cid + gdp_g + t + m + pop,
   data = aksoy) 
  # tidy() %>%
  # filter(!str_detect(term, 'cid'), !str_detect(term, '_Y'))

m3 <- lm(ISL ~ `_Y28` + `_Y32` + `_Y42` + `_Y46` + `_Y50` + `_Y54` + `_Y57` + `_Y62` + 
     `_Y66` + `_Y70` + `_Y71` + dlen + cid + gdp_g + t + m + pop + l_ISL,
   data = aksoy) 
  # tidy() %>%
  # filter(!str_detect(term, 'cid'), !str_detect(term, '_Y'))

aksoy_drop_na_y <- filter(aksoy, !is.na(ISL))





m1_grf_dml <- dml_grf(outcome = 'ISL', 
        treatment = 'dlen', 
        covariates = c('year'), 
        clustervar = 'cid_num',
        dataset = aksoy,
        target = 'all')


m2_grf_dml <- dml_grf(outcome = 'ISL', 
        treatment = 'dlen', 
        covariates = c('year', 'gdp_g', 't', 'm', 'pop'), 
        clustervar = 'cid_num',
        dataset = aksoy,
        target = 'all')

m3_grf_dml <- dml_grf(outcome = 'ISL', 
        treatment = 'dlen', 
        covariates = c('year', 'gdp_g', 't', 'm', 'pop', 'l_ISL'), 
        clustervar = 'cid_num',
        dataset = aksoy,
        target = 'all')



m1 <- data.frame(estimate = 7.159, std.error = 2.539)
m2 <- data.frame(estimate = 7.349, std.error = 2.491)
m3 <- data.frame(estimate = 5.317, std.error = 1.855)

compare_tbl(list(append(list(original = m1), m1_grf_dml), 
                                  append(list(original = m2), m2_grf_dml), 
                                  append(list(original = m3), m3_grf_dml)),
                             tidy = F) %>%
  write_rds(here('replication', 'aksoy_rep.RDS'))


# huxtable::huxreg(compare_tbl(list(append(list(original = m1), m1_grf_dml), 
#                                   append(list(original = m2), m2_grf_dml), 
#                                   append(list(original = m3), m3_grf_dml)),
#                              tidy = F),
#                  statistics = NA) %>%
#   huxtable::insert_row(c('Covariates', 'No', rep('Yes', 2)), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Lagged dependent variable', 'No', 'No', 'Yes'), after = nrow(.) - 1) %>%
#   huxtable::set_caption('Replication of aksoy et al. (2022) Table 2, models for outcome of Islamic Votes: "Effect of Fasting Hours (Daylength) during Ramadan on Various Outcome Variables Based on Regression Models That Include Fixed Effects for Provinces and Election Years"') %>%
#   write_rds(here('replication', 'aksoy_tab.RDS'))
# 
# read_rds(here('replication', 'aksoy_rep.RDS')) %>%
#   huxtable::huxreg()





```


```{r biegert, eval = F}
# Table 3

biegert <- read_dta(here('replication', 'biegert.dta')) %>%
  filter(sampleC == 1) %>%
  mutate(year_num = as.numeric(year),
         allstar_y_num = as.numeric(allstar_y),
         allstar_x_num = as.numeric(allstar_x),
    across(c(allstar_y, allstar_x, year), as.factor),
    player_id_num = as.integer(factor(player_id))) %>%
  as_factor()

m1 <- glm(allstar_y ~ allstar_x, 
          data = filter(biegert, sampleC == 1), family = 'binomial') %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 

m2 <- glm(allstar_y ~ allstar_x + height + pos + age_0 + age_0_2 + Black + time + time_2 + year, 
          data = filter(biegert, sampleC == 1),
          family = binomial(link = 'logit')) %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 

m3 <- glm(allstar_y ~ allstar_x + height + pos + age_0 + age_0_2 + Black + time + time_2 + year +
            pts_sdt_nd + ast_sdt_nd + trb_sdt_nd + min_played_nd +  playoffs_nd + win_nd + bigm_nd, 
          data = filter(biegert, sampleC == 1),
          family = binomial(link = 'logit')) %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 

m4 <- glm(allstar_y ~ allstar_x + height + pos + age_0 + age_0_2 + Black + time + time_2 + year +
            pts_sdt_nd + ast_sdt_nd + trb_sdt_nd + min_played_nd +  playoffs_nd + win_nd + bigm_nd +
            pts_sdt_t + ast_sdt_t + trb_sdt_t, 
          data = filter(biegert, sampleC == 1),
          family = binomial(link = 'logit')) %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 

m5 <- glm(allstar_y ~ allstar_x + height + pos + age_0 + age_0_2 + Black + time + time_2 + year +
            pts_sdt_nd + ast_sdt_nd + trb_sdt_nd + min_played_nd +  playoffs_nd + win_nd + bigm_nd +
            pts_sdt_t + ast_sdt_t + trb_sdt_t +
            min_played_t + playoffs_t + win_t + bigm_t, 
          data = filter(biegert, sampleC == 1),
          family = binomial(link = 'logit')) %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 

m6 <- glm(allstar_y ~ allstar_x + allstar_hi + height + pos + age_0 + age_0_2 + Black + time + time_2 + year +
            pts_sdt_nd + ast_sdt_nd + trb_sdt_nd + min_played_nd +  playoffs_nd + win_nd + bigm_nd +
            pts_sdt_t + ast_sdt_t + trb_sdt_t +
            min_played_t + playoffs_t + win_t + bigm_t +
            pts_sdt_hi + ast_sdt_hi + trb_sdt_hi + min_played_hi + playoffs_hi + win_hi + bigm_hi, 
          data = filter(biegert, sampleC == 1),
          family = binomial(link = 'logit')) %>%
  margins::margins(variables = 'allstar_x', 
                   vcov = sandwich::vcovCL(., cluster = ~player_id)) 





# M1 
set.seed(123)
Y <- as.numeric(biegert[['allstar_y']])
X <- data.frame(var = rep(NA_integer_, length(Y)))
W <- as.numeric(biegert[['allstar_x']])

grf_model1 <- grf::causal_forest(X = X, Y = Y, W = W, seed = 123,
                      clusters = biegert$player_id_num
                      )

grf_out1 <- grf::average_treatment_effect(grf_model1, target.sample = 'all')


dml_out1 <- data.frame(estimate = NA,
                       std.error = NA)
dml_cre_out1 <- dml_out1
dml_hybrid_out1 <- dml_out1

# M2
m2_grf_dml <- dml_grf(outcome = 'allstar_y_num', 
        treatment = 'allstar_x_num', 
        covariates = c('height', 'pos', 'age_0', 'Black', 'time', 'year_num'), 
        clustervar = 'player_id_num',
        dataset = biegert,
        target = 'all')


# M3 
m3_grf_dml <- dml_grf(outcome = 'allstar_y_num', 
        treatment = 'allstar_x_num', 
        covariates = c('height', 'pos', 'age_0', 'Black', 'time', 'year_num',
                       'pts_sdt_nd', 'ast_sdt_nd', 'trb_sdt_nd', 'min_played_nd',  'playoffs_nd', 'win_nd', 'bigm_nd'), 
        clustervar = 'player_id_num',
        dataset = biegert,
        target = 'all')

# M4
m4_grf_dml <- dml_grf(outcome = 'allstar_y_num', 
        treatment = 'allstar_x_num', 
        covariates = c('height', 'pos', 'age_0', 'Black', 'time', 'year_num',
                       'pts_sdt_nd', 'ast_sdt_nd', 'trb_sdt_nd', 'min_played_nd',  'playoffs_nd', 'win_nd', 'bigm_nd',
                       'pts_sdt_t', 'ast_sdt_t', 'trb_sdt_t'), 
        clustervar = 'player_id_num',
        dataset = biegert,
        target = 'all')


# M5
m5_grf_dml <- dml_grf(outcome = 'allstar_y_num', 
        treatment = 'allstar_x_num', 
        covariates = c('height', 'pos', 'age_0', 'Black', 'time', 'year_num',
                       'pts_sdt_nd', 'ast_sdt_nd', 'trb_sdt_nd', 'min_played_nd',  'playoffs_nd', 'win_nd', 'bigm_nd',
                       'pts_sdt_t', 'ast_sdt_t', 'trb_sdt_t',
                       'min_played_t', 'playoffs_t', 'win_t', 'bigm_t'), 
        clustervar = 'player_id_num',
        dataset = biegert,
        target = 'all')



# # M6
# m6_grf_dml <- dml_grf(outcome = 'allstar_y_num', 
#         treatment = 'allstar_x_num', 
#         covariates = c('height', 'pos', 'age_0', 'Black', 'time', 'year_num',
#                        'pts_sdt_nd', 'ast_sdt_nd', 'trb_sdt_nd', 'min_played_nd',  'playoffs_nd', 'win_nd', 'bigm_nd',
#                        'pts_sdt_t', 'ast_sdt_t', 'trb_sdt_t',
#                        'min_played_t', 'playoffs_t', 'win_t', 'bigm_t',
#                        'pts_sdt_hi', 'ast_sdt_hi', 'trb_sdt_hi', 'min_played_hi', 'playoffs_hi', 'win_hi', 'bigm_hi'), 
#         clustervar = 'player_id_num',
#         dataset = biegert,
#         target = 'all')

out_empty <- grf_out1
out_empty$estimate <- NA
out_empty$std.err <- 

compare_tbl(model_list = list(
                 list(original = m1, grf = grf_out1, dml = dml_out1, dml_cre = dml_cre_out1, dml_wg_cre = dml_hybrid_out1), 
                 append(list(original = m2), m2_grf_dml), 
                 append(list(original = m3), m3_grf_dml),
                 append(list(original = m4), m4_grf_dml),
                 append(list(original = m5), m5_grf_dml),
            treatment = 'allstar_x1',
            tidy = T) %>%
  write_rds(here('replication', 'biegert_rep.RDS'))


# Final table
# huxtable::huxreg(compare_tbl(list(list(m1, grf_out1, dml_out1),
#                                   list(m2, grf_out2, dml_out2),
#                                   list(m3, grf_out3, dml_out3),
#                                   list(m4, grf_out4, dml_out4),
#                                   list(m5, grf_out5, dml_out5),
#                                   list(m6, grf_out6, dml_out6)),
#                              treatment = 'allstar_x1'),
#                  statistics = NA) %>%
#   huxtable::insert_row(c('Baseline confounders', 'No', rep('Yes', 5)), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Prior situation + performance', rep('No', 2), rep('Yes', 4)), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Current performance', rep('No', 3), rep('Yes', 3)), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Current situation', rep('No', 4), rep('Yes', 2)), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Cumul. AS + cumul. mediators', rep('No', 5), 'Yes'), after = nrow(.) - 1) %>%
#   huxtable::set_caption('Replication of Biegert et al. (2023) Table 3: "Average Marginal Effects from Logistic Regression Models of All-Star Nomination"') %>%
#   write_rds(here('replication', 'biegert_tab.RDS'))
# 
# read_rds(here('replication', 'biegert_tab.RDS'))

```



```{r nussio-ind, eval = F}
nussio_ind <- read_dta(here('replication/nussio_2024', 'Mexico City Individual data.dta')) %>%
  filter(if_all(c(PARP, lognombre), ~!is.na(.x))) %>%
  mutate(across(c(PARP, IT1, ED, BASURA), as.numeric),
         cve_col_num = as.numeric(factor(cve_col))) %>%
  as_factor()





## Table 2 #### (individual analysis)

m1 <- lm(PARP ~ lognombre + IT1, 
         nussio_ind) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~cve_col))

m2 <- lm(PARP ~ lognombre + IT1 + 
           ED + age + female + INGi + unemployed, 
         nussio_ind) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~cve_col))

m3 <- lm(PARP ~ lognombre + IT1 + 
           ED + age + female + INGi + unemployed +
           HER + catholic + nonreligious + working + BASURA + RESS + 
           PELEA  + PADRES + index_trustgov, 
         nussio_ind) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~cve_col))

m4 <- lm(PARP ~ lognombre + IT1 + 
           ED + age + female + INGi + unemployed +
           HER + catholic + nonreligious + working + BASURA + RESS + 
           PELEA  + PADRES + index_trustgov +
           col, nussio_ind) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~cve_col)) 
  # tidy() %>%
  # filter(!(str_detect(term, 'col')))


# M1 
m1_grf_dml <- dml_grf(outcome = 'PARP', 
        treatment = 'lognombre', 
        covariates = c('IT1'), 
        clustervar = 'cve_col_num',
        dataset = nussio_ind,
        target = 'all')

# M2 
m2_grf_dml <- dml_grf(outcome = 'PARP', 
        treatment = 'lognombre', 
        covariates = c('IT1', 'ED', 'age', 'female', 'INGi', 'unemployed'), 
        clustervar = 'cve_col_num',
        dataset = nussio_ind,
        target = 'all')

# M3 & M4 (M4 has fixed effects)
m3_grf_dml <- dml_grf(outcome = 'PARP', 
        treatment = 'lognombre', 
        covariates = c('IT1', 'ED', 'age', 'female', 'INGi', 'unemployed',
                       'HER', 'catholic', 'nonreligious', 'working', 'BASURA', 'RESS', 
                       'PELEA' , 'PADRES', 'index_trustgov'), 
        clustervar = 'cve_col_num',
        dataset = nussio_ind,
        target = 'all')

m4_grf_dml <- m3_grf_dml


compare_tbl(model_list = list(append(list(original = m1), m1_grf_dml),
                              append(list(original = m2), m2_grf_dml), 
                              append(list(original = m3), m3_grf_dml),
                              append(list(original = m4), m4_grf_dml)),
            treatment = 'lognombre',
            tidy = T) %>%
  write_rds(here('replication', 'nussio_ind_rep.RDS'))


# Final table
# huxtable::huxreg(compare_tbl(list(list(m1, grf_out1, dml_out1),
#                                   list(m2, grf_out2, dml_out2),
#                                   list(m3, grf_out3, dml_out3),
#                                   list(m4, grf_out4, dml_out4)),
#                              treatment = 'lognombre'),
#                  statistics = NA) %>%
#   huxtable::insert_row(c('Colonia FE', 'No', 'No', 'No', 'Yes'), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Control variables', 'No', 'Some', 'All', 'All'), after = nrow(.) - 1) %>%
#   huxtable::set_caption('Replication of Nussio (2024) Table 2: "Individual-Level Analysis: Community Ties and Lynching Participation"') %>%
#   write_rds(here('replication', 'nussio_ind_tab.RDS'))
# 
# read_rds(here('replication', 'nussio_ind_tab.RDS'))
```

```{r nussio-ag, eval = F}
## Table 4 #### (aggregate analysis)
nussio_ag <- read_dta(here('replication/nussio_2024', 
                           'Mexico Municipality Cross-Sectional Data.dta')) %>%
  filter(if_all(c(loglynchpermio, organized_lighting_mean), ~!is.na(.x))) %>%
  mutate(estado_num = as.numeric(estado)) %>%
  # mutate(across(c(PARP, IT1, ED, BASURA), as.numeric),
  #        cve_col_num = as.integer(factor(cve_col))) %>%
  as_factor()

nussio_ag_drop_na <- nussio_ag %>% drop_na(loglynchpermio, organized_lighting_mean,
                          trust_neighbors_mean, problem_lighting_mean,
                          c_poptot, area, pconeval_poverty_pobreza_pob, 
                          coneval_gini_coeficientedegini,
                          indigenous, c_poptot_norelpop, homirate, robberyrate,
           victim_household_before_prev_yea, trust_army_mean)

m1 <- lm(loglynchpermio ~ organized_lighting_mean + trust_neighbors_mean + problem_lighting_mean,
         nussio_ag) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~estado))

m2 <- lm(loglynchpermio ~ organized_lighting_mean + trust_neighbors_mean + problem_lighting_mean +
           c_poptot + area + pconeval_poverty_pobreza_pob + coneval_gini_coeficientedegini + 
           indigenous + c_poptot_norelpop + homirate + robberyrate,
         nussio_ag) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~estado))

m3 <- lm(loglynchpermio ~ organized_lighting_mean + trust_neighbors_mean + problem_lighting_mean +
           c_poptot + area + pconeval_poverty_pobreza_pob + coneval_gini_coeficientedegini + 
           indigenous + c_poptot_norelpop + homirate + robberyrate +
           victim_household_before_prev_yea + trust_army_mean,
         nussio_ag) %>%
  lmtest::coeftest(., vcov = sandwich::vcovCL(., ~estado))

m4 <- lm(loglynchpermio ~ organized_lighting_mean + trust_neighbors_mean + problem_lighting_mean +
           c_poptot + area + pconeval_poverty_pobreza_pob + coneval_gini_coeficientedegini + 
           indigenous + c_poptot_norelpop + homirate + robberyrate +
           victim_household_before_prev_yea + trust_army_mean + 
           estado,
         nussio_ag)



# M1 
m1_grf_dml <- dml_grf(outcome = 'loglynchpermio', 
        treatment = 'organized_lighting_mean', 
        covariates = c('trust_neighbors_mean', 'problem_lighting_mean'), 
        clustervar = 'estado_num',
        dataset = nussio_ag,
        target = 'all')


# M2
m2_grf_dml <- dml_grf(outcome = 'loglynchpermio', 
        treatment = 'organized_lighting_mean', 
        covariates = c('trust_neighbors_mean', 'problem_lighting_mean',
                       'c_poptot', 'area', 'pconeval_poverty_pobreza_pob', 'coneval_gini_coeficientedegini', 
           'indigenous', 'c_poptot_norelpop', 'homirate', 'robberyrate'), 
        clustervar = 'estado_num',
        dataset = nussio_ag,
        target = 'all')

# M3
m3_grf_dml <- dml_grf(outcome = 'loglynchpermio', 
        treatment = 'organized_lighting_mean', 
        covariates = c('trust_neighbors_mean', 'problem_lighting_mean',
                       'c_poptot', 'area', 'pconeval_poverty_pobreza_pob', 'coneval_gini_coeficientedegini', 
           'indigenous', 'c_poptot_norelpop', 'homirate', 'robberyrate',
           'victim_household_before_prev_yea', 'trust_army_mean'), 
        clustervar = 'estado_num',
        dataset = nussio_ag,
        target = 'all')

# M4
m4_grf_dml <- m3_grf_dml


compare_tbl(model_list = list(append(list(original = m1), m1_grf_dml),
                              append(list(original = m2), m2_grf_dml), 
                              append(list(original = m3), m3_grf_dml),
                              append(list(original = m4), m4_grf_dml)),
            treatment = 'organized_lighting_mean',
            tidy = T) %>%
  write_rds(here('replication', 'nussio_ag_rep.RDS'))


# huxtable::huxreg(compare_tbl(list(list(m1, grf_out1, dml_out1),
#                                   list(m2, grf_out2, dml_out2),
#                                   list(m3, grf_out3, dml_out3),
#                                   list(m4, grf_out4, dml_out4)),
#                  treatment = 'organized_lighting_mean'),
#                  statistics = NA) %>%
#   huxtable::insert_row(c('Control variables', 'No', 'Some', 'All', 'All'), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Estado FE', 'No', 'No', 'No', 'Yes'), after = nrow(.) - 1) %>%
#   huxtable::insert_row(c('Estado clustered SE', 'Yes', 'Yes', 'Yes', 'No'), after = nrow(.) - 1) %>%
#   huxtable::set_caption('Replication of Nussio (2024) Table 4: "Aggregate-Level Analysis: Community Ties and Lynching Rate"') %>%
#   # huxtable::set_width(1) %>%
#   # huxtable::set_all_padding(0) %>%
#   write_rds(here('replication', 'nussio_ag_tab.RDS'))
# 
# read_rds(here('replication', 'nussio_ag_tab.RDS'))

# names(grf_out1) <- c('estimate', 'std.error')
# tidy(m1) %>%
#   filter(term == 'organized_lighting_mean') %>%
#   bind_rows(mutate(bind_rows(grf_out1))) %>%
#   mutate(term = c('original', 'TMLE (GRF)'), p.value = p_val(estimate, std.error)) %>%
#   huxtable::huxreg()
  
```

```{r nussio-ne, eval = F}
nussio_ne <- read_dta(here('replication/nussio_2024', 
                           'Mexico Municipality Panel Data.dta')) %>%
  # filter(if_all(c(loglynchpermio, organized_lighting_mean), ~!is.na(.x))) %>%
  mutate(cve_num = as.numeric(cve),
         year_num = year,
         year = as.factor(year),
         cve = cve,
         cve = as.factor(cve)) %>%
  # mutate(across(c(PARP, IT1, ED, BASURA), as.numeric),
  #        cve_col_num = as.integer(factor(cve_col))) %>%
  as_factor()

# m1 <- lm(lynch_event ~ eq2017_250X2017 + year + cve, data = nussio_ne) %>%
#   lmtest::coeftest(., vcov = sandwich::vcovCL(., ~cve))

# m1 <- fixest::feols(lynch_event ~ eq2017_250X2017 | year + cve, 
#               data = nussio_ne) %>%
#   summary(vcov = ~cve) %>%
#   tidy()


m1 <- data.frame(estimate = 0.108, std.error = 0.019)
m2 <- data.frame(estimate = 0.113, std.error = 0.021)
m3 <- data.frame(estimate = 0.087, std.error = 0.019)
m4 <- data.frame(estimate = 0.126, std.error = 0.026)
m5 <- data.frame(estimate = -0.010, std.error = 0.002)
m6 <- data.frame(estimate = -0.009, std.error = 0.002)



# M1
m1_grf_dml <- dml_grf(outcome = 'lynch_event', 
        treatment = 'eq2017_250X2017', 
        covariates = c('year_num'), 
        clustervar = 'cve_num',
        dataset = nussio_ne,
        target = 'treated',
        drop_na_grf = T)

# M2
m2_grf_dml <- dml_grf(outcome = 'lynch_event', 
        treatment = 'eq2017_250X2017', 
        covariates = c('year_num', 'homicidefull', 'robofull', 'secuestrofull', 'inegi_infant_mort'), 
        clustervar = 'cve_num',
        dataset = nussio_ne,
        target = 'treated',
        drop_na_grf = T)

# M3
m3_grf_dml <- dml_grf(outcome = 'lynch_event', 
        treatment = 'eq2017_damageX2017', 
        covariates = c('year_num'), 
        clustervar = 'cve_num',
        dataset = nussio_ne,
        target = 'treated',
        drop_na_grf = T)

# M4
m4_grf_dml <- dml_grf(outcome = 'lynch_event', 
        treatment = 'eq2017_damageX2017', 
        covariates = c('year_num', 'homicidefull', 'robofull', 'secuestrofull', 'inegi_infant_mort'), 
        clustervar = 'cve_num',
        dataset = nussio_ne,
        target = 'all',
        drop_na_grf = T)

# # M5
# m5_grf_dml <- dml_grf(outcome = 'lynch_event', 
#         treatment = 'eq2017_dist100X2017', 
#         covariates = c('year_num'), 
#         clustervar = 'cve_num',
#         dataset = nussio_ne,
#         target = 'all',
#         drop_na_grf = T)
# 
# # M6
# m6_grf_dml <- dml_grf(outcome = 'lynch_event', 
#         treatment = 'eq2017_dist100X2017', 
#         covariates = c('year_num', 'homicidefull', 'robofull', 'secuestrofull', 'inegi_infant_mort'), 
#         clustervar = 'cve_num',
#         dataset = nussio_ne,
#         target = 'all',
#         drop_na_grf = T)


compare_tbl(model_list = list(
                 append(list(original = m1), m1_grf_dml),
                 append(list(original = m2), m2_grf_dml), 
                 append(list(original = m3), m3_grf_dml),
                 append(list(original = m4), m4_grf_dml),
            treatment = NULL,
            tidy = F) %>%
  write_rds(here('replication', 'nussio_ne_rep.RDS'))

# Final table
# huxtable::huxreg(compare_tbl(list(list(m1, grf_out1, dml_out1),
#                                   list(m2, grf_out2, dml_out2),
#                                   list(m3, grf_out3, dml_out3),
#                                   list(m4, grf_out4, dml_out4),
#                                   list(m5, grf_out5, dml_out5),
#                                   list(m6, grf_out6, dml_out6)),
#                              tidy = F),
#                  statistics = NA) %>%
#   huxtable::insert_row(c('Control variables', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes'), 
#                        after = nrow(.) - 1) %>%
#   huxtable::set_caption('Replication of Nussio (2024) Table 5: "Natural Experiment: Earthquake Exposure and Lynching"') %>%
#   write_rds(here('replication', 'nussio_ne_tab.RDS'))
# 
# read_rds(here('replication', 'nussio_ne_tab.RDS'))
```





```{r replications, fig.height = 8, fig.cap = 'Replications of ASR papers using double robust machine learning methods. Bars represent 95-percent asymptotic confidence intervals.'}
aksoy_list <- readRDS(here('replication', 'aksoy_rep.RDS')) 
aksoy_list[[1]]$model <- '(1) Base'
aksoy_list[[2]]$model <- '(2) Covariates'
aksoy_list[[3]]$model <- '(3) Lagged dependent variable'

biegert_list <- readRDS(here('replication', 'biegert_rep.RDS')) 
biegert_list[[1]]$model <- '(1) No controls'
biegert_list[[2]]$model <- '(2) Baseline confounders'
biegert_list[[3]]$model <- '(3) Prior situation + performance'
biegert_list[[4]]$model <- '(4) Current performance'
biegert_list[[5]]$model <- '(5) Current situation'

nussio_ind_list <- readRDS(here('replication', 'nussio_ind_rep.RDS')) 
nussio_ind_list[[1]]$model <- '(1) Base'
nussio_ind_list[[2]]$model <- '(2) Some controls'
nussio_ind_list[[3]]$model <- '(3) All controls'
nussio_ind_list[[4]]$model <- '(4) Colonia FE'

nussio_ag_list <- readRDS(here('replication', 'nussio_ag_rep.RDS')) 
nussio_ag_list[[1]]$model <- '(1) Base'
nussio_ag_list[[2]]$model <- '(2) Some controls'
nussio_ag_list[[3]]$model <- '(3) All controls'
nussio_ag_list[[4]]$model <- '(4) Estado FE'

nussio_ne_list <- readRDS(here('replication', 'nussio_ne_rep.RDS')) 
nussio_ne_list[[1]]$model <- '(1) Within 250 km of earthquake'
nussio_ne_list[[2]]$model <- '(2) Within 250 km of earthquake, controls'
nussio_ne_list[[3]]$model <- '(3) Earthquake damage'
nussio_ne_list[[4]]$model <- '(4) Earthquake damage, controls'

bind_rows(
  mutate(bind_rows(aksoy_list), paper = 'A. Aksoy et al. (2022)'),
  mutate(bind_rows(biegert_list), paper = 'B. Biegert et al. (2022)'),
  mutate(bind_rows(nussio_ind_list), paper = 'C. Nussio et al. (2024) Table 2: Individual'),
  mutate(bind_rows(nussio_ag_list), paper = 'D. Nussio et al. (2024) Table 4: Aggregate'),
  mutate(bind_rows(nussio_ne_list), paper = 'E. Nussio et al. (2024) Table 5: Natural Experiment')
  ) %>%
  filter(term %in% c('Original', 'AIPW (GRF)', 'DML (SuperLearner)')) %>%
  mutate(term = factor(term, levels = unique({.$term})),
         paper = factor(paper, levels = unique({paper})),
         model = factor(model, levels = rev(unique({.$model})))) %>%
  ggplot(aes(x = model, y = estimate, color = term, shape = term)) +
  geom_hline(yintercept = 0, color = 'grey') +
  geom_pointrange(position = position_dodge(-0.7),
                  aes(ymin = estimate - 1.96*std.error, ymax = estimate + 1.96*std.error)) + 
  coord_flip() +
  facet_wrap(~paper, scales = 'free', ncol = 1) +
  labs(x = '', y = '') +
  theme(legend.position = 'bottom')
```

These simulations suggest that double robust methods with flexible machine learning algorithms may show some gains in bias reduction compared to traditional methods such as linear OLS regression. However, do these methods make a difference in practice? In this section, I replicate results from three *American Sociological Review* articles using two double robust methods. First, I use AIPW in conjunction with generalized random forests (GRF) using the `grf` package [@tibshirani_2024_grf]. Second, I use a partially linear model for DML with a SuperLearner that, like above, harnesses ensemble learning with GLM, glmnet, and XGBoost, using the `DoubleML` package [@bach_2022_doubleml].^[In the Online Appendix, I present the results of using these packages to estimate the full set of simulations shown above. Results are fairly similar to my own R code, though computation time is notably faster.] Note that AIPW with GRF allows for heterogeneous treatment effects, while this version of DML does not. I use packages rather than the R code presented above for two reasons. First, these packages calculate asymptotic standard errors without the need for bootstrapping, allowing straightforward comparison with the statistical significance of results in the original papers. Second, both packages allow clustering of standard errors, which all three papers employ.  

Two aspects of these replications deserve further explanation. First, it is common on social science to drop observations with missing data for some covariates, a technique called the "complete case method" or "listwise deletion." Two of these papers deal with missing data in this way -- by decreasing the sample size. Some machine learning methods, on the other hand, use missing data informatively [@mayer_2020_doubly]; the `grf` package allows covariates (but not the outcome or treatment assignment) to be missing, using missingness itself as a splitting criterion. Hence for data with moderate levels of missingness, I allow `grf` to work with the entire dataset. For `DoubleML`, I use the same samples as in the original papers, in these cases only observations with complete cases for a given regression model. An exception is Table 5 for @nussio_2024_dark, where the number of observations with missing control variables is far larger than the number without missing data; hence I drop incomplete cases for all analyses. I describe the exact procedures for handling missingness in the notes for each table of results in the Online Appendix.  

The second aspect is fixed effects. Fixed effects are commonly used in linear models; beginning with a categorical variable with $k$ group labels, one hot encoding converts this to $k-1$ dichotomous variables, and these are entered into the regression linearly. These are meant to account for between-group differences [@firebaugh_2013_fixed; but see @imai_2019_when]. In machine learning, however, categorical variables cannot be incorporated so easily [@johannemann_2021_sufficient]. Adding a large number of information-poor dichotomous variables to flexible tree methods like GRF and XGBoost does not have a fixed effects interpretation, since these methods can can interact any variable with any other and apply non-linear transformations. Time and other variables with a natural ordering can be entered as a single continuous variable; the model will allow the effect of time to vary nonlinearly, similarly to time fixed effects. But categorical variables without a natural ordering, such as individual or group fixed effects, need special treatment. Recently, @clarke_2024_double have developed an extension to DML that incorporates group-level fixed effects. In the main paper, I do not adjust for individual or group fixed effects in the machine learning replications (even when the original paper's model does), but in the Online Appendix I present two of Clarke and Polselli's approaches to incorporating fixed effects in DML. Results change very little from standard DML.  

Results of the replications are shown in Figure \@ref(fig:replications), with tables of coefficients and explanatory notes in the Online Appendix. For each paper, I replicate the main results. I do not replicate models testing mechanisms, interactions, mediators, or placebos.  

The first paper I replicate is "Commitment through Sacrifice: How Longer Ramadan Fasting Strengthens Religiosity and Political Islam", by @aksoy_2022_commitment. Using the natural variation in daylight from year to year during Ramadan, the authors test the relationship between Ramadan fasting time and the vote share of Islamist political parties in Turkey. The original paper found a coefficient of about 7, implying a half-hour increase in fasting time is associated with a 3.5 percentage point increase in Islamist vote share. As shown in Figure \@ref(fig:replications) Panel A, coefficients using the double robust methods are noticeably smaller (though not significantly different from the original coefficients). In Model 2, which includes covariates, the main coefficient is nonsignificant at the 5-percent level.  

The second paper for replication is "They Can’t All Be Stars: The Matthew Effect, Cumulative Status Bias, and Status Persistence in NBA All-Star Elections" [@biegert_2023_they]. The authors use detailed records on NBA players' performance to study the effect of previous year's All-Star nomination on being re-nominated in the current year. Panel B of Figure \@ref(fig:replications) shows the average marginal effects from the authors' original logistic regressions alongside the double robust replications. Models 1 and 2 do not control for previous performance and find that a previous year's nomination increases the likelihood of being nominated by 45 to 60 percentage points. Model 3 isolates the effect of the previous year's nomination from performance in the previous year; the original paper found a much smaller coefficient once these controls were incorporated, at about 5 percentage points increase, but the double robust methods retain large effect estimate sizes, at 30 and 59 percentage points for AIPW (GRF) and DML (SuperLearner), respectively. Models 4 and 5 control for other current factors, and double robust estimates are somewhat smaller than in Model 3 but still much larger than the original paper's estimate.  

The third and final paper for replication is "The 'Dark Side' of Community Ties: Collective Action and Lynching in Mexico" [@nussio_2024_dark]. The author estimates the effect of community ties in Mexico on individual participation in lynching or lynching rates at aggregate levels. I present his three main analyses as separate panels. In his first analyses, he looks at individual propensity to participate in a lynching, using the log number of names of people a respondent knows in his or her colonia (neighborhood) as the independent variable. In models with and without controls, the authors finds that a one-percent increase in names known is associated with about 0.0003 percentage points greater likelihood of participating in a lynching (a coefficient of about 0.03 for the logged variable). As shown in Panel C of Figure \@ref(fig:replications), For AIPW (GRF), coefficients and statistical significance are very similar to the original OLS coefficients. DML (SuperLearner), however, shows nonsignificant or negative effects.  

Panel D of Figure \@ref(fig:replications) replicates Nussio's [-@nussio_2024_dark] second analysis, which aggregates to the municipal level. Models estimate the effect of neighborly cooperation -- operationalized by the proportion of respondents who think that most neighbors help each other in problems related to electric lighting -- on the log municipal lynching rate. The original estimate was fairly stable across models with controls, at about a one percentage point increase in neighborly cooperation is associated with a two-percent increase the lynching rate [$\exp(2/100) = 1.02$]. This estimate is fairly stable across estimators, though (as shown in the Online Appendix) this is one case where incorporating fixed effects in the DML models reduces coefficient sizes.  

Finally, Panel E of Figure \@ref(fig:replications) replicates Nussio's [-@nussio_2024_dark] natural experiment, where he estimates the effect of the Puebla earthquake on September 19, 2017, on the number of lynching events in a municipality, using panel data from 2000 to 2020. The original models found that municipalities within 250 km of the earthquake or that experienced earthquake damage experienced an increase of about 0.1 in the number of lynchings in years after 2017 (the mean yearly lynchings per municipality is 0.034). While models for AIPW (GRF) are fairly similar to the original OLS estimates, DML (SuperLearner) estimates are double the original estimates. 

These replications show that results from the original OLS or logistic regressions that these papers employed are fairly robust. In most cases, effect sign and statistical significance are consistent across the original and double robust estimators. Notably, effect sizes for @aksoy_2022_commitment in the double robust models are smaller than the OLS estimates, reaching nonsignificance in one case. DML (SuperLearner) estimates for Nussio's [-@nussio_2024_dark] individual analysis change sign and significance, while the AIPW (GRF) estimates are close to the OLS estimates. However, the simulation results suggested that the DML (SuperLearner) estimator has higher RMSE than both OLS and AIPW (GRF) (Figure \@ref(fig:dorie-results-rmse)), so perhaps the original results are reliable.^[The Online Appendix shows that the DoubleML package with the SuperLearner has higher bias and RMSE in the simulations than my own R code.] Overall, these replication results highlight the importance of testing the robustness of estimates to different models, while suggesting that double robust, machine learning methods often do produce radically different results from traditional methods.  


# LaLonde NSW Data

In the Online Appendix, I provide another evaluation of these methods, using data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW), as provided by @dehejia_1999_causal. LaLonde compared experimental estimates to control samples drawn from the Panel Study of Income Dynamics (PSID) and Westat's Matched Current Population Survey-Social Security Administration File (CPS). Following @dehejia_1999_causal, I present results for the original samples analyzed by @lalonde_1986_evaluating, but I also include results using a subsample of the experimental group that has 1974 earnings data available.  

Results are somewhat difficult to interpret. In the original LaLonde data, most methods fail to estimate treatment effects in the observational data with the same sign as the experimental estimate. The exception is the PSID-3 sample, which includes only men who were not working in the spring of 1976 or 1975 and is thus more comparable to the individuals were recruited to the NSW study. When 1974 earnings are included as a covariate, many of the methods provide estimates across samples that are close to the experimental estimates. OLS, PSM, G-computation (SuperLearner), AIPW (SuperLearner), TMLE (SuperLearner), and DML (OLS/logit) appear the most reliable, while IPW (logit), IPW (GRF), IPW (SuperLearner), the Lin estimator, and DML (GRF) prove unstable. It is surprising that the Lin estimator provides unstable results in the NSW data, while it is among the lowest-RMSE methods in the simulation study. This highlights the complexity of doing causal inference with observational data where ignorability may not hold.  

# Conclusion

This paper aims to provide an introduction to and evaluation of double robust methods for covariate adjustment in causal inference. By comparing AIPW, TMLE, and DML to more traditional statistical methods such as OLS and PSM as well as flexible "single robust" methods such as G-computation and the Lin estimator, it allows evaluation of whether these methods are worth the effort and (computational) time for social scientists to adopt them.  

Results are nuanced. In the full range of simulated data, AIPW and TMLE with a SuperLearner or GRF are able to obtain smaller RMSE than OLS or PSM. However these differences are quite small, and G-computation with the same flexible machine learning methods performs just as well as the double robust methods. DML does not perform as well as AIPW or TMLE (however, a version of DML that allows for heterogeneous effects might perform better; see Chernozhukov et al. [-@chernozhukov_2018_double, p. C35]). The Lin estimator performs slightly better than OLS or PSM, without any increase in computation time. Methods relying on logit models to estimate propensity weights have the most error; researchers should use these methods with caution.  

Methods that come out on top in the full range of simulations also tend to do the best regardless of the data generating process, though the Lin estimator and OLS rise in the rankings when the true treatment and outcome models are linear. AIPW, TMLE, and G-computation with a SuperLearner or GRF perform the best, followed closely by the Lin estimator, OLS, and PSM. As sample size varies, the same rank order generally holds, though traditional methods are unable to produce estimates in small samples when there are many covariates, while the double robust methods are able to do so.  

In replications of prominent sociology articles, these methods do sometimes change conclusions. In the examples given in this paper, coefficients shift somewhat and sometimes change sign and significance. DML with a SuperLearner shows results that differ more dramatically from the OLS or logistic regressions that the authors originally used, while AIPW with GRF produces estimates that are, for the most part, fairly close to the original analyses'. This may be related to the higher error shown in the simulations for DML; this method may be somewhat more unstable. Overall, these replications suggest that researchers should test the robustness of their results using a variety of different models and estimators.  

This paper has a number of limitations. First, although it considers some of the most popular double robust, machine learning, traditional, and single robust methods, there are many methods that it could not evaluate, including variations and extensions of the three methods. Second, although the simulations are meant to cover a wide range of data generating processes, they only consider continuous outcomes and binary treatments; simulations with binary or categorical outcomes and continuous or multi-armed treatments may yield different results. Finally, in considering only functional form misspecification, the simulations in this paper do not consider situations where ignorability does not hold. In particular, it does not evaluate situations where causal identification is misspecified [@keil_2018_resolving]. Future research should assess violations of this and other assumptions underlying these methods.    

In conclusion, if researchers want small gains in accuracy, they may opt for AIPW, TMLE, or G-computation with a flexible machine learning algorithm. But these methods are computationally costly, taking as long as two minutes per dataset of 4,802 observations and 52 covariates, while OLS, PSM, and the Lin estimator each take a fraction of a second.^[The `grf` and `DoubleML`  packages are able to achieve faster computation time, at under 10 seconds. See tables of results in the Online Appendix.] While double robust methods are useful for social scientists to understand, in most applications, OLS or PSM provide similar results. However, in certain circumstances these double robust methods may be a better choice. Especially when paired with a highly flexible estimator like a SuperLearner or GRF, these methods may be slightly more accurate, and they can be useful when the number of covariates is high or even exceeds the number of observations. In longitudinal settings with time-varying confounders, they may be more useful [@tran_2019_double]. They can also be useful as a sensitivity check; if researchers obtain similar estimates across traditional and double robust methods, they can be more confident in the reliability of their estimates.  







<!-- \newpage -->

<!-- # (APPENDIX) Appendix {-} -->

<!-- # Appendix -->

<!-- ## DML with heterogeneous treatment effects -->


<!-- $$\psi = \mu_1(\mathbf X_i) - \mu_0(\mathbf X_i) + \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)}  -->
<!-- - \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} - \theta$$ -->


\newpage


# References

<div id="refs"></div>


