---
output:
  # bookdown::word_document2:
    # reference_docx: "word-template.docx"
  bookdown::pdf_document2:
    toc: no
    number_sections: no
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
    latex_engine: xelatex
always_allow_html: true
header-includes:
  #- \usepackage{setspace}\doublespace
  # - \usepackage[nolists, fighead, tabhead]{endfloat}
  # - \usepackage{endnotes}
  # - \let\footnote=\endnote
#- \setlength{\headheight}{14.5pt}
#- \setlength{\headheight}{13.6pt}
# - \usepackage{fancyhdr}
# - \pagestyle{fancy}
# - \lhead{N.I. Hoffmann}
# - \rhead{`r format(Sys.time(), '%B %e, %Y')`}
# - \newcommand{\beginsupplement}{\setcounter{table}{0}  
# \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} 
# \renewcommand{\thefigure}{A\arabic{figure}}}

editor_options: 
  chunk_output_type: console


citeproc: no
fontfamily: mathpazo
#fontsize: 11pt
# geometry: margin=.6in
indent: yes
link-citations: yes
linkcolor: blue
lang: 'en-US'

bibliography: "/Users/nathan/Documents/My Library.bib" 
# bibliography: "My Library.bib"  
csl: apa.csl
# csl: american-sociological-association.csl

title: "Demystifying Double Robust, Flexible Adjustment Methods for Causal Inference"
#subtitle: "PAA 2023"
author:  Nathan I. Hoffmann
#   | Department of Sociology, UCLA
date: "`r format(Sys.time(), '%B %e, %Y')`"

abstract: "Double robust methods for flexible covariate adjustment in causal inference have proliferated in recent years. Despite their apparent advantages, these methods remain underutilized by social scientists. It is also unclear whether these methods actually outperform more traditional methods in finite samples. This paper has two aims: It is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, and it compares these methods to more traditional statistical methods. It does this by using both simulated data where the treatment effect estimate is known, and then using comparisons of experimental and observational data from the National Supported Work Demonstration. Methods covered include Augmented Inverse Propensity Weighting, Targeted Maximum Likelihood Estimation, and Double/Debiased Machine Learning. Results suggest that these methods do not necessarily outperform OLS regression or matching on propensity score estimated by logistic regression, even in cases where the data generating process is not linear."

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = T)
options("yaml.eval.expr" = TRUE)

library(SuperLearner)
library(broom)
library(knitr)
library(here)
library(haven)
library(tidyverse)

knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


options("yaml.eval.expr" = TRUE, scipen = 3, digits = 2)

uclablue = '#2774AE'
gray = '#808080'
black = '#000000'
ucla_palette = c(black, uclablue, gray)

# theme_set(theme_cowplot(font_family = 'Palatino') + 
theme_set(theme_classic(base_family = 'Palatino') + 
      theme(legend.title=element_blank(), 
         panel.grid.major.y = element_line('grey80'),
         legend.background = element_rect(fill = alpha("white", 0.5))
         ))
ggplot <- function(...) ggplot2::ggplot(...) + 
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")

kable <- function(...) knitr::kable(..., format.args = list(big.mark = ","))
```




```{r load}
sims <- readRDS(here('files', 'sims.RDS'))


unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()
```


```{r functions}
## Pred functions ####
ols_logit_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  mu_mod <- lm(y ~  d + ., data.frame(y, d, x))
  mu1_pred <- predict(mu_mod, newdata = data.frame(y, d = 1, x))
  mu0_pred <- predict(mu_mod, newdata = data.frame(y, d = 0, x))
  
  pi_mod <- glm(d ~ ., data.frame(y, x), family = binomial(link = 'logit'))
  pi_pred <- predict(pi_mod, type = 'response')

  
  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}


grf_pred <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  forest_mu <- grf::regression_forest(X = data.frame(d, x), Y = y, 
                                 tune.parameters = "all")
  mu0_pred <- predict(forest_mu, newdata = data.frame(d = 0, x))$predictions
  mu1_pred <- predict(forest_mu, newdata = data.frame(d = 1, x))$predictions
  
  forest_pi <- grf::regression_forest(X = x, Y = d, tune.parameters = "all")
  pi_pred <- predict(forest_pi, newdata = x)$predictions

  return(
    list(
      mu1_pred = mu1_pred, 
      mu0_pred = mu0_pred, 
      pi_pred = pi_pred,
      d = d,
      y = y
    ))
}

superlearner_pred <- function(y, d, x, folds = 5, seed = 158){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  set.seed(seed)
  mu_fit <- SuperLearner(
    Y = y,
    X = data.frame(d, x),
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  pi_fit <- SuperLearner(
    Y = d,
    X = x,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  return(list(
    mu0_pred = predict(mu_fit, newdata = data.frame(d = 0, x), type = 'response')$pred,
    mu1_pred = predict(mu_fit, newdata = data.frame(d = 1, x), type = 'response')$pred,
    pi_pred = predict(pi_fit, type = 'response')$pred,
    d = d,
    y = y))
}

## Methods ####

lm_sim <- function(dat){
  start_time <- Sys.time()
  n <- length(dat)
  lm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    
    tryCatch({
      lm_out <- tidy(lm(y ~ d + ., sim_dat))
      lm_list[[i]] <- data.frame(dataset = i,
                                 ate = lm_out[[2,2]],
                                 #se = lm_out[[2,3]],
                                 truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      }, error=function(e){
        cat("ERROR :",conditionMessage(e), "\n")
        })
    
  }
  
  fail_count <- sum(sapply(lm_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(lm_list) == n, fail_count, fail_count + (n - length(lm_list)))
  
  end_time <- Sys.time()
  
  return(list(
    est_df = bind_rows(lm_list),
    fail_count = fail_count,
    comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
    )
}


psm_sim <- function(dat){
  start_time <- Sys.time()
  n <- length(dat)
  psm_list <- list()
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x)
    
    form <- as.formula(paste0('d ~ ', paste(names(dat[[i]]$x), collapse = '+')))
    match_out <- MatchIt::matchit(form,
                           data = sim_dat,
                           method = 'nearest',
                           distance = 'glm') 
    
    # match_data <- MatchIt::match.data(match_out) 
    # apply(match_data, 1, unique)
    form2 <- as.formula(paste0('y ~ d + ', paste(names(dat[[i]]$x), collapse = '+')))
    
  tryCatch({
    psm_out <- lm(form2, 
                  MatchIt::match.data(match_out), 
                  weights = weights) %>%
      tidy()
    psm_list[[i]] <- data.frame(dataset = i,
                                ate = psm_out[[2,2]],
                                # se = psm_out[[2,3]],
                                truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
    
  }
  
  fail_count <- sum(sapply(psm_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(psm_list) == n, fail_count, fail_count + (n - length(psm_list)))
  
  end_time <- Sys.time()
  
  return(list(
    est_df = bind_rows(psm_list),
    fail_count = fail_count,
    comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}


aipw_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}

aipw_calc_trunc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  pi_pred <- case_when(
    pi_pred < .01 ~ .01,
    pi_pred > .99 ~ .99,
    T ~ pi_pred)
  
  y1_pred <- (d*(y-mu1_pred))/pi_pred + mu1_pred
  y0_pred <- ((1-d)*(y-mu0_pred))/(1-pi_pred) + mu0_pred
  
  ate <- (1/n)*(sum(y1_pred)) - (1/n)*sum(y0_pred)
  
  return(ate)
}


tmle_calc <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(y)
  # H <- (d == 1)/pi_pred - (d==0)/(1-pi_pred)
  H0 = (1-d)/(1-pi_pred)
  H1 = d/pi_pred

  epsilon <- glm(y ~ -1 + H0 + H1 + offset(qlogis((d==1)*mu1_pred + (d==0)*mu0_pred)),
                 family = binomial(link = 'logit')) %>%
    tidy() %>%
    pull(estimate)
  
  H_0 = (1-d)/(1-pi_pred)
  H_1 = d/pi_pred
  
  target_0 <- plogis(qlogis(mu0_pred + epsilon[1]*H_0))
  target_1 <- plogis(qlogis(mu1_pred + epsilon[2]*H_1))
  
  ATE <- mean((target_1 - target_0), na.rm = T)
  return(ATE)
}

dml_pre <- function(y, d, x){
  if('factor' %in% unlist(lapply(x, class))){
    x <- fastDummies::dummy_cols(x, remove_first_dummy = T, remove_selected_columns = T) 
  }
  
  n <- length(y)
  n_2 <- n/2
  n_2_1 = ifelse(round(n_2) == n_2, n_2, round(n_2))
  n_2_2 = ifelse(round(n_2) == n_2, n_2, round(n_2)+1)
  
  # split the sample
  random_vec <- sample(1:n, n)
  I <- random_vec[1:n_2_1]
  I_c <- random_vec[(n_2_1+1):n]
  
  return(list(
    y_I = y[I_c],
    d_I = d[I_c],
    x_I = x[I_c,],
        y_I_c = y[I_c],
    d_I_c = d[I_c],
    x_I_c = x[I_c,]
    ))
}

dml_post <- function(y_I, d_I, x_I = NULL, y_I_c, d_I_c, x_I_c = NULL,
                     mu_pred1, pi_pred1, mu_pred2, pi_pred2){
  
  v1 <- d_I - pi_pred1
  delta1 <- (sum(v1 * d_I))^-1 * sum(v1 * (y_I - pi_pred1))
  
  v2 <- d_I_c - pi_pred2
  delta2 <- (sum(v2 * d_I_c))^-1 * sum(v2 * (y_I_c - pi_pred2))
  
  ate <- (delta1 + delta2)/2
  
  return(ate)
}

## Predictor functions
ols_logit_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- lm(y ~ ., data.frame(y = y_I_c, x_I_c))
  mu_pred1 <- predict(mu_mod1, newdata = data.frame(y = y_I, x_I))

  pi_mod1 <- glm(d ~ ., data.frame(d = d_I_c, x_I_c), 
                family = binomial(link = 'logit'))
  pi_pred1 <- predict(pi_mod1, 
                     newdata = data.frame(d = d_I, x_I), 
                     type = 'response')
  
  mu_mod2 <- lm(y ~ ., data.frame(y = y_I, x_I))
  mu_pred2 <- predict(mu_mod2, newdata = data.frame(y = y_I_c, x_I_c))

  pi_mod2 <- glm(d ~ ., data.frame(d = d_I, x_I), 
                family = binomial(link = 'logit'))
  pi_pred2 <- predict(pi_mod2, 
                     newdata = data.frame(d = d_I_c, x_I_c), 
                     type = 'response')
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

grf_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c){
  mu_mod1 <- grf::regression_forest(X = x_I_c, Y = y_I_c, 
                                       tune.parameters = "all")
  mu1_pred1 <- predict(mu_mod1, newdata = x_I)$predictions
  
  pi_mod1 <- grf::regression_forest(X = x_I_c, Y = d_I_c, tune.parameters = "all")
  pi_pred1 <- predict(pi_mod1, newdata = x_I)$predictions
  
  mu_mod2 <- grf::regression_forest(X = x_I, Y = y_I, 
                                       tune.parameters = "all")
  mu1_pred2 <- predict(mu_mod1, newdata = x_I_c)$predictions
  
  pi_mod2 <- grf::regression_forest(X = x_I, Y = d_I, tune.parameters = "all")
  pi_pred2 <- predict(pi_mod1, newdata = x_I_c)$predictions

  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

superlearner_dml <- function(y_I, d_I, x_I, y_I_c, d_I_c, x_I_c,
                             folds = 5){
  
  
  mu_mod1 <- SuperLearner(
    Y = y_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  pi_mod1 <- SuperLearner(
    Y = d_I_c,
    X = x_I_c,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  mu_pred1 <- predict(mu_mod1, newdata = x_I, type = 'response')$pred
  pi_pred1 <- predict(mu_mod1, newdata = x_I, type = 'response')$pred
  
  mu_mod2 <- SuperLearner(
    Y = y_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  pi_mod2 <- SuperLearner(
    Y = d_I,
    X = x_I,
    cvControl = list(V = folds),
    SL.library=c("SL.glm", 
                 "SL.glmnet", 
                 "SL.xgboost"),
    method="method.CC_nloglik", 
    family="gaussian"
  )
  
  mu_pred2 <- predict(mu_mod2, newdata = x_I_c, type = 'response')$pred
  pi_pred2 <- predict(mu_mod2, newdata = x_I_c, type = 'response')$pred
  
  return(list(
    mu_pred1 = mu_pred1,
    pi_pred1 = pi_pred1,
    mu_pred2 = mu_pred2,
    pi_pred2 = pi_pred2
  ))
}

## Functions using double robust packages ####
aipw_sim <- function(dat, seed = 185){

  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  aipw_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = as.numeric(ifelse(dat[[i]]$z == 'trt', 1, 0)), 
                          dat[[i]]$x)
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, 
                                   W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #                              Y = sim_dat$y, W = sim_dat$d)
      
      aipw_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'AIPW')
      
      aipw_list[[i]] <- data.frame(d = aipw_out[[1]],
                                   se = aipw_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(aipw_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(aipw_list) == n, fail_count, fail_count + (n - length(aipw_list)))
  
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(aipw_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

tmle_sim <- function(dat, seed = 185){
  start_time <- Sys.time()
  
  set.seed(seed)
  n <- length(dat)
  tmle_list <- list()
  # fail_count <- 0
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    tryCatch({
      sim_dat <- sim_dat %>%
        fastDummies::dummy_cols(remove_first_dummy = T, remove_selected_columns = T) 
      }, error=function(e){
      })

    tryCatch({
      forest <- grf::causal_forest(X = select(sim_dat, 3:length(names(sim_dat))), 
                                   Y = sim_dat$y, W = sim_dat$d)
      # forest <- grf::causal_forest(X = select(sim_dat, starts_with('x')), 
      #   Y = sim_dat$y, W = sim_dat$d)
      
      tmle_out <- grf::average_treatment_effect(forest, target.sample = 'treated', method = 'TMLE')
      
      tmle_list[[i]] <- data.frame(d = tmle_out[[1]],
                                   se = tmle_out[[2]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
        
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(tmle_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(tmle_list) == n, fail_count, fail_count + (n - length(tmle_list)))
  
  end_time <- Sys.time()
  
 return(list(
      est_df = bind_rows(tmle_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
  )
}

dml_sim <- function(dat, seed = 185){
  library(mlr3)
  library(mlr3learners)
  
  start_time <- Sys.time()
  
  set.seed(seed)
  
  n <- length(dat)
  
  dml_list <- list()
  
  for(i in 1:n){
    print(paste0(i, ' out of ', n))
    sim_dat <- data.frame(y = dat[[i]]$y, 
                          d = ifelse(dat[[i]]$z == 'trt', 1, 0), 
                          dat[[i]]$x) 
    
    lgr::get_logger("mlr3")$set_threshold("warn")
    
    learner = lrn("regr.ranger", num.trees=500, 
                  max.depth=5, min.node.size=2)
    ml_l = learner$clone()
    ml_m = learner$clone()

    tryCatch({
      dml_out <- DoubleML::DoubleMLPLR$new(
        DoubleML::DoubleMLData$new(sim_dat,
                                 y_col = 'y',
                                 d_cols = 'd',
                                 x_cols = names(dat[[i]]$x)), 
        ml_l=ml_l, ml_m=ml_m)
      
      dml_out$fit()
      dml_list[[i]] <- data.frame(d = dml_out$all_coef[[1,1]],
                                  se = dml_out$all_se[[1,1]],
                               truth = mean(dat[[i]]$y.1) - mean(dat[[i]]$y.0))
      
  }, error=function(e){
    cat("ERROR :",conditionMessage(e), "\n")
    })
  }
  
  fail_count <- sum(sapply(dml_list, function(x) is.null(x)))
  # in case last few are errors
  fail_count <- ifelse(length(dml_list) == n, fail_count, fail_count + (n - length(dml_list)))
  
  end_time <- Sys.time()
  
  return(list(
      est_df = bind_rows(dml_list),
      fail_count = fail_count,
      comp_time = as.numeric(difftime(end_time, start_time, units = 'secs'))/(n-fail_count))
      )
}



## Other functions ####
normalize <- function(x, y){(x - min(y)) / (max(y) - min(y))}
denormalize <- function(x, y){x * (max(y) - min(y))}
perform <- function(est_df, label){
  est_df %>%
    # mutate(d = d/truth,
    #        truth = 1) %>%
    summarize(bias = mean(d - truth),
              percent_bias = bias/sd(d),
              rmse = sqrt(mean((ate - truth)^2)),
              mae = median(abs(d - truth)),
              fail_count = first(fail_count),
              comp_time = first(comp_time)) %>%
    mutate(label = label,
           n = nrow(est_df) + fail_count) %>%
    select(label, everything()) %>%
    return()
}
```


<!-- # Abstract -->

<!-- This thesis aims to compare different advanced adjustment methods. It will compare Imben & Rubins propensity matching, stabilized IPWs, TMLE, double machine learning (Chernozhukov et al 2018), Ratkovic (2021), and BART. -->

# Introduction

Statistical methods for flexible covariate adjustment in causal inference have proliferated in recent years. These methods have a number of strengths over traditional regression methods: They make few functional form assumptions, can accommodate large numbers of covariates, and produce easily interpretable treatment effect estimates. Many of these methods also have a "double robust" property: They estimate one model for the treatment exposure and another for the outcome, and as long as at least one is correctly specified, then the treatment effect will be estimated consistently. Despite their apparent advantages, these methods remain underutilized by social scientists. Part of the barrier has been lack of familiarity with these methods. It has also been unclear how these methods compare, or whether such methods actually perform better than traditional methods in finite samples. 
<!-- And when researchers want to use an existing R package for these methods, the array of possible estimation methods and other options can be dizzying.   -->

This paper makes advances on these fronts. First, it is a guide to some of the latest methods in double robust, flexible covariate adjustment for causal inference, explaining the methods to a social scientist audience. Second, it compares these methods to more traditional statistical methods using both simulations [@dorie_2019_automated] and the National Support for Work Demonstration (NSW) originally analyzed by @lalonde_1986_evaluating.  

Methods covered include Targeted Maximum Likelihood Estimation [TMLE, @vanderlaan_2006_targeted], Double or Debiased Machine Learning [DML, @chernozhukov_2018_double], and Augmented Inverse Propensity Weighting [AIPW, @glynn_2010_introduction]. This paper reviews the theory behind these methods as well as simple R implementations of them on simulations and real data. These methods are compared to two methods commonly used by social scientists: ordinary least squares (OLS) regression, and matching on propensity scores estimated from logistic regression (PSM).  



# Conceptual Overview

Double robust methods estimate two models:  

- an *outcome model*

$$\mu_d(X_i) = E(Y_i \mid D_i = d, X_i)$$

- and an *exposure model* (or treatment model or propensity score):

$$\pi(X_i) = E(D_i \mid X_i)$$

where $\mu_d(\cdot)$ is the model of control or treatment $D_i = d=\{0, 1\}$, $X_i$ is a vector of covariates for unit $i = 1, \ldots, N$ for treatment (1) and control (0), $Y_i$ is the outcome, and $\pi(\cdot)$ is the exposure model. The covariates included in $X_i$ can be different for the two models. 

An estimator is called "double robust" if it achieves consistent estimation of the ATE (or whatever estimand the researcher is interested in) as long as *at least one* of these two models is consistently estimated. This means that the outcome model can be completely misspecified, but as long as the exposure model is correct, our estimation of the ATE will be consistent. This also means that the exposure model can be completely wrong, as along as the outcome model is correct.  


## Origins of Doubly Robust Methods

According to @bang_2005_doubly, double robust methods have their origins in missing data models. @robins_1994_estimation and @rotnitzky_1998_semiparametric developed augmented orthogonal inverse probability-weighted (AIPW) estimators in missing data models, and @scharfstein_1999_adjusting showed that AIPW was double robust and extended to causal inference.

But @kang_2007_demystifying argue that double robust methods are older. They cite work by @cassel_1976_results, who proposed “generalized regression estimators” for population means from surveys where sampling weights must be estimated. Arguably, double robust methods go back even further than this. The form of double robust methods is similar to residual-on-residual regression, which dates back to @frisch_1933_partial famous FWL theorem:

$$\beta_D = \frac{\text{Cov}(\tilde Y_i, \tilde D_i)}{\text{Var}(\tilde D_i)}$$

where $\tilde D_i$ is the residual part of $D_i$ after regressing it on $X_i$, and $\tilde Y_i$ is the residual part of $Y_i$ after regressing it on $X_i$. This formulation writes the regression coefficient as composed of an outcome model ($\tilde Y_i$) and exposure model ($\tilde D_i$), the two models used in double robust estimators.

There are also links between double robust methods and matching with regression adjustment. This work goes back to at least @rubin_1973_use, who suggested that regression adjustment in matched data produces less biased estimates that either matching (exposure adjustment) or regression (outcome adjustment) do by themselves.

## Assumptions

Most double robust methods require almost all of the standard assumptions necessary formost methods that depend on selection on observables. Although some double robust methods relax one or two of these, the six standard assumptions are:  

1. Consistency
2. Positivity/overlap
3. One version of treatment
4. No interference
5. IID observations
6. Conditional ignorability: $\{Y_{i0}, Y_{i1}\} \perp \!\!\! \perp D_i \mid X_i$

Special attention should be paid to Assumption 6: double robust methods will not work if we do not measure an important confounder that affects both treatment and exposure. But notably, the double robust methods covered in this tutorial make no functional form assumptions. These methods are designed to incorporate flexible machine learning algorithms to estimate both the outcome and exposure models, with regularization (often through cross-fitting) to avoid overfitting. 

# Overview of Techniques  
Each of the methods reviewed in this paper can be thought of as a collection of estimation techniques. Each involves a model for the outcome and another for the treatment exposure, but the ways these relate and are combined varies from method to method. Choice of estimation technique for these two models is left to the discretion of the user; often ensemble learning is recommended, but in practice simpler methods can also work well.  

## Augmented Inverse Propensity Weighting (AIPW)

The oldest of these modern methods, AIPW arose in the context of missing data imputation. The method simply combines estimates from a model for the treatment exposure, $\pi(X)$, and a model for the outcome, $\mu(X)$. The name comes from the close similarity to inverse propensity weights (IPW), but whereas IPW only weights for propensity of treatment, AIPW "augments" these weights with an estimate of the response surface as well.  

Formally, the model can be written as the difference between an estimated outcome for treated units and the estimated outcome for untreated units (see the demonstration below):

$$\begin{aligned}
\widehat{ATE}_{AIPW} = &\frac{1}{n} \sum_{i=1}^n \left( \frac{D_i(Y_i - \hat \mu_1 (\mathbf X_i))}{\hat \pi (\mathbf X_i)} + \hat \mu_1(\mathbf X_i) \right) 
- \frac{1}{n} \sum_{i=1}^n \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (\mathbf X_i))}{1-\hat \pi(\mathbf X_i)} + \hat \mu_0(\mathbf X_i) \right)
\end{aligned}$$

@glynn_2010_introduction provide an alternate formula, where the basic inverse probability weight (IPW) estimator (which incorporates only the exposure model $\hat \pi$) is corrected using a weighted average of two outcome regression estimates: 
$$\begin{aligned}
\widehat{ATE}_{AIPW} = & \frac{1}{n} \sum_{i=1}^n \left\{ \left[ \frac{D_i Y_i}{\hat \pi(\mathbf X_i)} - \frac{(1-D_i) Y_i}{ 1- \hat \pi(\mathbf X_i)} \right] - \frac{D_i - \hat \pi (\mathbf X_i)}{\hat \pi (\mathbf X_i)(1 - \hat pi (\mathbf X_i))} [(1- \hat \pi (\mathbf X_i)) \hat \mu_1(\mathbf X_i) + \hat \pi (\mathbf X_i) \hat \mu_0(\mathbf X_i)] \right\}
\end{aligned}$$

## Targeted Maximum Likelihood Estimation (TMLE)

TMLE begins by estimating the relevant part of the data-generating distribution $P(Y)$, i.e. the conditional density $Q = P(Y \mid X)$. It next estimates the exposure model. Although any estimation method can be used for these steps, the originators of the method suggest using a "super learner," i.e. ensemble learning with cross-validation. Next, the exposure model is used to calculate a "clever covariate," which is similar to an IPW. The coefficient for this clever covariate is estimated using maximum likelihood -- whence the "MLE" in "TMLE." Finally, the estimate of $Q$ is updated in a function involving the clever covariate. This process can be iterated, but usually one iteration is enough. The estimate of the distribution $Q$ can be used to calculate the estimand of interest.  

Formulaly, first generate estimates of $\mu_{d}(\mathbf X_i) = E(Y \mid D=d, \mathbf X_i)$ and $\pi(\mathbf X_i) = P(D=1 \mid \mathbf X_i)$. Then create variable for targeting step:
$$H_{di} = \frac{I(D_i = 1)}{\hat \pi (\mathbf X_i)} - \frac{I(D_i=0)}{1 - \hat \pi (\mathbf X_i)}$$

Next, calculate the clever covariates for each individual in the data. These quantities are similar to inverse probability weights:
$$\begin{aligned}
H_{1i}(D=1, \mathbf X_i) &= \frac{d_i}{\hat \pi (\mathbf X_i)}, &
H_{0i}(D=0, \mathbf X_i) = \frac{1-d_i}{1- \hat \pi (\mathbf X_i)}.
\end{aligned}$$

Then estimate fluctuation parameters $\epsilon = (\epsilon_0, \epsilon_1)$ through maximum likelihood of the following logistic regression with fixed intercept $\text{logit}(\mu_{di})$: 
<!-- $$E(Y=1 \mid D, \mathbf X) = \text{logit}(\mu_{di}) + \epsilon H_{di}$$ -->
$$\text{logit}[E(Y=1 \mid D, \mathbf X)] = \text{logit}(\hat \mu_{di}) + \epsilon_0 H_{0i} + \epsilon_1 H_{1i}$$

Then generate updated ("targeted") estimates of potential outcomes:
$$\begin{aligned}
\hat \mu_1^*(\mathbf X_i) &= \text{expit}[\text{logit}(\hat \mu_1(\mathbf X_i)) + \hat \epsilon H_{1i}]\\
\hat \mu_0^*(\mathbf X_i) &= \text{expit}[\text{logit}(\hat \mu_0(\mathbf X_i)) + \hat \epsilon H_{0i}]
\end{aligned}$$

Finally, estimate targeted parameter (ATE):
$$\widehat{ATE}_{TMLE} = \frac{1}{n} \sum_{i=1}^n [\hat \mu_1^*(\mathbf X_i) - \hat \mu_1^*(\mathbf X_i)]$$

## Double or Debiased Machine Learning* (DML)

The most recent of the methods reviewed here, DML is motivated by the need to handle problems with high-dimensional nuisance parameters, i.e. a large number of measured confounders. Flexible machine learning is appropriate for this task, but such methods suffer from regularization bias. DML removes this bias in a two-step procedure. First, it solves the auxiliary problem of estimating the treatment exposure model $E(D|X) =  \pi(X)$. It then uses this model to remove bias: Neyman orthogonalization allows the creation of an orthogonalized regressor, essentially partialing out the effect of covariates $X$ from treatment $D$. The debiased $D$ is then used to estimate the conditional mean of the outcome $E(Y\mid X) = \mu(X)$, which can be used to calculate the estimand of interest.  

We can motivate this as follows. Suppose we want to estimate $\delta$ in the following framework:
$$y_i = \delta d_i + g_0(\mathbf x_i) + u_i,$$
$$d_i = m_0(\mathbf x_i) + v_i.$$

The idea is to estimate $g_0$ and $m_0$ separately, then use residual-on-residual regression to obtain an estimate of $\delta$. However this leaves a term in the asymptotic distribution of $\hat \delta$ that biases the estimate. To avoid this, DML uses sample splitting.  

We randomly split the sample of $n$ observations into two sets, $I$ and $I^c$, each of size $n/2$. We then estimate the response and treatment models using only set $I^c$:  

1) Estimate $d_i = \hat m_0(\mathbf x_i) + \hat v_i, i \in I^c$.  
2) Estimate $y_i =  \hat g_0(\mathbf x_i) + \hat u_i, i \in I^c$, without using $d_i$.  

Next, we use the estimated models to perform residual-on-residual regression *on the left out set* $I$ to obtain an estimate of $\delta$:
$$\hat \delta(I^c, I) = \left(\sum_{i \in I} \hat v_i d_i \right)^{-1} 
\sum_{i \in I} \hat v_i (y_i - \hat g_0 (\mathbf x_i)).$$

Using half the sample results in efficiency loss. To rectify this, we repeat the above procedure, switching the split sets. We then have $\delta(I^c, I)$ and $\delta(I, I^c)$. The cross-fitting DML estimator is:
$$\widehat {ATE}_{DML} = \frac{\hat \delta (I^c, I) + \hat \delta (I, I^c)}{2}.$$



## A simple demonstration using AIPW

To demonstrate double robustness, this section presents one of the simpler double robust estimators, AIPW [@glynn_2010_introduction]. As shown above, we can write this estimator as follows:

$$\begin{aligned}
\widehat{ATE} = &\frac{1}{N} \sum_{i=1}^N \left( \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \right) 
- \frac{1}{N} \sum_{i=1}^N \left( \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i) \right)
\end{aligned}$$

For each individual in the sample, this estimator calculates two quantities:

- The treated potential outcome

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

- The control potential outcome

$$\hat Y_{0i} = \frac{(1-D_i)(Y_i - \hat \mu_0 (X_i))}{1-\hat \pi(X_i)} + \hat \mu_0(X_i)$$

Let's focus on the treated model:

$$\hat Y_{1i} = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i)$$

First, assume that the outcome model $\mu_1(X_i)$ is *correctly* specified and the exposure model $\pi(X_i)$ is *incorreclty* specified. Let's also assume (for now) that we're dealing with a treated unit, i.e. $D_i = 1$. Then

$$\hat \mu_1 (X_i) = Y_i$$

and hence

$$\hat Y_{1i} = \frac{D_i(0)}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

So the model relies *only* on the outcome model. The incorrectly specified exposure model completely disappears from the equation. If we're dealing with a control unit ($D_i=0$), we get the same result:

$$\hat Y_{1i} = \frac{0(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) = \hat \mu_1(X_i).$$

Now, what if the *exposure* model $\pi(X_i)$ is correctly specified and the outcome model $\mu_1(X)$ is incorrect? First, we rewrite the estimator for the treated outcome:

$$\begin{aligned}
\hat Y_{1i}& = \frac{D_i(Y_i - \hat \mu_1 (X_i))}{\hat \pi (X_i)} + \hat \mu_1(X_i) \\
&= \frac{D_iY_i}{\hat \pi (X_i)} - \frac{D_i\hat \mu_1 (X_i)}{\hat \pi (X_i)} + \frac{\hat \pi (X_i)\hat \mu_1(X_i)}{\hat \pi (X_i)} \\
& = \frac{D_iY_i}{\hat \pi (X_i)} - \left( \frac{D_i - \hat \pi(X_i)}{\hat \pi (X_i)}\right) \hat \mu_1(X_i). &&(*)
\end{aligned}$$

Since the exposure model is correclty specified, we have $D_i = \hat \pi(X_i)$ on average, so

$$E[D_i - \hat \pi(X_i)] = 0.$$

This means that the second term in equation $(*)$ is 0, so

$$E[\hat Y_{1i}]= E \left [ \frac{D_iY_i}{\hat \pi (X_i)}\right].$$

This shows that when the exposure model is correct, then the estimator depends *only* on the exposure model. We can make similar arguments for the control model $\hat Y_{0i}$.

This demonstration shows that this estimator achieves double robustness: the estimator is robust to misspecification of either the exposure or the outcome model (but not both).

# Methods  

These methods have many similarities. How do the results they give compare? This section tests the performance of each in practice using two methods. First, results are compared using simulated data from a causal inference competition (@dorie_2019_automated). The true treatment effect is known, so these simulations allow assessment of accuracy. Second, these methods are applied to data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW). The NSW randomly provided training to disadvantaged workers, allowing an experimental estimate of the effect of the intervention, and data assembled by @dehejia_1999_causal allows these experimental estimates to be compared to observational ones.  

The three double robust methods are compared to two traditional estimation methods used as benchmarks: linear regression estimated using ordinary least squares regression ("OLS") and propensity-score matching with scores estimated from logistic regression using the `MatchIt` package ("PSM").    

Double robust can use a variety of techniques to estimate the underlying treatment and outcome models. The simulation results below compare three estimation techniques. First is using a logistic regression for the exposure model and an OLS regression for the outcome model. Second is generalized random forests [GRF, @athey_2019_generalized] using the `grf` R package, with separate models for exposure and outcome. The final technique is the SuperLearner (as promoted by the makers of TMLE) using the `SuperLearner` package, again with separate models for exposure and outcome. GLM, glmnet (lasso), and XGBoost models are considered for the SuperLearner.    

For the simulations, I have coded original functions to to implement the three double robust methods. This allows the use of the same exposure and outcome models' predictions in implementing these methods, permitting maximum comparability. However, the evaluation of the simulations only considers point estimates and not standard errors. For evaluation of the experimental LaLonde data, I use availabel R packages that calculate standard errors, using generalized random forests as the estimation technique. The augmented inverse propensity weighted estimator and targeted maximum likelihood estimation use the `grf` package, and double/debiased machine learning uses `DoubleML` and `mlr3` packages.  


# Results


## Simluations with Dorie et al. (2019) Data

In 2016, the Atlantic Causal Inference Conference hosted a competition for causal inference methods that adjust on observables. @dorie_2019_automated published the results of this competition, along with the data used in the competition. Below, I test double robust methods on the 20 data sets used for the "do-it-yourself" part of the competition. The data represent a hypothetical twins study investigating the impact of birth weight on IQ. The data have 4802 observations and 52 covariates. The authors of the study specify a different data generating process for the potential outcomes in each data set. In all cases, ignorability holds, but the authors vary the following:  

- degree of nonlinearity
- percentage of treated
- overlap for the treatment group
- alignment (correspondence between the assignment mechanism and the response surface)
- treatment effect heterogeneity
- overall magnitude of the treatment effect  

The 20 data sets used here cover a range of these attributes; see the supplemental material from @dorie_2019_automated for details. I use 10 simulations of each data set, resulting in 200 data sets. I also use 100 simulations of each of the two datasets with linear data generating processes (numbers 1 and 3) to show how these methods perform when the underlying models are linear. I then calculate bias, percent bias (the estimator's bias as a percentage of its standard error), root mean squared error (rmse), and median absolute error (mae). I also present the number of datasets for which the method fails and the average computation time for each data set, in seconds.  

Results for the full range of simulations are shown in Table \@ref(tab:dorie-results). Using AIPW as written above results in some wildly biased estimates for the ols_logit estimator, due to dividing by some very small propensity scores. Hence I present estimates from a trimmed AIPW estimator, where predicted exposure model values are set to 0.01 if they are less than 0.01 and to 0.99 if they are greater than 0.99.  

The lowest bias is achieved by AIPW with GRF, followed by propensity score matching and then OLS. The lowest root mean squared error is obtained by AIPW with GRF, followed by TMLE with GRF, and next by OLS regression. Results are similar if we consider percent bias or mean absolute error. DML does not perform particularly well and also has the highest computation times due to sample splitting.  

If we consider only the linear datasets (Table \@ref(tab:dorie-results-linear)), OLS obtains the lowest bias, followed by AIPW with GRF and then propensity score matching. The lowest root mean squared error is achieved by OLS, then AIPW with GRF, and then TMLE with GRF. Percent bias and mean absolute error show similar results.  

Overall, traditional methods perform surprisingly well in comparison with the double robust methods. Even in the full range of datasets -- which include highly nonlinear exposure and outcome data-generating processes -- OLS and propensity score matching obtain some of the smallest bias. Generalized random forests appear to be the best estimators for the double robust methods, though a SuperLearner than incorporates GRF has the potential to outperform GRF alone, at the expense of computation time. Notably, the method with the longest computation time -- DML with a SuperLearner -- takes over 2,000 times as along as OLS.  


```{r dorie-sim, eval = F}
set.seed(185)

sim_list <- list()
for(i in 1:20){ # up to 77
  #sim_list_nest <- list()
  for(j in 1:10){ # up to 100
    print(paste0(i,',',j))
    sim_list[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016,
                                      i, j, extraInfo = T)
  }
}

sim_linear <- list()
for(i in c(1, 3)){
  for(j in 1:100){ 
    print(paste0(i,',',j))
    sim_linear[[paste0(i,',',j)]] <- aciccomp2016::dgp_2016(aciccomp2016::input_2016, 
                                      i, j, extraInfo = T)
  }
}

saveRDS(append(sim_list, sim_linear), here('files', 'sims.RDS'))
```


```{r lm-sim, eval = F}


lm_sim_out <- lm_sim(sims) 

lm_df <- lm_sim_out$est_df %>%
  mutate(fail_count = lm_sim_out$fail_count,
         comp_time = lm_sim_out$comp_time)

write_csv(lm_df, here('files', 'lm_df.csv'))


```


```{r psm-sim, eval = F}
psm_sim_out <- psm_sim(sims) 

psm_df <- psm_sim_out$est_df %>%
  mutate(fail_count = psm_sim_out$fail_count,
         comp_time = psm_sim_out$comp_time)

write_csv(psm_df, here('files', 'psm_df.csv'))
```


```{r pred-calc, eval = F}

ols_logit_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  ols_logit_pred_list[[i]] <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(ols_logit_pred_list, here('files/homemade/', 'ols_logit_pred.RDS'))

# tibble(ols_logit_pred_list[[i]]$y,
#              ols_logit_pred_list[[i]]$mu1_pred, 
#              ols_logit_pred_list[[i]]$mu0_pred,
#              ols_logit_pred_list[[i]]$d,
#              hist(ols_logit_pred_list[[12]]$pi_pred)
#              ) 

grf_pred_list <- list()
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  grf_pred_list[[i]] <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(grf_pred_list, here('files/homemade/', 'grf_pred.RDS'))


superlearner_pred_list <- list() 
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time = Sys.time()
  
  superlearner_pred_list[[i]] <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) %>%
    append(list(truth = mean(dat$y.1) - mean(dat$y.0),
                comp_time = as.numeric(difftime(Sys.time(), start_time, units = 'secs'))))
}
saveRDS(superlearner_pred_list, here('files/homemade/', 'superlearner_pred.RDS'))


```








```{r aipw, eval = F}

aipw_calc2 <- function(mu1_pred, mu0_pred, pi_pred, d, y){
  n <- length(mu1_pred)
  
  
  # pi_pred <- case_when(
  #   pi_pred < quantile(pi_pred, c(.025)) ~ as.numeric(quantile(pi_pred, c(.025))),
  #   pi_pred > quantile(pi_pred, c(.975)) ~ as.numeric(quantile(pi_pred, c(.975))),
  #   T ~ pi_pred)
  
  # pi_pred <- case_when(
  #   pi_pred < .01 ~ .01,
  #   pi_pred > .99 ~ .99,
  #   T ~ pi_pred)
  
  ipw <- (d*y)/pi_pred - (1-d)*y/(1-pi_pred)
  adjust <- (d-pi_pred)/(pi_pred*(1-pi_pred)) * (1-pi_pred)*mu1_pred + pi_pred*mu0_pred
  
  ate <- (1/n) * sum(ipw - adjust)
  
  return(ate)
}


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade/', 'aipw.csv'))


aipw_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')
for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    
    start_time <- Sys.time()
    aipw_out <- with(predictions[[j]][[i]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    aipw_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = aipw_out, 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time)
  }
}

write_csv(bind_rows(aipw_out_list), here('files/homemade/', 'aipw_trunc.csv'))



```





```{r tmle, eval = F}

tmle_out_list <- list()
predictions <- list(ols_logit_pred_list2, grf_pred_list2, superlearner_pred_list2)
pred_names <- c('ols_logit', 'grf', 'superlearner')

for(i in 1:length(sims)){
  print(i)
  for(j in 1:length(predictions)){
    start_time <- Sys.time()
    tryCatch({
      tmle_out <- with(predictions[[j]][[i]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
    }, error=function(e){
      tmle_out <- NA
      })
    comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
    
    tmle_out_list[[paste0(i, ',', j)]] <- data.frame(
      dataset = i,
      method = pred_names[[j]],
      ate = denormalize(tmle_out, predictions[[j]][[i]]$y), 
      truth = predictions[[j]][[i]]$truth, 
      comp_time = comp_time + predictions[[j]][[i]]$comp_time,
      fail = is.na(tmle_out))
  }
}

write_csv(bind_rows(tmle_out_list), here('files/homemade/', 'tmle.csv'))


# logit_logit_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   logit_logit_pred_out <- logit_logit_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(logit_logit_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y)) 
#   truth <- mean(dat$y.1) - mean(dat$y.0)
# 
#   logit_logit_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(logit_logit_pred_list), label = 'ols, logit, tmle')
# 
# 
# #### 
# 
# grf_pred_list <- list()
# 
# for(i in 1:10){ #length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   grf_pred_out <- grf_pred(
#     y = dat$y, # with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   yt <- function(x, y){(x - min(y)) / (max(y) - min(y))}
#   
#   ate <- with(grf_pred_out, 
#               tmle_calc(mu1_pred = yt(mu1_pred, dat$y), 
#                         mu0_pred = yt(mu0_pred, dat$y),
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = yt(y, dat$y)))
#   truth <- mean(dat$y.1) - mean(dat$y.0)
#   
#   grf_pred_list[[i]] <-  data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(grf_pred_list), label = 'grf, tmle')


# superlearner_pred_list <- list()
# 
# for(i in 1:length(sim_linear)){
#   print(i)
#   dat <- sim_linear[[i]]
#   
#   superlearner_pred_out <- superlearner_pred(
#     y = with(dat, (y - min(y)) / (max(y) - min(y))),
#     d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
#     x = dat$x
#     )
#   
#   ate <- with(superlearner_pred_out, 
#               tmle_calc(mu1_pred = mu1_pred, 
#                         mu0_pred = mu0_pred,
#                         pi_pred = pi_pred,
#                         d = d,
#                         y = y))
#   truth = mean(dat$y.1) - mean(dat$y.0)
#   
#   superlearner_pred_list[[i]] <- data.frame(ate = ate * (max(dat$y)-min(dat$y)),
#                                             truth = truth)
# }
# 
# perform(bind_rows(superlearner_pred_list), label = 'superlearner, tmle')
```




```{r dml, eval = F}




## DML OLS logit

ols_logit_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)

  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  ols_logit_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time)
}

write_csv(bind_rows(ols_logit_dml_list), 
          here('files/homemade/', 'dml_ols_logit.csv'))

# perform(bind_rows(ols_logit_dml_list), label = 'grf, dml')

## GRF
grf_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  
  grf_dml_out <- do.call(grf_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  grf_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time)
}

write_csv(bind_rows(grf_dml_list), 
          here('files/homemade/', 'dml_grf.csv'))

# perform(bind_rows(grf_dml_list), label = 'grf, dml')

## Superlearner
superlearner_dml_list <- list()

set.seed(1285)
for(i in 1:length(sims)){
  print(i)
  dat <- sims[[i]]
  
  start_time <- Sys.time()
  
  dml_pre_out <- dml_pre(dat$y,
                       as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                       x = dat$x)
  
  superlearner_dml_out <- do.call(superlearner_dml, dml_pre_out)
  
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))
  
  comp_time <- as.numeric(difftime(Sys.time(), start_time, units = 'secs'))
  
  truth <- mean(dat$y.1) - mean(dat$y.0)
  
  superlearner_dml_list[[i]] <- data.frame(ate = dml_post_out, 
                              truth = truth, 
                              comp_time = comp_time)
}

write_csv(bind_rows(superlearner_dml_list), 
          here('files/homemade/', 'dml_superlearner.csv'))

#perform(bind_rows(superlearner_dml_list), label = 'superlearner, dml')

```


```{r dorie-results}

lm_df <- read_csv(here('files', 'lm_df.csv')) %>%
  mutate(method = 'ols')

psm_df <- read_csv(here('files', 'psm_df.csv')) %>%
  mutate(method = 'psm')

aipw_df <- read_csv(here('files/homemade', 'aipw.csv')) %>%
  mutate(estimator = method,
         method = 'aipw')

aipw_trunc_df <- read_csv(here('files/homemade', 'aipw_trunc.csv')) %>%
  mutate(estimator = method,
         method = 'aipw (trim)')

tmle_df <- read_csv(here('files/homemade', 'tmle.csv')) %>%
  filter(fail == F) %>%
  select(-fail) %>%
  mutate(estimator = method,
         method = 'tmle')

dml_df <- bind_rows(
  mutate(read_csv(here('files/homemade', 'dml_ols_logit.csv')), 
         estimator = 'ols_logit',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_grf.csv')), 
         estimator = 'grf',
         method = 'dml',
         dataset = row_number()),
  mutate(read_csv(here('files/homemade', 'dml_superlearner.csv')), 
         estimator = 'superlearner',
         method = 'dml',
         dataset = row_number())
) %>%
  select(dataset, estimator, ate, truth, comp_time, method)

bind_rows(lm_df, psm_df, aipw_trunc_df, tmle_df, dml_df) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'aipw', 'aipw (trim)', 'tmle', 'dml'))) %>%
  filter(dataset <= 200) %>%
  group_by(estimator, method) %>%
  summarize(bias = round(mean(ate - truth), 3),
              percent_bias = bias/sd(ate),
              rmse = round(sqrt(mean((ate - truth)^2)), 3),
              mae = median(abs(ate - truth)),
              comp_time = sum(comp_time)/n()
              ) %>%
  select(method, everything()) %>%
  arrange(method, estimator) %>%
  # mutate(bias = as.character(bias),
  #        rmse = as.character(rmse)) %>%
  # mutate(across(bias:comp_time, function(x){as.character(round(x, 3))})) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the first 20 datasets from Dorie et al. (2019), 10 replications each. Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.')
```

```{r dorie-results-linear}
bind_rows(lm_df, psm_df, aipw_trunc_df, tmle_df, dml_df) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'aipw', 'aipw (trim)', 'tmle', 'dml'))) %>%
  filter(dataset > 200) %>%
  group_by(estimator, method) %>%
  summarize(bias = round(mean(ate - truth), 3),
              percent_bias = bias/sd(ate),
              rmse = round(sqrt(mean((ate - truth)^2)), 3),
              mae = median(abs(ate - truth)),
              comp_time = sum(comp_time)/n()
              ) %>%
  select(method, everything()) %>%
  arrange(method, estimator) %>%
  mutate(bias = as.character(bias),
         rmse = as.character(rmse)) %>%
  # mutate(across(bias:comp_time, function(x){as.character(round(x, 3))})) %>%
  kableExtra::kable(booktabs = T, 
                    digits = 3,
                    linesep = '',
                    caption = 'Results of Monte Carlo simulations using the two datasets from Dorie et al. (2019), with linear data generating processes, 100 replications each ("linear"). Percent bias is calculated as the estimator\'s bias as a percentage of its standard error, rmse is root mean squared error, mae is median absolute error, and comp\\_time is average computation time measured in seconds for each dataset.')
```

## Comparisons using LaLonde NSW Data

As another test, I use data from LaLonde's [-@lalonde_1986_evaluating] study of the National Supported Work Demonstration (NSW), as provided by @dehejia_1999_causal. Between March 1975 and July 1977, the NSW randomly provided training to disadvantaged workers. LaLonde used earnings in 1978 as the outcome of interest; comparing earnings in this year for treated and untreated workers allows an experimental estimate of the effect of the intervention. Restricting the sample to men, this study had `r nrow(filter(lalonde_exp, treat == T))` treated and `r nrow(filter(lalonde_exp, treat == F))` control participants. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, and not having a high school degree. Following @dehejia_1999_causal, I add a variable indicating whether each respondent's earnings in 1975 was $0 -- i.e., they were unemployed.     

LaLonde compared these experimental estimates to control samples drawn from the Panel Study of Income Dynamics (PSID) and Westat's Matched Current Population Survey-Social Security Administration File (CPS). The PSID-1 sample (*n* = `r nrow(lalonde_psid1_controls)`) contains all male household heads under 55 who did not classify themselves as retired in 1975, and the PSID-3 sample (*n* = `r nrow(lalonde_psid3_controls)`) further restricts this to men who were not working in spring of 1975 or 1976. The CPS-1 sample (*n* = `r nrow(lalonde_cps1_controls)`) includes all CPS males under 55, and CPS-3 (*n* = `r nrow(lalonde_cps3_controls)`) restricts this to those who were not working in March 1976 whose earnings in 1975 was below the poverty level. Restricting these observational samples gets closer to the group eligible for the NSW.  

Following @dehejia_1999_causal, I present results for the original samples analyzed by @lalonde_1986_evaluating, but I also include results using a subsample of the experimental group that has 1974 earnings data available (`r nrow(filter(lalonde_exp_74, treat == T))` treated and `r nrow(filter(lalonde_exp_74, treat == F))` control participants) and include this additional covariate, along with an indicator variable for no earnings in 1974.  

I again compare the three double robust methods to a linear model fitted by OLS ("ols") and propensity score matching using logistic regression ("psm"). Results are presented in Table \@ref(tab:lalonde).  

The "experimental" column provides a baseline for the comparison, suggesting that the program resulted in an earnings gain of about \$700 to \$800. If selection on observables holds, then we should be able to recover these estimates from the non-experimental control groups. Most of the methods do not perform very well; the only ones to estimate a positive treatment effects are OLS for the PSID-3 sample and TMLE for the CPS samples, and these are still smaller than the experimental baselines.  

Including 1974 earnings data results in much better estimates with the observational control groups. For PSID-1, TMLE provides estimates closest to the experimental ones, while DML performs much worse than even OLS. For PSID-3, OLS and TMLE both perform well. For CPS-1 and CPS-3, TMLE performs the best.  

These results highlight the importance of selection on observables holding. Without including 1974 earnings as a covariate, it appears that selection on observables does not hold, as most methods provide highly inaccurate estimates with the wrong sign. Once 1974 earnings are included, all of the methods provide estimates much closer to the experimental values. PSM, AIPW, and TMLE all do fairly well, while OLS and DML are more unstable across samples.



```{r lalonde-calc-homemade, eval = F}

lalonde_func <- function(dataset, sample, include_74 = F){
  set.seed(872)
  
  
  if(include_74 == F){
    lalonde_variables <- names(lalonde_exp)[3:9]
    dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
    
  } else {
    lalonde_variables <- names(lalonde_exp_74)[3:10]
    dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
  }
  
  dat <- list(y = dataset$re78,
                z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                x = dataset[,lalonde_variables])
  
  
  out_list <- list()
  
  out_list[['ols']] <- data.frame(
      method = 'ols',
      estimator = NA,
      ate = lm_sim(list(dat))$est_df$ate) 
  
  out_list[['psm']] <- data.frame(
      method = 'psm',
      estimator = NA,
      ate = lm_sim(list(dat))$est_df$ate)
  
  ols_logit_pred_out <- ols_logit_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 

  
  grf_pred_out <- grf_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 

  superlearner_pred_out <- superlearner_pred(
    y = dat$y,
    d = as.numeric(ifelse(dat$z == 'trt', 1, 0)),
    x = dat$x) 
  
  predictions <- list(ols_logit_pred_out, grf_pred_out, superlearner_pred_out)
  pred_names <- c('ols_logit', 'grf', 'superlearner')
  for(j in 1:length(predictions)){
    aipw_out <- with(predictions[[j]], 
                     aipw_calc_trunc(mu1_pred = mu1_pred, 
                               mu0_pred = mu0_pred,
                               pi_pred = pi_pred,
                               d = d,
                               y = y))
    
    out_list[[paste0('aipw', j)]] <- data.frame(
      method = 'aipw',
      estimator = pred_names[[j]],
      ate = aipw_out)
    
    tmle_out <- with(predictions[[j]], 
                       tmle_calc(mu1_pred = normalize(mu1_pred, y), 
                                 mu0_pred = normalize(mu0_pred, y),
                                 pi_pred = pi_pred,
                                 d = d,
                                 y = normalize(y, y))
                       )
      
      out_list[[paste0('tmle', j)]] <- data.frame(
        method = 'tmle',
        estimator = pred_names[[j]],
        ate =  denormalize(tmle_out, predictions[[j]]$y))
  }
  
  dml_pre_out <- dml_pre(dat$y,
                         as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                         x = dat$x)
  ols_logit_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, ols_logit_dml_out))

  out_list[['dml_ols_logit']] <- data.frame(method = 'dml',
                                            estimator = 'ols_logit',
                                            ate = dml_post_out)
  
  dml_pre_out <- dml_pre(dat$y,
                     as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                     x = dat$x)
  grf_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, grf_dml_out))

  out_list[['dml_grf']] <- data.frame(method = 'dml',
                                            estimator = 'grf',
                                            ate = dml_post_out)
  
  dml_pre_out <- dml_pre(dat$y,
                     as.numeric(ifelse(dat$z == 'trt', 1, 0)),
                     x = dat$x)
  superlearner_dml_out <- do.call(ols_logit_dml, dml_pre_out)
  dml_post_out <- do.call(dml_post, append(dml_pre_out, superlearner_dml_out))

  out_list[['dml_superlearner']] <- data.frame(method = 'dml',
                                            estimator = 'superlearner',
                                            ate = dml_post_out)
  
  bind_rows(out_list) %>%
    mutate(sample = sample) %>%
    return()
}
  


lalonde_func_out <- bind_rows(
  lalonde_func(filter(lalonde_exp, treat == 0), sample = 'experimental'),
  lalonde_func(lalonde_psid1_controls, 'PSID-1'),
  lalonde_func(lalonde_psid3_controls, 'PSID-3'),
  lalonde_func(lalonde_cps1_controls, 'CPS-1'),
  lalonde_func(lalonde_cps1_controls, 'CPS-3')) 
  
  lalonde_func_out %>%
    pivot_wider(names_from = 'sample', values_from = ate) %>%
  mutate(estimator = factor(estimator, levels = c('ols_logit', 'grf', 'superlearner')),
         method = factor(method, levels = c('ols', 'psm', 'aipw', 'aipw (trim)', 'tmle', 'dml'))) %>%
  arrange(method, estimator) %>%
  #write_csv(here('files/homemade', 'lalonde.csv'))


bind_rows(
  lalonde_func(filter(lalonde_exp_74, treat == 0), sample = 'experimental', T),
  lalonde_func(lalonde_psid1_controls, 'PSID-1', T),
  lalonde_func(lalonde_psid3_controls, 'PSID-3', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-1', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-3', T)) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files/homemade', 'lalonde_74.csv'))

```

```{r lalonde-calc, eval = F}
unemp_func <- function(x){
  x %>%
    as.data.frame() %>%
    mutate(re74_0 = re74 == 0,
         re75_0 = re75 == 0) 
}

lalonde_exp <- read_dta(here('data', 'nsw.dta')) %>%
  as.data.frame() %>%
  mutate(re75_0 = re75 == 0)
lalonde_exp_74 <- read_dta(here('data', 'nsw_dw.dta')) %>%
  unemp_func()
lalonde_cps1_controls <- read_dta(here('data', 'cps_controls.dta')) %>%
  unemp_func()
lalonde_cps3_controls <- read_dta(here('data', 'cps_controls3.dta')) %>%
  unemp_func()
lalonde_psid1_controls <- read_dta(here('data', 'psid_controls.dta')) %>%
  unemp_func()
lalonde_psid3_controls <- read_dta(here('data', 'psid_controls3.dta')) %>%
  unemp_func()


lalonde_func <- function(dataset, sample, include_74 = F){
  set.seed(872)
  
  if(include_74 == F){
    lalonde_variables <- names(lalonde_exp)[3:9]
    dataset <- bind_rows(filter(lalonde_exp, treat == 1), dataset)
    
  } else {
    lalonde_variables <- names(lalonde_exp_74)[3:10]
    dataset <- bind_rows(filter(lalonde_exp_74, treat == 1), dataset)
  }
  
  dataset <- list(list(y = dataset$re78,
                z = ifelse(dataset$treat == 1, 'trt', 'ctl'),
                x = dataset[,lalonde_variables]))
  
  lapply(list(
    lm_sim(dataset),
    psm_sim(dataset),
    aipw_sim(dataset),
    tmle_sim(dataset),
    dml_sim(dataset)),
  bind_cols) %>%
    bind_rows() %>%
    mutate(method = c('ols', 'psm', 'aipw', 'tmle', 'dml'),
           sample = sample,
           ate = round(d),
           se = round(se),
           estimate = paste0(round(d), ' (', round(se), ')')) %>%
    select(method, sample, estimate) 
}

bind_rows(
  lalonde_func(filter(lalonde_exp, treat == 0), sample = 'experimental'),
  lalonde_func(lalonde_psid1_controls, 'PSID-1'),
  lalonde_func(lalonde_psid3_controls, 'PSID-3'),
  lalonde_func(lalonde_cps1_controls, 'CPS-1'),
  lalonde_func(lalonde_cps1_controls, 'CPS-3')) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde.csv'))


bind_rows(
  lalonde_func(filter(lalonde_exp_74, treat == 0), sample = 'experimental', T),
  lalonde_func(lalonde_psid1_controls, 'PSID-1', T),
  lalonde_func(lalonde_psid3_controls, 'PSID-3', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-1', T),
  lalonde_func(lalonde_cps1_controls, 'CPS-3', T)) %>%
    pivot_wider(names_from = 'sample', values_from = estimate) %>%
  write_csv(here('files', 'lalonde_74.csv'))
```


```{r lalonde}
bind_rows(
  read_csv(here('files', 'lalonde.csv')) %>%
    mutate(sample = 'Original LaLonde'),
  read_csv(here('files', 'lalonde_74.csv')) %>%
    mutate(sample = 'With 1974 earnings')) %>%
  select(sample, method, everything()) %>%
  kableExtra::kable(booktabs = T, 
                    # digits = 3,
                    # linesep = '',
                    caption = 'ATE estimates for Lalonde NSW data as provided by Dehejia and Wahba (1999), with CPS and PSID comparison groups. Standard errors shown in parentheses. Covariates include age, education in years of schooling, earnings in 1975, and dichotomous variables for Black and Hispanic race, married, not having a high school degree, and having no earnings in 1975. The "With 1974 earnings" estimates additionally include earnings in 1974 as a covariate, along with an indicator for having no earnigns in 1974.')

# bind_rows(
#   read_csv(here('files', 'lalonde.csv')) %>%
#     mutate(sample = 'LaLonde'),
#   read_csv(here('files', 'lalonde_74.csv')) %>%
#     mutate(sample = 'DW (with 1974 earnings)')) %>%
#   select(sample, everything()) %>%
#   kableExtra::kable(booktabs = T, 
#                     # digits = 3,
#                     linesep = '',
#                     caption = 'ATE estimates for Lalonde NSW data from Dehejia and Wahba (1999), with PSID ')


# lalonde_exp_df <- filter(lalonde, exper == 1)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_exp_df, ATT = TRUE)
# summary(fit)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_exp_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
# 
# 
# lalonde_psid_df <- filter(lalonde, (exper == 1 & treat == 1) | exper == 0)
# fit <- CBPS::CBPS(treat ~ age + educ + re75 + re74 + 
# 			I(re75==0) + I(re74==0), 
# 			data = lalonde_psid_df, ATT = TRUE)
# m.out <- MatchIt::matchit(treat ~ fitted(fit), method = "nearest", 
# 				 data = lalonde_psid_df, replace = TRUE)
# lm(re78 ~ treat + age + educ + re75 + re74, MatchIt::match.data(m.out), weights = weights) %>%
#   tidy()
```

# Conclusion

This paper aimed to provide an introduction to and evaluation of double robust methods for covariate adjustment in causal inference. By comparing AIPW, TMLE, and DML to the more traditional statistical methods of OLS and PSM, it allows evaluation of whether these methods are worth the effort and (computational) time for social scientists to adopt them.  

Results are mixed. On the one hand, in the full range of simulated data, AIPW with GRF estimators achieves the lowest bias and root mean squared error. This is the simplest (and most computationally efficient) of the double robust methods, although GRF is computationally expensive. TMLE with GRF also does fairly well. But OLS and PSM have strong showings, in the top three smallest bias or root mean squared error. In the datasets with linear data-generating processes, OLS achieves the lowest bias and root mean squared error.  

In the experimental LaLonde data, OLS does not perform as strongly. Although it is one of the few methods to provide experimental estimates with the correct sign in the original LaLonde set of covariates, its estimates are highly unstable across samples. When 1974 earnings are used as a covariate, PSM, AIPW, and TMLE all perform fairly well, though TMLE comes out on top.  

Overall, these double robust methods do not show a clear advantage over simpler, more computationally efficient methods such as OLS and PSM. While they are useful for social scientists to understand, in most applications, OLS or PSM provide similar results. However, in certain circumstances these double robust methods may be a better choice. Especially when paired with a highly flexible estimator like GRF, these methods can be useful when the number of covariates is high or even exceeds the number of observations. If the researcher believes the data-generating process is highly nonlinear, they may also be a good a choice, at least as a sensitivity check.   



\newpage


# References
